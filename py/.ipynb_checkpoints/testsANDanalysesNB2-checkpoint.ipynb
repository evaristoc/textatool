{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group)\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "\n",
    "    return [cand for cand in candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text().replace('’',\"'\")\n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token.lower()\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var4: wordimportance_var3 modified to fit keyphrases; changes in the \"opacity\" formula\n",
    "############################\n",
    "def wordimportance_var4(lemmposrecs, lemmws_fd):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    unigrams = dict([(grams, count) \n",
    "                     for grams, count in lemmws_fd.items() \n",
    "                     if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                    ])\n",
    "    maxdiv = math.log(sorted(unigrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "    opacity = collections.defaultdict(float)\n",
    "    for grams, counts in lemmws_fd.items():\n",
    "        opval = []\n",
    "        for gram in grams.split():\n",
    "            if gram == '':\n",
    "                continue\n",
    "            if gram in unigrams:\n",
    "                opval.append(math.log(unigrams[gram]))\n",
    "            else:\n",
    "                opval.append(0)\n",
    "        averopval = sum(opval)/len(opval)\n",
    "        if 1 - averopval/maxdiv != 0.0:\n",
    "            opacity[grams] = 1 - averopval/maxdiv\n",
    "        else:\n",
    "            opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "        \n",
    "    sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "\n",
    "    ## Count lemmatized words/characters per text  \n",
    "    for i,lemmpos_t in enumerate(lemmposrecs):\n",
    "        for k, lemmpos_TUPLE in enumerate(lemmpos_t[3]):\n",
    "            ## Use lemmatized word\n",
    "            lemmw = lemmpos_TUPLE[2]\n",
    "            sizing_matrix[lemmw][i] = sizing_matrix[lemmw][i] + 1\n",
    "        for cand in lemmpos_t[4]: #<--------------------------------- problem!! it counted candidates only once!!!\n",
    "            if cand not in sizing_matrix:\n",
    "                sizing_matrix[cand][i] = sizing_matrix[cand][i] + 1\n",
    "\n",
    "       \n",
    "    ## Normalization\n",
    "    normalization = dict([(k, (sum(vector)-max(vector))/sum(vector)) if sum(vector) != 0 else (k,0.0) for k, vector in sizing_matrix.items()])\n",
    "    #normalization = collections.defaultdict(float)\n",
    "    #for k, vector in sizing_matrix.items():\n",
    "    #    if sum(vector) != 0:\n",
    "    #        normalization[k] = (sum(vector)-max(vector))/sum(vector)\n",
    "    #    else:\n",
    "    #        \n",
    "        \n",
    "    \n",
    "    wordimportance = dict([(k, valnorm*opacity[k]) for k, valnorm in normalization.items()])\n",
    "\n",
    "    return wordimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaningtext(st, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    treated_st = []\n",
    "    countwds = len(st)\n",
    "    for w in st:\n",
    "        if 'freecodecamp' in w:\n",
    "            w = w.replace('freecodecamp','fcc')\n",
    "        treated_st.append(w)\n",
    "    return treated_st, countwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## gensim_models2 re-evaluated: modification of gensim_model2's cleanedsts_from_lemmpostxts to fit keyphrases\n",
    "############################\n",
    "\n",
    "def gensim_models2(lemmposrecs, NUM_TOPICS = 15, lemmws_fd = {}, wordimportance = {}, nltk = nltk, gensim = gensim):\n",
    "    \n",
    "    def cleanedsts_from_lemmpostxts2(lemmposrecs, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "        redo_corpus_by_sts = []\n",
    "        for lemmpos_r in lemmposrecs:\n",
    "            candidates = lemmpos_r[4]\n",
    "            treated_lemmcands = cleaningtext(candidates)[0] #passing candidates only\n",
    "            redo_corpus_by_sts.append(treated_lemmcands) \n",
    "        \n",
    "        #print(len(redo_corpus_by_sts), redo_corpus_by_sts[-1])\n",
    "        return redo_corpus_by_sts\n",
    "        \n",
    "    def basedonBOW(redo_corpus_by_sts):\n",
    "        dictionary = gensim.corpora.Dictionary(redo_corpus_by_sts) #[token for st in redo_corpus_by_sts for token in st]\n",
    "        corpus = [dictionary.doc2bow(text) for text in redo_corpus_by_sts]\n",
    "        return corpus, dictionary\n",
    "    \n",
    "    def basedonTFIDF(corpus):\n",
    "        return gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    def basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance):\n",
    "        \n",
    "        def metriccalc(w):\n",
    "            if w in wordimportance:\n",
    "                return 1.0+2.0**float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        corpus = []\n",
    "        for sts in redo_corpus_by_sts:\n",
    "            st = []\n",
    "            for w in sts:\n",
    "                st.append((dictionary.token2id[w], metriccalc(w)))\n",
    "            corpus.append(st)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    redo_corpus_by_sts  = cleanedsts_from_lemmpostxts2(lemmposrecs)\n",
    "    \n",
    "    corpus, dictionary = basedonBOW(redo_corpus_by_sts)\n",
    "    if wordimportance == {'tfidf':True}:\n",
    "        tfidf = basedonTFIDF(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "    if wordimportance != {} and wordimportance != {'tfidf':True}:\n",
    "        corpus = basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance)\n",
    "    \n",
    "    \n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, minimum_probability=0.005, per_word_topics = True, minimum_phi_value = 0.001, id2word=dictionary)\n",
    "    lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "   \n",
    "    return lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n",
      "Error (not in corpus) ! .\n",
      "pos KeyErrors happy JJ\n",
      "Error (not in corpus) to TO\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) that IN\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors first JJ\n",
      "Error (not in corpus) offer/trainee JJ\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) the DT\n",
      "pos KeyErrors a DT\n",
      "pos KeyErrors digital JJ\n",
      "Error (not in corpus) and CC\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) they PRP\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors graphic JJ\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors some DT\n",
      "pos KeyErrors basic JJ\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors will MD\n",
      "pos KeyErrors graphic JJ\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors basic JJ\n",
      "Error (not in corpus) ( (\n",
      "Error (not in corpus) html5 NN\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) ) )\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors a DT\n",
      "Error (not in corpus) of IN\n",
      "pos KeyErrors a DT\n",
      "pos KeyErrors paid JJ\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) where WRB\n",
      "pos KeyErrors full JJ\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors but CC\n",
      "pos KeyErrors by IN\n",
      "pos KeyErrors a DT\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) the DT\n",
      "pos KeyErrors 3 CD\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors after IN\n",
      "Error (not in corpus) which WDT\n",
      "Error (not in corpus) they PRP\n",
      "pos KeyErrors will MD\n",
      "pos KeyErrors me PRP\n",
      "pos KeyErrors a DT\n",
      "pos KeyErrors full JJ\n",
      "Error (not in corpus) ( (\n",
      "Error (not in corpus) if IN\n",
      "IndexErrors (POS not found) i VBN\n",
      "IndexErrors (POS not found) deliver NN\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) that WDT\n",
      "Error (not in corpus) ) )\n",
      "Error (not in corpus) . .\n",
      "IndexErrors (POS not found) fcc RB\n",
      "pos KeyErrors 4 CD\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) since IN\n",
      "Error (not in corpus) the DT\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors basic JJ\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) the DT\n",
      "IndexErrors (POS not found) js VBP\n",
      "pos KeyErrors on IN\n",
      "Error (not in corpus) mozilla NN\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors through IN\n",
      "pos KeyErrors a DT\n",
      "Error (not in corpus) php NN\n",
      "Error (not in corpus) from IN\n",
      "pos KeyErrors scratch JJ\n",
      "pos KeyErrors on IN\n",
      "Error (not in corpus) udemy NN\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors in IN\n",
      "Error (not in corpus) and CC\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors off IN\n",
      "Error (not in corpus) from IN\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors 3 CD\n",
      "Error (not in corpus) . .\n",
      "IndexErrors (POS not found) i RB\n",
      "Error (not in corpus) ( (\n",
      "Error (not in corpus) with IN\n",
      "Error (not in corpus) the DT\n",
      "pos KeyErrors financial JJ\n",
      "Error (not in corpus) of IN\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors significant JJ\n",
      "pos KeyErrors other JJ\n",
      "Error (not in corpus) ) )\n",
      "Error (not in corpus) to TO\n",
      "pos KeyErrors on IN\n",
      "Error (not in corpus) than IN\n",
      "pos KeyErrors another DT\n",
      "pos KeyErrors retail JJ\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors on IN\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors all DT\n",
      "pos KeyErrors in IN\n",
      "pos KeyErrors all DT\n",
      "pos KeyErrors no DT\n",
      "pos KeyErrors real JJ\n",
      "pos KeyErrors in IN\n",
      "Error (not in corpus) the DT\n",
      "pos KeyErrors or CC\n",
      "pos KeyErrors it PRP\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) what WP\n",
      "pos KeyErrors me PRP\n",
      "Error (not in corpus) the DT\n",
      "Error (not in corpus) ? .\n",
      "pos KeyErrors a DT\n",
      "pos KeyErrors graphic JJ\n",
      "Error (not in corpus) ( (\n",
      "pos KeyErrors adapted JJ\n",
      "pos KeyErrors one CD\n",
      "Error (not in corpus) of IN\n",
      "Error (not in corpus) those DT\n",
      "Error (not in corpus) photoshop JJ\n",
      "Error (not in corpus) ) )\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors motivational JJ\n",
      "pos KeyErrors i JJ\n",
      "pos KeyErrors willing JJ\n",
      "Error (not in corpus) to TO\n",
      "Error (not in corpus) and CC\n",
      "Error (not in corpus) for IN\n",
      "pos KeyErrors new JJ\n",
      "Error (not in corpus) would MD\n",
      "Error (not in corpus) they PRP\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors weird JJ\n",
      "Error (not in corpus) of IN\n",
      "pos KeyErrors graphic JJ\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors basic JJ\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors most JJS\n",
      "Error (not in corpus) ( (\n",
      "Error (not in corpus) : :\n",
      "Error (not in corpus) - :\n",
      "Error (not in corpus) where WRB\n",
      "IndexErrors (POS not found) i RB\n",
      "Error (not in corpus) ) )\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors but CC\n",
      "Error (not in corpus) the DT\n",
      "Error (not in corpus) of IN\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) we PRP\n",
      "pos KeyErrors through IN\n",
      "Error (not in corpus) my PRP$\n",
      "pos KeyErrors previous JJ\n",
      "Error (not in corpus) ( (\n",
      "pos KeyErrors some DT\n",
      "Error (not in corpus) wordpress NN\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors various JJ\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) seo JJ\n",
      "Error (not in corpus) optimisation… NN\n",
      "Error (not in corpus) ) )\n",
      "Error (not in corpus) and CC\n",
      "Error (not in corpus) how WRB\n",
      "pos KeyErrors i JJ\n",
      "Error (not in corpus) the DT\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors most JJS\n",
      "Error (not in corpus) of IN\n",
      "pos KeyErrors all DT\n",
      "Error (not in corpus) they PRP\n",
      "Error (not in corpus) my PRP$\n",
      "Error (not in corpus) self-learning JJ\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) they PRP\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) they PRP\n",
      "Error (not in corpus) for IN\n",
      "pos KeyErrors who WP\n",
      "pos KeyErrors will MD\n",
      "pos KeyErrors different JJ\n",
      "Error (not in corpus) and CC\n",
      "pos KeyErrors new JJ\n",
      "pos KeyErrors as IN\n",
      "IndexErrors (POS not found) arises NNS\n",
      "Error (not in corpus) . .\n",
      "Error (not in corpus) : :\n",
      "pos KeyErrors 3 CD\n",
      "pos KeyErrors foreign JJ\n",
      "Error (not in corpus) ( (\n",
      "pos KeyErrors english JJ\n",
      "pos KeyErrors one CD\n",
      "Error (not in corpus) of IN\n",
      "Error (not in corpus) them PRP\n",
      "Error (not in corpus) ) )\n",
      "pos KeyErrors all DT\n",
      "pos KeyErrors in IN\n",
      "pos KeyErrors all DT\n",
      "pos KeyErrors happy JJ\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors will MD\n",
      "pos KeyErrors able JJ\n",
      "Error (not in corpus) to TO\n",
      "pos KeyErrors new JJ\n",
      "pos KeyErrors in IN\n",
      "pos KeyErrors a DT\n",
      "pos KeyErrors real JJ\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors a DT\n",
      "pos KeyErrors small JJ\n",
      "Error (not in corpus) and CC\n",
      "Error (not in corpus) my PRP$\n",
      "Error (not in corpus) diy-ing NN\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors i JJ\n",
      "IndexErrors (POS not found) i RB\n",
      "pos KeyErrors will MD\n",
      "Error (not in corpus) kickstart VB\n",
      "Error (not in corpus) my PRP$\n",
      "Error (not in corpus) . .\n",
      "pos KeyErrors many JJ\n",
      "Error (not in corpus) to TO\n",
      "pos KeyErrors free JJ\n",
      "Error (not in corpus) , ,\n",
      "Error (not in corpus) for IN\n",
      "pos KeyErrors all PDT\n",
      "Error (not in corpus) the DT\n",
      "pos KeyErrors technical JJ\n",
      "pos KeyErrors but CC\n",
      "pos KeyErrors important JJ\n",
      "Error (not in corpus) the DT\n",
      "Error (not in corpus) of IN\n",
      "Error (not in corpus) ! .\n",
      "pos KeyErrors i JJ\n",
      "Error (not in corpus) this DT\n",
      "Error (not in corpus) , ,\n",
      "pos KeyErrors wish JJ\n",
      "pos KeyErrors me PRP\n",
      "Error (not in corpus) ! .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'STOPWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-9aa438269348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlemmposrecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmws_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallrecordsPreparation3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-819c8ae07208>\u001b[0m in \u001b[0;36mallrecordsPreparation3\u001b[1;34m(allrecords, STOPWORDS, punct)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;31m## Keyphrases candidates; complete lemmws with candidates that are not still there\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_candidate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmpostxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmws\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-c8375c6f9cfe>\u001b[0m in \u001b[0;36mextract_candidate_chunks\u001b[1;34m(lemmpostxt, grammar)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlemmposst_redo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmposst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[1;31m#or not all(char in punct for char in w):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mlemmposst_redo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STOPWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordimportance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9f9500758a50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsi_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredo_corpus_by_sts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim_models2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmposrecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordimportance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordimportance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wordimportance' is not defined"
     ]
    }
   ],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, wordimportance = wordimportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"lot\" + 0.005*\"interview\" + 0.005*\"company\" + 0.005*\"year\" + 0.005*\"month\" + 0.005*\"github\" + 0.005*\"day\" + 0.005*\"fcc\" + 0.004*\"experience\" + 0.004*\"job\"'),\n",
       " (1,\n",
       "  '0.007*\"something\" + 0.006*\"day\" + 0.005*\"people\" + 0.004*\"mistake\" + 0.004*\"interview\" + 0.004*\"time\" + 0.004*\"project\" + 0.004*\"month\" + 0.004*\"portfolio\" + 0.004*\"company\"'),\n",
       " (2,\n",
       "  '0.007*\"job\" + 0.004*\"company\" + 0.004*\"project\" + 0.004*\"year\" + 0.004*\"code\" + 0.004*\"lot\" + 0.003*\"group\" + 0.003*\"question\" + 0.003*\"someone\" + 0.003*\"people\"'),\n",
       " (3,\n",
       "  '0.009*\"company\" + 0.005*\"bit\" + 0.005*\"project\" + 0.004*\"anything\" + 0.004*\"something\" + 0.003*\"money\" + 0.003*\"full time job\" + 0.003*\"stuff\" + 0.003*\"it’s\" + 0.003*\"huge thank quincy everyone community\"'),\n",
       " (4,\n",
       "  '0.005*\"experience\" + 0.004*\"share\" + 0.004*\"job\" + 0.004*\"solve challenge\" + 0.003*\"app\" + 0.003*\"interview\" + 0.003*\"thing\" + 0.003*\"someone\" + 0.003*\"something\" + 0.003*\"company\"'),\n",
       " (5,\n",
       "  '0.015*\"job\" + 0.010*\"something\" + 0.008*\"day\" + 0.008*\"week\" + 0.007*\"month\" + 0.006*\"interview\" + 0.005*\"fcc\" + 0.005*\"time\" + 0.004*\"test\" + 0.004*\"new job\"'),\n",
       " (6,\n",
       "  '0.006*\"start\" + 0.006*\"announce\" + 0.006*\"let\" + 0.006*\"job\" + 0.005*\"year\" + 0.005*\"interview idea type question\" + 0.005*\"interview front end developer job today skill\" + 0.005*\"self taught fcc\" + 0.005*\"year time\" + 0.005*\"html cs javascript\"'),\n",
       " (7,\n",
       "  '0.006*\"interview\" + 0.006*\"challenge\" + 0.006*\"advice\" + 0.006*\"someone\" + 0.005*\"fcc\" + 0.005*\"project\" + 0.005*\"job\" + 0.004*\"hey\" + 0.004*\"camper\" + 0.004*\"thanks\"'),\n",
       " (8,\n",
       "  '0.014*\"job\" + 0.010*\"company\" + 0.008*\"month\" + 0.008*\"code\" + 0.007*\"fcc\" + 0.007*\"course\" + 0.006*\"hey\" + 0.005*\"something\" + 0.005*\"camper\" + 0.005*\"week\"'),\n",
       " (9,\n",
       "  '0.009*\"project\" + 0.009*\"day\" + 0.005*\"fcc\" + 0.004*\"guide\" + 0.004*\"course\" + 0.004*\"logic\" + 0.004*\"contract\" + 0.004*\"march\" + 0.004*\"thing\" + 0.004*\"resource\"'),\n",
       " (10,\n",
       "  '0.008*\"fcc\" + 0.007*\"interview\" + 0.005*\"developer job i’ve\" + 0.005*\"end certificate\" + 0.005*\"second week new place lot learn first period\" + 0.005*\"use xml xsl\" + 0.003*\"hi\" + 0.003*\"month\" + 0.003*\"job\" + 0.003*\"everything\"'),\n",
       " (11,\n",
       "  '0.007*\"job\" + 0.005*\"lot\" + 0.004*\"company\" + 0.004*\"interview\" + 0.003*\"people\" + 0.003*\"month\" + 0.003*\"year\" + 0.003*\"fact\" + 0.003*\"way\" + 0.003*\"fcc\"'),\n",
       " (12,\n",
       "  '0.007*\"job\" + 0.006*\"project\" + 0.005*\"fcc\" + 0.005*\"time\" + 0.005*\"thing\" + 0.004*\"dev job\" + 0.004*\"response\" + 0.004*\"share\" + 0.003*\"week\" + 0.003*\"stuff\"'),\n",
       " (13,\n",
       "  '0.010*\"job\" + 0.008*\"project\" + 0.007*\"people\" + 0.006*\"way\" + 0.005*\"month\" + 0.005*\"fcc\" + 0.005*\"learning\" + 0.005*\"day\" + 0.004*\"interview\" + 0.004*\"advice\"'),\n",
       " (14,\n",
       "  '0.009*\"year\" + 0.008*\"company\" + 0.008*\"code\" + 0.008*\"job\" + 0.007*\"time\" + 0.006*\"work\" + 0.005*\"hey\" + 0.005*\"thing\" + 0.004*\"resume\" + 0.004*\"project\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"free code camp\" + 0.002*\"python\" + 0.001*\"team\" + 0.001*\"computer\" + 0.001*\"week\" + 0.001*\"language like python\" + 0.001*\"off-chance i\" + 0.001*\"regressive business ethos\" + 0.001*\"4th week\" + 0.001*\"data entry for example\" + 0.001*\"middle\" + 0.001*\"active coder\" + 0.001*\"resource for web scraping\" + 0.001*\"first website\" + 0.001*\"beaten-up laptop\"'),\n",
       " (1,\n",
       "  '0.002*\"data science\" + 0.002*\"developer job\" + 0.002*\"fresh graduate\" + 0.001*\"dream job\" + 0.001*\"tutorial\" + 0.001*\"hackathons\" + 0.001*\"fullstack\" + 0.001*\"pay\" + 0.001*\"fcc backend project\" + 0.001*\"data scientist\" + 0.001*\"frontend\" + 0.001*\"city\" + 0.001*\"big thanks\" + 0.001*\"angularjs\" + 0.001*\"year since i\"'),\n",
       " (2,\n",
       "  '0.002*\"failure\" + 0.002*\"half\" + 0.002*\"year\" + 0.002*\"developer\" + 0.002*\"response\" + 0.002*\"hard work\" + 0.002*\"web developer job\" + 0.002*\"front-end web developer job\" + 0.002*\"sacrifice\" + 0.002*\"word of motivation\" + 0.002*\"education background\" + 0.002*\"web development before free code camp”\" + 0.002*\"article “how i\" + 0.002*\"clone app\" + 0.002*\"last project\"'),\n",
       " (3,\n",
       "  '0.002*\"year time\" + 0.002*\"type of question\" + 0.002*\"front end developer job today\" + 0.002*\"self taught through fcc\" + 0.002*\"start\" + 0.002*\"recruiter\" + 0.002*\"interview\" + 0.002*\"skill\" + 0.002*\"portfolio\" + 0.002*\"udacity\" + 0.002*\"friend\" + 0.002*\"language\" + 0.001*\"time\" + 0.001*\"idea\" + 0.001*\"anything\"'),\n",
       " (4,\n",
       "  '0.000*\"thank\" + 0.000*\"xsl\" + 0.000*\"lawyer\" + 0.000*\"medium article\" + 0.000*\"part\" + 0.000*\"self taught through fcc\" + 0.000*\"job\" + 0.000*\"back-end certificate\" + 0.000*\"yesterday\" + 0.000*\"experience of real time project\" + 0.000*\"community\" + 0.000*\"employer\" + 0.000*\"fcc\" + 0.000*\"front end developer job today\" + 0.000*\"year\"'),\n",
       " (5,\n",
       "  '0.004*\"lawyer\" + 0.002*\"ideas.ataccama.com i\" + 0.002*\"medium article\" + 0.002*\"developer with free code camp\" + 0.002*\"current job\" + 0.002*\"engineering role\" + 0.002*\"infrastructure\" + 0.002*\"march\" + 0.002*\"math\" + 0.002*\"advance logic\" + 0.002*\"look\" + 0.002*\"hey fellow camper\" + 0.002*\"topic\" + 0.002*\"people\" + 0.001*\"internet\"'),\n",
       " (6,\n",
       "  '0.003*\"jquery\" + 0.002*\"test\" + 0.002*\"class\" + 0.002*\"interview process\" + 0.002*\"couple of month\" + 0.002*\"future\" + 0.002*\"in-person interview\" + 0.002*\"other thing\" + 0.002*\"intro\" + 0.002*\"simple portfolio website ready -https\" + 0.002*\"remote web developer\" + 0.002*\"internship role\" + 0.002*\"shantanu\" + 0.002*\"month i\" + 0.002*\"career in web development\"'),\n",
       " (7,\n",
       "  '0.002*\"lol\" + 0.002*\"first interview\" + 0.002*\"-i\" + 0.002*\"fcc student\" + 0.001*\"system test\" + 0.001*\"own flaw\" + 0.001*\"zeppelin\" + 0.001*\"month since i\" + 0.001*\"dont\" + 0.001*\"previous knowledge\" + 0.001*\"edit:3\" + 0.001*\"good luck\" + 0.001*\"self / strength\" + 0.001*\"position i\" + 0.001*\"below\"'),\n",
       " (8,\n",
       "  '0.002*\"fcc front end certificate\" + 0.002*\"gulp\" + 0.002*\"accomplishment\" + 0.002*\"addition\" + 0.002*\"mind\" + 0.002*\"day\" + 0.001*\"fcc community\" + 0.001*\"way\" + 0.001*\"timeline\" + 0.001*\"log\" + 0.001*\"momentum\" + 0.001*\"front end certicate day\" + 0.001*\"react markdown app day\" + 0.001*\"coding everyday\" + 0.001*\"company interview day\"'),\n",
       " (9,\n",
       "  '0.002*\"journey on february\" + 0.002*\"email developer\" + 0.002*\"amazing platform\" + 0.002*\"previous programming knowledge\" + 0.002*\"larson\" + 0.002*\"hi everybody\" + 0.002*\"much fcc\" + 0.002*\"kind of resource\" + 0.002*\"local code camp in armenia\" + 0.002*\"web development offer\" + 0.002*\"bright day i\" + 0.002*\"ma in political science\" + 0.002*\"codacademy\" + 0.002*\"learning as i\" + 0.002*\"marketing agency\"'),\n",
       " (10,\n",
       "  '0.003*\"interview call\" + 0.003*\"developer post\" + 0.003*\"experience of real time project\" + 0.003*\"thats\" + 0.003*\"venezuela\" + 0.002*\"platform\" + 0.002*\"qa analyst\" + 0.002*\"thank\" + 0.002*\"yesterday\" + 0.002*\"coding\" + 0.002*\"part\" + 0.002*\"employer\" + 0.002*\"challenge\" + 0.002*\"quincy\" + 0.002*\"portfolio\"'),\n",
       " (11,\n",
       "  '0.003*\"second week\" + 0.003*\"t\" + 0.003*\"first frontend developer job\" + 0.003*\"xml\" + 0.003*\"new place\" + 0.003*\"first period\" + 0.003*\"xsl\" + 0.003*\"back-end certificate\" + 0.002*\"basic\" + 0.001*\"thank\" + 0.001*\"lot\" + 0.001*\"project\" + 0.001*\"interview\" + 0.000*\"lawyer\" + 0.000*\"css\"'),\n",
       " (12,\n",
       "  '0.002*\"web developer\" + 0.002*\"impressive i\" + 0.002*\"other company\" + 0.002*\"roadmap\" + 0.002*\"cv/resume\" + 0.002*\"single company that i\" + 0.002*\"student developer within day\" + 0.002*\"lot of interest\" + 0.002*\"bachelor in software technology\" + 0.002*\"company i\" + 0.002*\"object\" + 0.002*\"something\" + 0.002*\"many thanks\" + 0.002*\"reply\" + 0.002*\"closure\"'),\n",
       " (13,\n",
       "  '0.004*\"success story\" + 0.003*\"angular\" + 0.003*\"topic doesnt belong\" + 0.003*\"contributor\" + 0.003*\"school kid\" + 0.002*\"learning process\" + 0.002*\"ma\" + 0.002*\"curriculum\" + 0.002*\"developer job\" + 0.002*\"big thanks\" + 0.002*\"udemy course\" + 0.002*\"journey\" + 0.002*\"study\" + 0.002*\"article\" + 0.002*\"bar\"'),\n",
       " (14,\n",
       "  '0.003*\"college degree\" + 0.002*\"junior\" + 0.001*\"knowledge\" + 0.001*\"london\" + 0.001*\"progression opportunity\" + 0.001*\"senior devs\" + 0.001*\"structure\" + 0.001*\"cv\" + 0.001*\"final interview\" + 0.001*\"access\" + 0.001*\"informed decision\" + 0.001*\"role\" + 0.001*\"project\" + 0.001*\"https\" + 0.001*\"web-development study\"'),\n",
       " (15,\n",
       "  '0.002*\"design\" + 0.002*\"graphic design\" + 0.001*\"deliver\" + 0.001*\"hi fcc\" + 0.001*\"basic website\" + 0.001*\"new challenge i\" + 0.001*\"various company design\" + 0.001*\"basic js part\" + 0.001*\"web development diy-ing\" + 0.001*\"wordpress site\" + 0.001*\"slovenia\" + 0.001*\"textile engineering\" + 0.001*\"first job offer/trainee position\" + 0.001*\"seo optimisation…\" + 0.001*\"php from scratch tutorial\"'),\n",
       " (16,\n",
       "  '0.003*\"prototype\" + 0.002*\"full time commitment\" + 0.002*\"free time\" + 0.002*\"same day\" + 0.002*\"gaining experience\" + 0.002*\"many many job application\" + 0.002*\"helpful tip\" + 0.002*\"whoo\" + 0.002*\"little bit of code\" + 0.002*\"share with others\" + 0.002*\"couple freelance project\" + 0.002*\"other people\" + 0.002*\"good thing\" + 0.002*\"development knowledge\" + 0.002*\"master\"'),\n",
       " (17,\n",
       "  '0.003*\"time http\" + 0.002*\"only developer\" + 0.002*\"organisation\" + 0.002*\"full stack developer job offer\" + 0.002*\"app\" + 0.002*\"upwork\" + 0.002*\"fcc curriculum\" + 0.002*\"boss\" + 0.002*\"quincy\" + 0.002*\"idea\" + 0.002*\"sass\" + 0.002*\"january i\" + 0.002*\"fcc i\" + 0.001*\"graphic design\" + 0.001*\"month ago i\"'),\n",
       " (18,\n",
       "  '0.003*\"web development\" + 0.002*\"insurmountable task\" + 0.002*\"small agency\" + 0.002*\"/\" + 0.002*\"politics at university\" + 0.002*\"thing i\" + 0.002*\"first dev job\" + 0.002*\"interview on friday\" + 0.002*\"job reading that job\" + 0.002*\"hybrid mobile apps\" + 0.002*\"promising career\" + 0.002*\"web apps\" + 0.002*\"stuck\" + 0.002*\"first time i\" + 0.002*\"graduation\"'),\n",
       " (19,\n",
       "  '0.002*\"current job\" + 0.002*\"degree\" + 0.001*\"country\" + 0.001*\"internet\" + 0.001*\"college\" + 0.001*\"field\" + 0.001*\"i\\'am\" + 0.001*\"html/css\" + 0.001*\"% mechanical engineer course\" + 0.001*\"lot of calculus\" + 0.001*\"university\" + 0.001*\"prerequisite\" + 0.001*\"statement\" + 0.001*\"css questions…\" + 0.001*\"lot of different front end project\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.130*\"company\" + 0.126*\"project\" + 0.110*\"way\" + 0.108*\"interview\" + 0.105*\"day\" + 0.101*\"year\" + 0.099*\"week\" + 0.098*\"month\" + 0.098*\"job\" + 0.097*\"fcc\"'),\n",
       " (1,\n",
       "  '0.146*\"developer job\" + -0.137*\"lawyer\" + -0.127*\"thanks\" + -0.125*\"hey fellow camper\" + -0.110*\"success story\" + 0.109*\"part\" + 0.105*\"big thanks\" + 0.096*\"topic doesnt belong\" + 0.096*\"school kid\" + 0.096*\"contributor\"'),\n",
       " (2,\n",
       "  '-0.188*\"jquery\" + -0.125*\"css\" + -0.108*\"test\" + -0.104*\"html\" + -0.102*\"javascript\" + -0.098*\"time http\" + -0.096*\"experience\" + -0.093*\"cheer\" + -0.093*\"week i\" + 0.092*\"way\"'),\n",
       " (3,\n",
       "  '-0.161*\"developer job\" + -0.146*\"web development\" + -0.139*\"learning process\" + -0.136*\"big thanks\" + -0.134*\"topic doesnt belong\" + -0.134*\"contributor\" + -0.134*\"school kid\" + -0.112*\"curriculum\" + -0.110*\"community\" + -0.102*\"article\"'),\n",
       " (4,\n",
       "  '-0.167*\"web development\" + -0.143*\"month\" + 0.123*\"part\" + 0.108*\"year\" + -0.102*\"graduation\" + -0.102*\"local code camp in armenia\" + -0.102*\"guys success\" + -0.102*\"ma in political science\" + -0.102*\"august\" + -0.102*\"learning as i\"'),\n",
       " (5,\n",
       "  '0.198*\"success story\" + 0.137*\"hey fellow camper\" + 0.118*\"lawyer\" + 0.111*\"bar\" + -0.108*\"i’m\" + 0.106*\"mern stack\" + -0.094*\"jquery\" + -0.091*\"yesterday\" + 0.089*\"thanks\" + -0.087*\"portfolio\"'),\n",
       " (6,\n",
       "  '-0.130*\"prototype\" + -0.118*\"first job\" + -0.116*\"lawyer\" + 0.108*\"recruiter\" + -0.105*\"community i\" + -0.098*\"hey fellow camper\" + -0.096*\"idea\" + -0.094*\"app\" + -0.088*\"year\" + -0.087*\"good thing\"'),\n",
       " (7,\n",
       "  '0.188*\"lawyer\" + 0.151*\"hey fellow camper\" + 0.138*\"jquery\" + 0.122*\"current job\" + 0.101*\"developer job\" + 0.094*\"success story\" + 0.094*\"developer with free code camp\" + 0.094*\"medium article\" + 0.094*\"idea ataccama com i\" + -0.090*\"month\"'),\n",
       " (8,\n",
       "  '-0.129*\"life\" + 0.124*\"success story\" + -0.114*\"half\" + -0.112*\"failure\" + -0.095*\"graphic design\" + 0.087*\"developer job\" + -0.086*\"developer\" + -0.082*\"junior\" + -0.081*\"time http\" + 0.079*\"community\"'),\n",
       " (9,\n",
       "  '-0.121*\"thanks\" + 0.107*\"lawyer\" + -0.097*\"developer job\" + -0.092*\"idea\" + 0.087*\"failure\" + -0.082*\"https\" + -0.081*\"technology\" + -0.080*\"lot of interest\" + -0.080*\"other company\" + -0.080*\"bachelor in software technology\"'),\n",
       " (10,\n",
       "  '-0.107*\"yesterday\" + 0.105*\"developer job\" + -0.101*\"quincy\" + -0.087*\"organisation\" + -0.087*\"full stack developer job offer\" + -0.082*\"future\" + -0.082*\"experience of real time project thank\" + -0.082*\"interview call i\" + -0.082*\"developer post\" + -0.082*\"week i\"'),\n",
       " (11,\n",
       "  '0.115*\"i’m\" + 0.103*\"only developer\" + -0.102*\"developer job\" + 0.100*\"start\" + -0.099*\"topic doesnt belong\" + -0.099*\"school kid\" + -0.099*\"contributor\" + 0.095*\"idea\" + -0.094*\"learning process\" + 0.093*\"self taught through fcc\"'),\n",
       " (12,\n",
       "  '0.115*\"self taught through fcc\" + 0.115*\"year time\" + 0.115*\"type of question\" + 0.115*\"front end developer job today skill\" + -0.115*\"venezuela\" + -0.108*\"experience of real time project thank\" + -0.108*\"developer post\" + -0.108*\"interview call i\" + -0.103*\"home\" + -0.099*\"employer\"'),\n",
       " (13,\n",
       "  '-0.112*\"mind\" + -0.092*\"experience of real time project thank\" + -0.092*\"interview call i\" + -0.092*\"developer post\" + 0.092*\"venezuela\" + -0.090*\"part\" + -0.085*\"goal\" + -0.082*\"platform\" + -0.076*\"path\" + -0.076*\"cliche\"'),\n",
       " (14,\n",
       "  '-0.129*\"time http\" + 0.121*\"accomplishment\" + -0.120*\"graphic design\" + 0.115*\"lawyer\" + 0.112*\"gulp\" + 0.088*\"react\" + -0.088*\"january i\" + 0.080*\"thought\" + 0.075*\"challenge\" + 0.072*\"today\"'),\n",
       " (15,\n",
       "  '-0.156*\"platform\" + -0.153*\"interview call i\" + -0.153*\"developer post\" + -0.153*\"experience of real time project thank\" + -0.117*\"lawyer\" + 0.104*\"bar\" + -0.096*\"full time job\" + 0.091*\"internet\" + -0.090*\"portfolio\" + -0.087*\"yesterday\"'),\n",
       " (16,\n",
       "  '0.118*\"year time\" + 0.118*\"front end developer job today skill\" + 0.118*\"type of question\" + 0.118*\"self taught through fcc\" + 0.110*\"start\" + -0.103*\"life\" + -0.099*\"half\" + -0.095*\"accomplishment\" + -0.091*\"failure\" + 0.090*\"interview i\"'),\n",
       " (17,\n",
       "  '-0.159*\"developer post\" + -0.159*\"interview call i\" + -0.159*\"experience of real time project thank\" + -0.121*\"yesterday\" + 0.114*\"venezuela\" + -0.110*\"platform\" + -0.102*\"employer\" + 0.095*\"college\" + -0.094*\"year time\" + -0.094*\"self taught through fcc\"'),\n",
       " (18,\n",
       "  '-0.128*\"i’ll\" + -0.126*\"first period\" + -0.126*\"xsl\" + -0.126*\"first frontend developer job\" + -0.126*\"interview thank\" + -0.126*\"second week\" + -0.126*\"use xml\" + -0.126*\"new place\" + -0.115*\"end certificate\" + -0.115*\"reply\"'),\n",
       " (19,\n",
       "  '0.121*\"interview process\" + -0.092*\"first frontend developer job\" + -0.092*\"interview thank\" + -0.092*\"use xml\" + -0.092*\"second week\" + -0.092*\"xsl\" + -0.092*\"new place\" + -0.092*\"first period\" + -0.087*\"object\" + 0.084*\"reply\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## raw_lda_frankjupyter2: modified to fit allrecordsPreparation2\n",
    "############################\n",
    "def raw_lda_frankjupyter2(lemmposrecs, wordimportance, metmodel=2, STOPWORDS=nltk.corpus.stopwords.words('english')):\n",
    "    '''\n",
    "    description: modified model based on https://www.frankcleary.com/svd/ for a more raw construction of a lda\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    def metriccalc(st, normalizer, wordimportance):\n",
    "        '''\n",
    "        description:\n",
    "        text normalization based on ALL characters in the sentence; why? Example: if two writers wrote 20 words, 2 of them very important, but one of them wrote half of characters stopwords, those 2 words wouldnt be penalized accordingly for this writer: the other wrote more important content\n",
    "        '''       \n",
    "        metfuncs = [\n",
    "            lambda w: math.pow(0.1+float(wordimportance[w]),textbow[w]/normalizer) if w in list(wordimportance.keys()) else 0.0, #a sort of idf-normalization based on number of words in the text: the more the words in a text, the more important\n",
    "            lambda w: float(wordimportance[w])*textbow[w] if w in list(wordimportance.keys()) else 0.0, #good but ignore those words with worimportance too low or 0 but that are frequent in text\n",
    "            lambda w: 1.0+2.0**float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- probably the best one; because it is not normilized this indicator would simply say that if it has the word at least once is already on topic\n",
    "            lambda w: float(wordimportance[w]) + textbow[w]/normalizer if w in list(wordimportance.keys()) else 0.0,\n",
    "            lambda w: (1.0+textbow[w]/normalizer)*float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- apparently the second best with the adhoc word ranking\n",
    "            lambda w: normalizer/textbow[w]*wordimportance[w]/sum(list(wordimportance.values())), #tfidf-ish\n",
    "            lambda w: float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0\n",
    "        ]\n",
    "        likedict = collections.defaultdict(float)\n",
    "        textbow = collections.Counter(st)\n",
    "        for w in st:\n",
    "            likedict[w] = metfuncs[metmodel](w)\n",
    "        return likedict\n",
    "\n",
    "    #redo_corpus_by_sts = []\n",
    "    words_df = pandas.DataFrame()\n",
    "    textreference = {}\n",
    "    \n",
    "    \n",
    "    for textindex, lemmpos_r in enumerate(lemmposrecs):\n",
    "        lemmpos_t = lemmpos_r[3]\n",
    "        #print('lemmpos_t', len(lemmpos_t))\n",
    "        lemm_sts = ''\n",
    "        for lemmpos_TUPLE in lemmpos_t:\n",
    "            w = lemmpos_TUPLE[2]\n",
    "            lemm_sts = lemm_sts + w + ' '\n",
    "        sts = lemm_sts.split('.')\n",
    "        for stindex, lemmpos_st in enumerate(sts):\n",
    "            treated_st, lensts = cleaningtext(lemmpos_st.split(), STOPWORDS=STOPWORDS)        \n",
    "            #print('treated_st', lensts)\n",
    "            if len(treated_st) > 3:\n",
    "                likedict = metriccalc(treated_st, lensts, wordimportance)\n",
    "                st_df = pandas.DataFrame.from_dict(likedict, orient='index')\n",
    "                textindexing = str(textindex)+'_'+str(stindex)\n",
    "                st_df.columns = [textindexing]\n",
    "                textreference[textindexing] = {}\n",
    "                textreference[textindexing]['treated_st'] = treated_st\n",
    "                #st_df.columns = [str(count)]\n",
    "                words_df = words_df.join(st_df, how='outer', )\n",
    "    \n",
    "    words_df = words_df.fillna(0)\n",
    "    print(\"Number of unique words: %s\" % len(words_df))\n",
    "    print(words_df.head(10))\n",
    "    #print(words_df.sort(columns=words_df.columns[0], ascending=False).head(10))\n",
    "    \n",
    "    return words_df, textreference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
