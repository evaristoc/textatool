{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/derekgreene/topic-model-tutorial \n",
    "* https://github.com/derekgreene/topic-model-tutorial/blob/master/topic-modelling-with-scikitlearn.pdf\n",
    "* https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n",
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d80b2206-d953-46b3-b807-2bf5a8557d28"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    def createcands(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        #print(list(group)[::-1])\n",
    "        reversing = [None]\n",
    "        #for el in group:\n",
    "        #    print(el)\n",
    "        for el in group:\n",
    "            if reversing[0] == None:\n",
    "                reversing[0] = el[0]\n",
    "            else:\n",
    "                reversing.insert(0, el[0])\n",
    "        #print(reversing)\n",
    "        for word in reversing:\n",
    "            if current == '':\n",
    "                current = word\n",
    "            else:\n",
    "                current = word + ' ' + current\n",
    "            total.append(current)\n",
    "        #print(list(set(reversing+total)))\n",
    "        return list(set(reversing+total))\n",
    "    \n",
    "    def createcands2(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        for word, pos, chunk in list(group):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            #print(word)\n",
    "            if current == '':\n",
    "                current = word\n",
    "                total.append(word)\n",
    "            else:\n",
    "                current = current+' '+word\n",
    "                total.append(word)\n",
    "        total.append(current)\n",
    "\n",
    "        return list(set(total))\n",
    "            \n",
    "    #candidates = [' '.join(word for word, pos, chunk in group)\n",
    "    candidates = [createcands2(group)\n",
    "                    for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "    #print(candidates)\n",
    "\n",
    "    #return [cand for cand in candidates]\n",
    "    #print(list(itertools.chain.from_iterable(candidates)))\n",
    "    return list(itertools.chain.from_iterable(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        pattern_A01 = re.compile(r'^hey( |$)')\n",
    "        \n",
    "        ## Text extraction, REGEXs and Replacements\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text() \\\n",
    "                                                        .lower() \\\n",
    "                                                        .replace('’',\"'\") \\\n",
    "                                                        .replace('‘', \"'\") \\\n",
    "                                                        .replace('´', \"'\") \\\n",
    "                                                        .replace('free code camp', 'fcc') \\\n",
    "                                                        .replace('javascript', 'js') \\\n",
    "                                                        .replace('part time', 'parttime') \\\n",
    "                                                        .replace('part-time', 'parttime') \\\n",
    "                                                        .replace('full time', 'fulltime') \\\n",
    "                                                        .replace('full-time', 'fulltime') \\\n",
    "                                                        .replace('web application', 'web app') \\\n",
    "                                                        .replace('web development', 'web dev') \\\n",
    "                                                        .replace('developer', 'dev') \\\n",
    "                                                        .replace('front end', 'frontend') \\\n",
    "                                                        .replace('front-end','frontend') \\\n",
    "                                                        .replace('back end', 'backend') \\\n",
    "                                                        .replace('back-end','backend') \\\n",
    "                                                        .replace('full stack', 'fullstack') \\\n",
    "                                                        .replace('full-stack','fullstack') \\\n",
    "                                                        .replace('resume', 'cv') \\\n",
    "                                                        .replace('hello', 'hi') \\\n",
    "                                                        .replace(' angular ', ' angularjs ')\n",
    "        \n",
    "        soup_forumpostTEXT = re.sub(pattern_A01, ' hello ', soup_forumpostTEXT)\n",
    "        \n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    #print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    #print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                #print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f8e459ff-ae34-48aa-8146-50365df9ea53"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var4: wordimportance_var3 modified to fit keyphrases; changes in the \"opacity\" formula\n",
    "############################\n",
    "def wordimportance_var4(lemmposrecs, lemmws_fd):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    unigrams = dict([(grams, count) \n",
    "                     for grams, count in lemmws_fd.items() \n",
    "                     if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                    ])\n",
    "    maxdiv = math.log(sorted(unigrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "    opacity = collections.defaultdict(float)\n",
    "    for grams, counts in lemmws_fd.items():\n",
    "        opval = []\n",
    "        for gram in grams.split():\n",
    "            if gram == '':\n",
    "                continue\n",
    "            if gram in unigrams:\n",
    "                opval.append(math.log(unigrams[gram]))\n",
    "            else:\n",
    "                opval.append(0)\n",
    "        averopval = sum(opval)/len(opval)\n",
    "        if 1 - averopval/maxdiv != 0.0:\n",
    "            opacity[grams] = 1 - averopval/maxdiv\n",
    "        else:\n",
    "            opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "        \n",
    "    sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "\n",
    "    ## Count lemmatized words/characters per text  \n",
    "    for i,lemmpos_t in enumerate(lemmposrecs):\n",
    "        for k, lemmpos_TUPLE in enumerate(lemmpos_t[3]):\n",
    "            ## Use lemmatized word\n",
    "            lemmw = lemmpos_TUPLE[2]\n",
    "            sizing_matrix[lemmw][i] = sizing_matrix[lemmw][i] + 1\n",
    "        for cand in lemmpos_t[4]: #<--------------------------------- problem!! it counted candidates only once!!!\n",
    "            if cand not in sizing_matrix:\n",
    "                sizing_matrix[cand][i] = sizing_matrix[cand][i] + 1\n",
    "\n",
    "       \n",
    "    ## Normalization\n",
    "    normalization = dict([(k, (sum(vector)-max(vector))/sum(vector)) if sum(vector) != 0 else (k,0.0) for k, vector in sizing_matrix.items()])\n",
    "    #normalization = collections.defaultdict(float)\n",
    "    #for k, vector in sizing_matrix.items():\n",
    "    #    if sum(vector) != 0:\n",
    "    #        normalization[k] = (sum(vector)-max(vector))/sum(vector)\n",
    "    #    else:\n",
    "    #        \n",
    "        \n",
    "    \n",
    "    wordimportance = dict([(k, valnorm*opacity[k]) for k, valnorm in normalization.items()])\n",
    "\n",
    "    return wordimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a"
    }
   },
   "outputs": [],
   "source": [
    "def cleaningtext(st, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    treated_st = []\n",
    "    countwds = len(st)\n",
    "    for w in st:\n",
    "        if 'freecodecamp' in w:\n",
    "            w = w.replace('freecodecamp','fcc')\n",
    "        treated_st.append(w)\n",
    "    return treated_st, countwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "59c7646c-3944-4e2c-81a1-1728a02396ae"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## gensim_models2 re-evaluated: modification of gensim_model2's cleanedsts_from_lemmpostxts to fit keyphrases\n",
    "############################\n",
    "\n",
    "def gensim_models2(lemmposrecs, NUM_TOPICS = 15, lemmws_fd = {}, wordimportance = {}, nltk = nltk, gensim = gensim):\n",
    "    \n",
    "    def cleanedsts_from_lemmpostxts2(lemmposrecs, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "        redo_corpus_by_sts = []\n",
    "        for lemmpos_r in lemmposrecs:\n",
    "            candidates = lemmpos_r[4]\n",
    "            treated_lemmcands = cleaningtext(candidates)[0] #passing candidates only\n",
    "            redo_corpus_by_sts.append(treated_lemmcands) \n",
    "        \n",
    "        #print(len(redo_corpus_by_sts), redo_corpus_by_sts[-1])\n",
    "        return redo_corpus_by_sts\n",
    "        \n",
    "    def basedonBOW(redo_corpus_by_sts):\n",
    "        dictionary = gensim.corpora.Dictionary(redo_corpus_by_sts) #[token for st in redo_corpus_by_sts for token in st]\n",
    "        corpus = [dictionary.doc2bow(text) for text in redo_corpus_by_sts]\n",
    "        return corpus, dictionary\n",
    "    \n",
    "    def basedonTFIDF(corpus):\n",
    "        return gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    def basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance):\n",
    "        \n",
    "        def metriccalc(w):\n",
    "            if w in wordimportance:\n",
    "                return 1.0+2.0**float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        corpus = []\n",
    "        for sts in redo_corpus_by_sts:\n",
    "            st = []\n",
    "            for w in sts:\n",
    "                st.append((dictionary.token2id[w], metriccalc(w)))\n",
    "            corpus.append(st)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    redo_corpus_by_sts  = cleanedsts_from_lemmpostxts2(lemmposrecs)\n",
    "    \n",
    "    corpus, dictionary = basedonBOW(redo_corpus_by_sts)\n",
    "    if wordimportance == {'tfidf':True}:\n",
    "        tfidf = basedonTFIDF(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "    if wordimportance != {} and wordimportance != {'tfidf':True}:\n",
    "        corpus = basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance)\n",
    "    \n",
    "    \n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, minimum_probability=0.005, iterations=100, per_word_topics = True, minimum_phi_value = 0.001, id2word=dictionary)\n",
    "    lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "   \n",
    "    return lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e294b910-1a25-4e48-abb5-37239f441f2e"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d236a59f-ff21-406d-b1df-9436aedbdb11"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of treated posts (len(count)) :: 53\n",
      "lemm errs: 19296\n"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, \n",
    "#                                                                                NUM_TOPICS=20,\n",
    "#                                                                                wordimportance = {'tfidf':True})\n",
    "\n",
    "#lda_model.print_topics(num_words=15)\n",
    "\n",
    "#[' '.join([l for wr in rec[3] for w,_,l,pos in wr]) for rec in lemmposrecs]\n",
    "#[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs]\n",
    "#[' '.join([cand for cand in rec[4]]) for rec in lemmposrecs]\n",
    "##https://stackoverflow.com/questions/46282473/error-while-identify-the-coherence-value-from-lda-model\n",
    "#texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "#texts\n",
    "\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                                                  texts=texts,\n",
    "#                                                                  #corpus=corpus,\n",
    "#                                                                  window_size=20,\n",
    "#                                                                  dictionary=dictionary, \n",
    "#                                                                  coherence='c_uci')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, \n",
    "#                                                   texts=texts, \n",
    "#                                                   dictionary=dictionary,\n",
    "#                                                   coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "## Compute Coherence Score using UMass\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "#                                     corpus = corpus,\n",
    "#                                     dictionary=dictionary, \n",
    "#                                     coherence=\"u_mass\")\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#dictionary\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TOPICS = range(2,61,2)\n",
    "cv_coherence_values = []\n",
    "for numtopics in TOPICS:\n",
    "    print('\\n\\nFor NUM_TOPICS:', numtopics)\n",
    "    lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=numtopics, wordimportance = {'tfidf':True})\n",
    "    umass_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "                                     corpus = corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"u_mass\")\n",
    "    umass_coherence_lda = umass_coherence_model_lda.get_coherence()\n",
    "    print('U-Mass Coherence Score: ', umass_coherence_lda)\n",
    "    texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "    cv_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     texts=texts,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"c_v\")\n",
    "    cv_coherence_lda = cv_coherence_model_lda.get_coherence()\n",
    "    cv_coherence_values.append(cv_coherence_lda)\n",
    "    print('C_V Coherence Score: ', cv_coherence_lda)\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.quora.com/Can-I-combine-LSI-and-K-means-for-text-document-clustering-Are-there-any-sources-to-learn-about-it\n",
    "#http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##https://stackoverflow.com/questions/14261903/how-can-i-open-the-interactive-matplotlib-window-in-ipython-notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "limit=61; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, cv_coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=40, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.print_topics(num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.124*\"project\" + 0.100*\"company\" + 0.097*\"interview\" + 0.097*\"people\" + 0.094*\"lot\" + 0.093*\"code\" + 0.092*\"way\" + 0.092*\"app\" + 0.086*\"cv\" + 0.086*\"skill\" + 0.085*\"day\" + 0.084*\"time\" + 0.079*\"experience\" + 0.078*\"web\" + 0.078*\"recruiter\"'),\n",
       " (1,\n",
       "  '-0.226*\"lawyer\" + -0.224*\"fellow camper\" + -0.224*\"fellow\" + -0.168*\"success\" + -0.126*\"story\" + -0.121*\"camper\" + -0.113*\"ideas.ataccama.com\" + -0.113*\"satisfied\" + -0.113*\"dev fcc\" + -0.113*\"satisfied current job\" + -0.107*\"internship\" + -0.105*\"medium article\" + -0.101*\"proud\" + -0.101*\"friend local fcc group everyone\" + -0.101*\"success story fcc forum\"'),\n",
       " (2,\n",
       "  '-0.174*\"design\" + -0.124*\"graphic\" + -0.114*\"internship\" + 0.093*\"github\" + 0.092*\"manager\" + 0.088*\"app\" + 0.088*\"recruiter\" + 0.079*\"source\" + 0.076*\"open\" + -0.074*\"lawyer\" + -0.071*\"agency\" + 0.070*\"city\" + -0.070*\"role\" + -0.069*\"hi\" + 0.068*\"company\"'),\n",
       " (3,\n",
       "  '-0.113*\"taught\" + -0.104*\"app\" + -0.102*\"css js\" + -0.101*\"study\" + -0.100*\"adwords\" + -0.100*\"logic\" + -0.096*\"lol\" + -0.093*\"degree\" + -0.090*\"self taught fcc\" + -0.090*\"year time\" + -0.090*\"interview frontend dev job today\" + -0.090*\"idea type question\" + -0.088*\"skill\" + -0.085*\"college\" + 0.084*\"phone\"'),\n",
       " (4,\n",
       "  '0.133*\"technology\" + -0.128*\"thank\" + 0.127*\"role\" + 0.125*\"lawyer\" + 0.113*\"current\" + -0.105*\"backend\" + 0.098*\"student\" + -0.094*\"web dev\" + 0.093*\"cv/cv\" + 0.093*\"bachelor software technology\" + 0.093*\"student dev within day company\" + 0.093*\"roadmap\" + 0.093*\"lot interest technology roadmap\" + 0.093*\"single company\" + 0.089*\"many thanks\"'),\n",
       " (5,\n",
       "  '-0.109*\"science\" + 0.105*\"jquery\" + 0.105*\"internship\" + -0.097*\"dream\" + 0.086*\"app\" + 0.083*\"thought\" + 0.082*\"html\" + 0.080*\"logic\" + -0.079*\"life\" + 0.077*\"study\" + -0.075*\"sleep\" + 0.075*\"github\" + 0.074*\"fellow\" + 0.074*\"fellow camper\" + 0.071*\"lawyer\"'),\n",
       " (6,\n",
       "  '0.130*\"lawyer\" + -0.125*\"quincy\" + -0.121*\"qa\" + -0.104*\"thats\" + -0.103*\"course\" + -0.099*\"certificate\" + -0.091*\"analyst\" + -0.087*\"thanks\" + -0.087*\"offer\" + -0.085*\"assignment\" + -0.084*\"organisation\" + -0.084*\"codecs\" + -0.084*\"fullstack dev job offer\" + -0.083*\"frontend dev\" + -0.075*\"success\"'),\n",
       " (7,\n",
       "  '-0.093*\"group\" + 0.090*\"dev job\" + 0.084*\"freelance\" + 0.082*\"css js\" + -0.079*\"role\" + 0.078*\"qa\" + 0.076*\"idea\" + 0.073*\"self taught fcc\" + 0.073*\"interview frontend dev job today\" + 0.073*\"idea type question\" + 0.073*\"year time\" + -0.072*\"people\" + 0.072*\"email\" + -0.071*\"challenge\" + 0.070*\"internship\"'),\n",
       " (8,\n",
       "  '0.107*\"jquery\" + -0.106*\"email\" + -0.106*\"hi everybody\" + -0.106*\"everybody\" + -0.100*\"larson\" + -0.100*\"first job frontend email dev digital marketing agency\" + -0.100*\"previous programming knowledge\" + -0.100*\"thank much fcc\" + -0.100*\"amazing platform community\" + -0.100*\"journey february\" + -0.087*\"february\" + 0.084*\"time http\" + 0.081*\"graphic\" + 0.078*\"student\" + -0.075*\"quincy\"'),\n",
       " (9,\n",
       "  '-0.095*\"graduation\" + -0.082*\"interview process\" + 0.081*\"lawyer\" + -0.079*\"bar\" + -0.078*\"drunk\" + -0.074*\"email\" + 0.072*\"experience real time project\" + 0.072*\"happy part platform\" + 0.072*\"interview call\" + 0.072*\"portfolio employer\" + 0.072*\"dev post yesterday\" + -0.071*\"friday\" + -0.070*\"promising\" + -0.070*\"politics\" + -0.070*\"politics university\"'),\n",
       " (10,\n",
       "  '-0.101*\"dev job\" + -0.097*\"community big thanks\" + -0.097*\"doesnt\" + -0.097*\"belong\" + -0.097*\"part learning process\" + -0.097*\"school kid\" + -0.097*\"topic doesnt belong\" + -0.097*\"contributor\" + -0.097*\"article share story resource\" + -0.094*\"recruiter\" + -0.093*\"kid\" + 0.081*\"team\" + 0.080*\"everybody\" + 0.080*\"hi everybody\" + 0.080*\"employer\"'),\n",
       " (11,\n",
       "  '0.183*\"internship\" + 0.104*\"jquery\" + 0.088*\"remote\" + -0.084*\"github\" + 0.077*\"data\" + 0.076*\"machine\" + -0.073*\"technology\" + 0.072*\"skill\" + 0.072*\"upwork\" + 0.072*\"future\" + 0.071*\"pay\" + -0.069*\"web dev\" + 0.068*\"udemy\" + 0.067*\"cheer\" + 0.067*\"ready\"'),\n",
       " (12,\n",
       "  '0.116*\"venezuela\" + 0.108*\"freelance\" + 0.105*\"internship\" + 0.101*\"college\" + -0.096*\"data\" + -0.095*\"self taught fcc\" + -0.095*\"interview frontend dev job today\" + -0.095*\"idea type question\" + -0.095*\"year time\" + -0.090*\"taught\" + -0.087*\"machine\" + -0.084*\"skill\" + -0.083*\"start\" + -0.081*\"friend\" + 0.079*\"day\"'),\n",
       " (13,\n",
       "  '0.126*\"year half\" + -0.112*\"design\" + 0.111*\"internship\" + 0.109*\"failure\" + -0.095*\"student\" + 0.092*\"intro\" + 0.091*\"jquery\" + 0.086*\"process\" + -0.084*\"css js\" + 0.079*\"right\" + -0.079*\"graphic\" + -0.077*\"lol\" + 0.077*\"path\" + 0.076*\"couple month\" + -0.070*\"interview frontend dev job today\"'),\n",
       " (14,\n",
       "  '0.164*\"backend certificate\" + 0.164*\"first frontend dev job\" + 0.164*\"xml\" + 0.164*\"second week new place\" + 0.164*\"first period\" + 0.164*\"xsl\" + 0.142*\"backend\" + 0.136*\"venezuela\" + 0.120*\"period\" + 0.111*\"place\" + 0.104*\"experience real time project\" + 0.104*\"dev post yesterday\" + 0.104*\"happy part platform\" + 0.104*\"portfolio employer\" + 0.104*\"interview call\"'),\n",
       " (15,\n",
       "  '0.114*\"half\" + -0.106*\"self\" + 0.105*\"adwords\" + 0.099*\"agency\" + -0.091*\"taught\" + 0.086*\"year half\" + 0.080*\"app\" + 0.080*\"salary\" + -0.077*\"right\" + -0.069*\"math\" + 0.067*\"failure\" + -0.067*\"logic\" + -0.066*\"idea type question\" + -0.066*\"interview frontend dev job today\" + -0.066*\"year time\"'),\n",
       " (16,\n",
       "  '0.141*\"belong\" + 0.141*\"doesnt\" + 0.141*\"community big thanks\" + 0.141*\"part learning process\" + 0.141*\"school kid\" + 0.141*\"article share story resource\" + 0.141*\"topic doesnt belong\" + 0.141*\"contributor\" + 0.116*\"dev job\" + -0.111*\"taught\" + -0.108*\"self taught fcc\" + -0.108*\"interview frontend dev job today\" + -0.108*\"idea type question\" + -0.108*\"year time\" + 0.104*\"kid\"'),\n",
       " (17,\n",
       "  '0.136*\"logic\" + 0.121*\"math\" + -0.112*\"adwords\" + -0.106*\"internship\" + 0.089*\"energy\" + 0.084*\"topic\" + -0.082*\"web dev\" + 0.080*\"tribute\" + 0.080*\"trainer\" + 0.080*\"advance logic\" + 0.080*\"advance\" + 0.079*\"contributor\" + 0.079*\"article share story resource\" + 0.079*\"community big thanks\" + 0.079*\"doesnt\"'),\n",
       " (18,\n",
       "  '0.137*\"student\" + 0.133*\"student dev within day company\" + 0.133*\"single company\" + 0.133*\"roadmap\" + 0.133*\"lot interest technology roadmap\" + 0.133*\"cv/cv\" + 0.133*\"bachelor software technology\" + 0.125*\"within\" + 0.106*\"technology\" + 0.102*\"many thanks\" + 0.099*\"thanks\" + 0.097*\"reply\" + -0.090*\"group\" + -0.086*\"lawyer\" + 0.084*\"single\"'),\n",
       " (19,\n",
       "  '0.128*\"qa\" + -0.113*\"recruiter\" + 0.096*\"analyst\" + -0.084*\"udacity\" + 0.081*\"employer\" + -0.081*\"http\" + 0.080*\"github\" + -0.078*\"json\" + -0.078*\"previous programming knowledge\" + -0.078*\"journey february\" + -0.078*\"thank much fcc\" + -0.078*\"first job frontend email dev digital marketing agency\" + -0.078*\"larson\" + -0.078*\"amazing platform community\" + -0.078*\"digital\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=30, num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"machine\" + 0.001*\"much time learning\" + 0.001*\"month graduation\" + 0.001*\"local code camp armenia\" + 0.001*\"armenia\" + 0.001*\"bright day\" + 0.001*\"codacademy\" + 0.001*\"bright\" + 0.001*\"political science\" + 0.001*\"web dev offer\" + 0.001*\"good kind resource\" + 0.001*\"science\" + 0.001*\"graduation\" + 0.001*\"political\" + 0.001*\"new job\" + 0.001*\"pay\" + 0.001*\"camp\" + 0.001*\"profession\" + 0.001*\"backend\" + 0.001*\"everyone\"'),\n",
       " (1,\n",
       "  '0.002*\"github\" + 0.002*\"thats\" + 0.001*\"everyday\" + 0.001*\"frontend dev\" + 0.001*\"july\" + 0.001*\"certificate\" + 0.001*\"challenge\" + 0.001*\"gulp\" + 0.001*\"day\" + 0.001*\"react\" + 0.001*\"right\" + 0.001*\"accomplishment\" + 0.001*\"source\" + 0.001*\"blog\" + 0.001*\"open\" + 0.001*\"//manuelbasanta.github.io/\" + 0.001*\"cs50\" + 0.001*\"awsome course\" + 0.001*\"data course\" + 0.001*\"course amazing it´s inttoductory\"'),\n",
       " (2,\n",
       "  '0.002*\"design\" + 0.001*\"graphic\" + 0.001*\"sleep\" + 0.001*\"friday\" + 0.001*\"mind\" + 0.001*\"site\" + 0.001*\"student\" + 0.001*\"meetup\" + 0.001*\"recruiter\" + 0.001*\"dream\" + 0.001*\"optimistic\" + 0.001*\"politics\" + 0.001*\"promising\" + 0.001*\"promising career\" + 0.001*\"politics university\" + 0.001*\"story life\" + 0.001*\"idea graduation\" + 0.001*\"web apps hybrid mobile apps\" + 0.001*\"something stuck\" + 0.001*\"first time\"'),\n",
       " (3,\n",
       "  '0.002*\"group\" + 0.001*\"prototype\" + 0.001*\"portal\" + 0.001*\"good thing\" + 0.001*\"daughter\" + 0.001*\"weekend\" + 0.001*\"person\" + 0.001*\"app\" + 0.001*\"wednesday\" + 0.001*\"local fcc group\" + 0.001*\"office clerk\" + 0.001*\"clerk\" + 0.001*\"love concept\" + 0.001*\"real app\" + 0.001*\"3-year-old daughter\" + 0.001*\"data visualization certification\" + 0.001*\"startup company\" + 0.001*\"circle\" + 0.001*\"intern\" + 0.001*\"io\"'),\n",
       " (4,\n",
       "  '0.001*\"udacity\" + 0.001*\"json\" + 0.001*\"lead\" + 0.001*\"linkedin\" + 0.001*\"designer\" + 0.001*\"recruiter\" + 0.001*\"church\" + 0.001*\"json tree\" + 0.001*\"cosmopolitan\" + 0.001*\"academy\" + 0.001*\"director\" + 0.001*\"tree\" + 0.001*\"try\" + 0.001*\"track\" + 0.001*\"solution\" + 0.001*\"fact\" + 0.001*\"area\" + 0.001*\"top\" + 0.001*\"program\" + 0.001*\"tip\"'),\n",
       " (5,\n",
       "  '0.001*\"junior\" + 0.001*\"london\" + 0.001*\"building\" + 0.001*\"ability\" + 0.001*\"decision\" + 0.001*\"take\" + 0.001*\"box\" + 0.001*\"access\" + 0.001*\"factor\" + 0.001*\"progression opportunity\" + 0.001*\"progression\" + 0.001*\"final interview\" + 0.001*\"final\" + 0.001*\"informed\" + 0.001*\"senior\" + 0.001*\"etc\" + 0.001*\"role\" + 0.001*\"skill\" + 0.001*\"sale\" + 0.001*\"management\"'),\n",
       " (6,\n",
       "  '0.002*\"dev post yesterday\" + 0.002*\"portfolio employer\" + 0.002*\"interview call\" + 0.002*\"experience real time project\" + 0.002*\"happy part platform\" + 0.001*\"call\" + 0.001*\"recruiter\" + 0.001*\"platform\" + 0.001*\"happy\" + 0.001*\"yesterday\" + 0.001*\"similar\" + 0.001*\"salary\" + 0.001*\"employer\" + 0.001*\"moment\" + 0.001*\"project\" + 0.001*\"puzzle\" + 0.001*\"english\" + 0.001*\"fresh\" + 0.001*\"webmaster\" + 0.001*\"universe\"'),\n",
       " (7,\n",
       "  '0.002*\"year half\" + 0.002*\"upwork\" + 0.002*\"failure\" + 0.001*\"half\" + 0.001*\"vp\" + 0.001*\"view\" + 0.001*\"profile\" + 0.001*\"client\" + 0.001*\"word motivation\" + 0.001*\"failure part process\" + 0.001*\"sacrifice\" + 0.001*\"posting\" + 0.001*\"interview offer\" + 0.001*\"job posting\" + 0.001*\"article “how\" + 0.001*\"frontend web dev job\" + 0.001*\"fcc”\" + 0.001*\"web dev fcc”\" + 0.001*\"last project\" + 0.001*\"“how\"'),\n",
       " (8,\n",
       "  '0.002*\"internship\" + 0.001*\"udemy\" + 0.001*\"drunk\" + 0.001*\"bar\" + 0.001*\"remote\" + 0.001*\"true\" + 0.001*\"test well.and\" + 0.001*\"supportive community\" + 0.001*\"edit:3. interview\" + 0.001*\"edit:3.\" + 0.001*\"dont\" + 0.001*\"answer system test\" + 0.001*\"big thank\" + 0.001*\"thank good luck\" + 0.001*\"\\'yes\" + 0.001*\"supportive\" + 0.001*\"list stuff\" + 0.001*\"fcc one\" + 0.001*\"course true question\" + 0.001*\"zeppelin\"'),\n",
       " (9,\n",
       "  '0.001*\"degree\" + 0.001*\"thing lot\" + 0.001*\"university\" + 0.001*\"taught\" + 0.001*\"current\" + 0.001*\"country\" + 0.001*\"internet\" + 0.001*\"lot\" + 0.001*\"without\" + 0.001*\"resource\" + 0.001*\"field\" + 0.001*\"science\" + 0.001*\"self\" + 0.001*\"uneducated pos- high school\" + 0.001*\"\\'cs\" + 0.001*\"dollars/hours\" + 0.001*\"ticket\" + 0.001*\"“our\" + 0.001*\"kind mentality internet place\" + 0.001*\"subclass\"'),\n",
       " (10,\n",
       "  '0.002*\"venezuela\" + 0.002*\"self taught fcc\" + 0.002*\"year time\" + 0.002*\"interview frontend dev job today\" + 0.002*\"idea type question\" + 0.002*\"start\" + 0.002*\"taught\" + 0.002*\"css js\" + 0.001*\"freelance\" + 0.001*\"recruitment\" + 0.001*\"type\" + 0.001*\"idea\" + 0.001*\"js\" + 0.001*\"half\" + 0.001*\"self\" + 0.001*\"call guy\" + 0.001*\"night hour spent reading coding\" + 0.001*\"freelance job week\" + 0.001*\"obligation\" + 0.001*\"local web dev job\"'),\n",
       " (11,\n",
       "  '0.002*\"school kid\" + 0.002*\"contributor\" + 0.002*\"doesnt\" + 0.002*\"part learning process\" + 0.002*\"topic doesnt belong\" + 0.002*\"belong\" + 0.002*\"community big thanks\" + 0.002*\"article share story resource\" + 0.001*\"kid\" + 0.001*\"dev job\" + 0.001*\"topic\" + 0.001*\"school\" + 0.001*\"article\" + 0.001*\"big\" + 0.001*\"resource\" + 0.001*\"process\" + 0.001*\"share\" + 0.001*\"curriculum\" + 0.001*\"part\" + 0.001*\"story\"'),\n",
       " (12,\n",
       "  '0.002*\"single company\" + 0.002*\"roadmap\" + 0.002*\"student dev within day company\" + 0.002*\"bachelor software technology\" + 0.002*\"cv/cv\" + 0.002*\"lot interest technology roadmap\" + 0.001*\"student\" + 0.001*\"many thanks\" + 0.001*\"within\" + 0.001*\"technology\" + 0.001*\"single\" + 0.001*\"reply\" + 0.001*\"thanks\" + 0.001*\"fcc project\" + 0.001*\"bachelor\" + 0.001*\"software\" + 0.001*\"company\" + 0.001*\"interest\" + 0.001*\"application\" + 0.001*\"something\"'),\n",
       " (13,\n",
       "  '0.002*\"time http\" + 0.002*\"http\" + 0.001*\"weekly\" + 0.001*\"event\" + 0.001*\"graphic design year\" + 0.001*\"tracker\" + 0.001*\"college degree\" + 0.001*\"subject\" + 0.001*\"college\" + 0.001*\"skill\" + 0.001*\"project\" + 0.001*\"december\" + 0.001*\"lot\" + 0.001*\"people\" + 0.001*\"intermediate\" + 0.001*\"january\" + 0.001*\"since\" + 0.001*\"coursera\" + 0.001*\"mobile\" + 0.001*\"graphic\"'),\n",
       " (14,\n",
       "  '0.000*\"objective\" + 0.000*\"order\" + 0.000*\"outside control\" + 0.000*\"others culture-fit type interview\" + 0.000*\"new language framework\" + 0.000*\"oauth\" + 0.000*\"number interview company\" + 0.000*\"notation\" + 0.000*\"non-trivial project technology\" + 0.000*\"non-trivial\" + 0.000*\"nice people\" + 0.000*\"next webpack\" + 0.000*\"next step\" + 0.000*\"next round interview\" + 0.000*\"next phase initial take-home/pair-programming session\" + 0.000*\"new thing practice\" + 0.000*\"new thing\" + 0.000*\"obstacle\" + 0.000*\"outline detailed blog post\" + 0.000*\"one one\"'),\n",
       " (15,\n",
       "  '0.002*\"others\" + 0.002*\"energy\" + 0.001*\"couple\" + 0.001*\"share others\" + 0.001*\"development knowledge gaining experience fulltime job\" + 0.001*\"gaining\" + 0.001*\"helpful tip\" + 0.001*\"internship couple freelance project\" + 0.001*\"little bit code free time\" + 0.001*\"many many job application\" + 0.001*\"whoo\" + 0.001*\"people\" + 0.001*\"employer\" + 0.001*\"code\" + 0.001*\"intro\" + 0.001*\"knowledge\" + 0.001*\"internship\" + 0.001*\"application\" + 0.001*\"cover\" + 0.001*\"way\"'),\n",
       " (16,\n",
       "  '0.002*\"logic\" + 0.002*\"adwords\" + 0.002*\"first period\" + 0.002*\"xsl\" + 0.002*\"backend certificate\" + 0.002*\"second week new place\" + 0.002*\"first frontend dev job\" + 0.002*\"xml\" + 0.002*\"study\" + 0.002*\"first\" + 0.001*\"role\" + 0.001*\"math\" + 0.001*\"larson\" + 0.001*\"amazing platform community\" + 0.001*\"journey february\" + 0.001*\"first job frontend email dev digital marketing agency\" + 0.001*\"thank much fcc\" + 0.001*\"previous programming knowledge\" + 0.001*\"basic\" + 0.001*\"lol\"'),\n",
       " (17,\n",
       "  '0.002*\"success\" + 0.001*\"additional\" + 0.001*\"closure\" + 0.001*\"web dev\" + 0.001*\"next month\" + 0.001*\"codecs\" + 0.001*\"fullstack dev job offer\" + 0.001*\"organisation\" + 0.001*\"hard\" + 0.001*\"story\" + 0.001*\"stack\" + 0.001*\"hard work\" + 0.001*\"book\" + 0.001*\"object\" + 0.001*\"quincy\" + 0.001*\"friend local fcc group everyone\" + 0.001*\"thanks success story\" + 0.001*\"success story fcc forum\" + 0.001*\"frontend certificate along udemy course\" + 0.001*\"post long time\"'),\n",
       " (18,\n",
       "  '0.003*\"lawyer\" + 0.002*\"satisfied current job\" + 0.002*\"satisfied\" + 0.002*\"ideas.ataccama.com\" + 0.002*\"dev fcc\" + 0.002*\"jquery\" + 0.001*\"medium article\" + 0.001*\"fellow\" + 0.001*\"fellow camper\" + 0.001*\"look\" + 0.001*\"medium\" + 0.001*\"current\" + 0.001*\"ad\" + 0.001*\"portfolio project codepen github page\" + 0.001*\"call next morning\" + 0.001*\"link relevant project\" + 0.001*\"twitch api project open impressed code\" + 0.001*\"skill real world\" + 0.001*\"person phone\" + 0.001*\"jquery frontend asp.net visual basic backend\"'),\n",
       " (19,\n",
       "  '0.002*\"qa\" + 0.002*\"analyst\" + 0.001*\"assignment\" + 0.001*\"tuesday\" + 0.001*\"fun\" + 0.001*\"hunt\" + 0.001*\"quincy\" + 0.001*\"position\" + 0.001*\"play\" + 0.001*\"straight\" + 0.001*\"straight hour\" + 0.001*\"tic tac toe assignment\" + 0.001*\"comeback\" + 0.001*\"contribution first paycheck\" + 0.001*\"parttime web-based development gig\" + 0.001*\"dev position\" + 0.001*\"offer qa analyst\" + 0.001*\"play time\" + 0.001*\"month fulltime course web dev\" + 0.001*\"kid toronto\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.id2token[2298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "#https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t):\n",
    "        print(dictionary.id2token[ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "k-means to approximate the number of topics before trying a more elaborate form\n",
    "Check and improve previous work:\n",
    "* https://github.com/evaristoc/fccgitterDataScience/blob/master/Identifying%20Relevant%20Topics%20in%20a%20Chatroom.ipynb\n",
    "* https://stackoverflow.com/questions/24816912/number-of-latent-semantic-indexing-topics\n",
    "* https://stackoverflow.com/questions/9582291/how-do-we-decide-the-number-of-dimensions-for-latent-semantic-analysis/9759218#9759218\n",
    "\n",
    "Also check:\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/ (good example but conceptually a bit wrong)\n",
    "* https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "* http://hojunhao.github.io/sgparliament/LDA.html\n",
    "* https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process\n",
    "* https://nlpforhackers.io/recipe-text-clustering/\n",
    "* https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling\n",
    "* http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=3&lambda=1&term=\n",
    "* https://stackoverflow.com/questions/50106516/k-means-for-topic-modelling-elbow-method\n",
    "* https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html\n",
    "* http://wdsinet.org/Annual_Meetings/2016_Proceedings/papers/Paper45.pdf\n",
    "* http://ramet.elte.hu/~podani/Methods.htm\n",
    "* https://hk.saowen.com/a/edc29232eae094158f66e8ff3f08d6f35b8a2a45d628fce8917d2dce6f94282e\n",
    "* https://rare-technologies.com/validating-gensims-topic-coherence-pipeline/\n",
    "* http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html\n",
    "* https://www.searchenginejournal.com/latent-semantic-indexing-wont-help-seo/240705/\n",
    "* https://www.quora.com/What-is-topic-coherence + http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "* https://stackoverflow.com/questions/50340657/pyldavis-with-mallet-lda-implementation-ldamallet-object-has-no-attribute-inf\n",
    "* https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 8\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "K = list(range(1, n_components+1))\n",
    "KM = [KMeans(n_clusters=k).fit(X) for k in K]\n",
    "centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [numpy.argmin(D,axis=1) for D in D_k]\n",
    "dist = [numpy.min(D,axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "\n",
    "kIdx = 8-1\n",
    "\n",
    "# elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(K, avgWithinSS, 'b*-')\n",
    "ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for KMeans clustering')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "true_k = 8\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "newX = numpy.array(pandas.concat([pandas.DataFrame(X),datadf_foran['timestamp_norm'].reset_index()['timestamp_norm']],axis=1))\n",
    "km.fit(newX)\n",
    "\n",
    "print()\n",
    "\n",
    "labels = [x for x in range(datadf_foran.shape[0])]\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(newX, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "fig_clusters = plt.figure()\n",
    "fig_clusters.suptitle('Clusters over first 2 Components')\n",
    "ax = fig_clusters.add_subplot(111)\n",
    "ax.set_xlabel('Component I')\n",
    "ax.set_ylabel('Component II')\n",
    "plt.scatter(newX[:,0],newX[:,1], c=km.fit_predict(newX))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727"
    }
   },
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "038497e7-3cf4-4c64-867c-1bc637cad5e5"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## raw_lda_frankjupyter2: modified to fit allrecordsPreparation2\n",
    "############################\n",
    "def raw_lda_frankjupyter2(lemmposrecs, wordimportance, metmodel=2, STOPWORDS=nltk.corpus.stopwords.words('english')):\n",
    "    '''\n",
    "    description: modified model based on https://www.frankcleary.com/svd/ for a more raw construction of a lda\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    def metriccalc(st, normalizer, wordimportance):\n",
    "        '''\n",
    "        description:\n",
    "        text normalization based on ALL characters in the sentence; why? Example: if two writers wrote 20 words, 2 of them very important, but one of them wrote half of characters stopwords, those 2 words wouldnt be penalized accordingly for this writer: the other wrote more important content\n",
    "        '''       \n",
    "        metfuncs = [\n",
    "            lambda w: math.pow(0.1+float(wordimportance[w]),textbow[w]/normalizer) if w in list(wordimportance.keys()) else 0.0, #a sort of idf-normalization based on number of words in the text: the more the words in a text, the more important\n",
    "            lambda w: float(wordimportance[w])*textbow[w] if w in list(wordimportance.keys()) else 0.0, #good but ignore those words with worimportance too low or 0 but that are frequent in text\n",
    "            lambda w: 1.0+2.0**float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- probably the best one; because it is not normilized this indicator would simply say that if it has the word at least once is already on topic\n",
    "            lambda w: float(wordimportance[w]) + textbow[w]/normalizer if w in list(wordimportance.keys()) else 0.0,\n",
    "            lambda w: (1.0+textbow[w]/normalizer)*float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- apparently the second best with the adhoc word ranking\n",
    "            lambda w: normalizer/textbow[w]*wordimportance[w]/sum(list(wordimportance.values())), #tfidf-ish\n",
    "            lambda w: float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0\n",
    "        ]\n",
    "        likedict = collections.defaultdict(float)\n",
    "        textbow = collections.Counter(st)\n",
    "        for w in st:\n",
    "            likedict[w] = metfuncs[metmodel](w)\n",
    "        return likedict\n",
    "\n",
    "    #redo_corpus_by_sts = []\n",
    "    words_df = pandas.DataFrame()\n",
    "    textreference = {}\n",
    "    \n",
    "    \n",
    "    for textindex, lemmpos_r in enumerate(lemmposrecs):\n",
    "        lemmpos_t = lemmpos_r[3]\n",
    "        #print('lemmpos_t', len(lemmpos_t))\n",
    "        lemm_sts = ''\n",
    "        for lemmpos_TUPLE in lemmpos_t:\n",
    "            w = lemmpos_TUPLE[2]\n",
    "            lemm_sts = lemm_sts + w + ' '\n",
    "        sts = lemm_sts.split('.')\n",
    "        for stindex, lemmpos_st in enumerate(sts):\n",
    "            treated_st, lensts = cleaningtext(lemmpos_st.split(), STOPWORDS=STOPWORDS)        \n",
    "            #print('treated_st', lensts)\n",
    "            if len(treated_st) > 3:\n",
    "                likedict = metriccalc(treated_st, lensts, wordimportance)\n",
    "                st_df = pandas.DataFrame.from_dict(likedict, orient='index')\n",
    "                textindexing = str(textindex)+'_'+str(stindex)\n",
    "                st_df.columns = [textindexing]\n",
    "                textreference[textindexing] = {}\n",
    "                textreference[textindexing]['treated_st'] = treated_st\n",
    "                #st_df.columns = [str(count)]\n",
    "                words_df = words_df.join(st_df, how='outer', )\n",
    "    \n",
    "    words_df = words_df.fillna(0)\n",
    "    print(\"Number of unique words: %s\" % len(words_df))\n",
    "    print(words_df.head(10))\n",
    "    #print(words_df.sort(columns=words_df.columns[0], ascending=False).head(10))\n",
    "    \n",
    "    return words_df, textreference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e3fa11e-feb2-487d-9d53-43dc3deb650c": {
     "id": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "prev": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "regions": {
      "07b1e699-5143-43f7-a938-d61bc27e8e62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e1c251e9-1608-43d5-ba5a-e27bb09739f5",
        "part": "whole"
       },
       "id": "07b1e699-5143-43f7-a938-d61bc27e8e62"
      }
     }
    },
    "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5": {
     "id": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "prev": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "regions": {
      "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727",
        "part": "whole"
       },
       "id": "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0"
      }
     }
    },
    "1cf808ac-39f2-4009-8112-3ef3a17b4af6": {
     "id": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "prev": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "regions": {
      "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5992afc-b214-46ae-8f12-1026835184cd",
        "part": "whole"
       },
       "id": "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8"
      }
     }
    },
    "1f7a3d0c-f066-482e-8b40-cf107299d110": {
     "id": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "prev": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "regions": {
      "1badc1e9-6ff7-4358-9493-665026267eab": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d236a59f-ff21-406d-b1df-9436aedbdb11",
        "part": "whole"
       },
       "id": "1badc1e9-6ff7-4358-9493-665026267eab"
      }
     }
    },
    "3659283d-893a-48fa-9115-7dd64d2aed00": {
     "id": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "prev": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "regions": {
      "835576ab-c1f9-4bfa-af97-ed77dbde5925": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e294b910-1a25-4e48-abb5-37239f441f2e",
        "part": "whole"
       },
       "id": "835576ab-c1f9-4bfa-af97-ed77dbde5925"
      }
     }
    },
    "395c02de-c982-4ca8-a48e-10a79828812d": {
     "id": "395c02de-c982-4ca8-a48e-10a79828812d",
     "prev": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "regions": {
      "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3cda7d21-4a2d-4d01-ac74-10e33c83629e",
        "part": "whole"
       },
       "id": "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7"
      }
     }
    },
    "3f4f1ce3-4872-48cc-8c96-31c19dba5a74": {
     "id": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "prev": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "regions": {
      "7fb22b2c-4bff-4c35-86aa-40952a6b9b24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5bb4cd0-4cb1-493f-99e0-e75214f084a7",
        "part": "whole"
       },
       "id": "7fb22b2c-4bff-4c35-86aa-40952a6b9b24"
      }
     }
    },
    "45ee2b8f-c113-44cf-a993-6a20d43e99f5": {
     "id": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "prev": "532186bd-d619-49da-be76-e28eb7db691a",
     "regions": {
      "e4a63835-d541-4073-bee5-0017da9a065b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8e459ff-ae34-48aa-8146-50365df9ea53",
        "part": "whole"
       },
       "id": "e4a63835-d541-4073-bee5-0017da9a065b"
      }
     }
    },
    "532186bd-d619-49da-be76-e28eb7db691a": {
     "id": "532186bd-d619-49da-be76-e28eb7db691a",
     "prev": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "regions": {
      "2b7cff32-e764-4b30-8301-0cb4bb2268dc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384",
        "part": "whole"
       },
       "id": "2b7cff32-e764-4b30-8301-0cb4bb2268dc"
      }
     }
    },
    "99fa0d29-85d3-4e57-a540-74b76519f4ac": {
     "id": "99fa0d29-85d3-4e57-a540-74b76519f4ac",
     "prev": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "regions": {
      "afebcb71-18ec-40b4-9e65-8f01e4c70d57": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "038497e7-3cf4-4c64-867c-1bc637cad5e5",
        "part": "whole"
       },
       "id": "afebcb71-18ec-40b4-9e65-8f01e4c70d57"
      }
     }
    },
    "a59429ee-539c-4c02-bfc8-8582111bf455": {
     "id": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "prev": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "regions": {
      "34164544-7832-445e-a820-4ef4dbc522ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a",
        "part": "whole"
       },
       "id": "34164544-7832-445e-a820-4ef4dbc522ac"
      }
     }
    },
    "aa51d16f-a5b4-460f-91fa-569faac9a6fe": {
     "id": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "prev": null,
     "regions": {
      "e46d430e-2c0b-416c-bac3-86a2d27711c1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "560a94b7-6c8e-465f-96fa-fcf2c3859245",
        "part": "whole"
       },
       "id": "e46d430e-2c0b-416c-bac3-86a2d27711c1"
      }
     }
    },
    "ac6a71ca-a1d6-4449-9ee7-87ff28115363": {
     "id": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "prev": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "regions": {
      "ce9295e6-aa18-4ceb-a277-fb19127d89ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67bf390c-9e34-4764-a6de-ab9d3082090b",
        "part": "whole"
       },
       "id": "ce9295e6-aa18-4ceb-a277-fb19127d89ed"
      }
     }
    },
    "baa3a1da-cdb2-4638-8bec-2986349ac603": {
     "id": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "prev": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "regions": {
      "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59c7646c-3944-4e2c-81a1-1728a02396ae",
        "part": "whole"
       },
       "id": "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce"
      }
     }
    },
    "ca7a6c34-27e7-4921-9da2-43a92faf8e8e": {
     "id": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "prev": "395c02de-c982-4ca8-a48e-10a79828812d",
     "regions": {
      "6414ffa6-ea4d-40fe-9d90-5b0816ff0789": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c1fd569-b2c4-423d-a598-1466fc992a49",
        "part": "whole"
       },
       "id": "6414ffa6-ea4d-40fe-9d90-5b0816ff0789"
      }
     }
    },
    "dae9886f-f215-4b89-aefb-8831c7f2ddfc": {
     "id": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "prev": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "regions": {
      "5eeb753c-493e-4bba-b5ab-025339c264f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d80b2206-d953-46b3-b807-2bf5a8557d28",
        "part": "whole"
       },
       "id": "5eeb753c-493e-4bba-b5ab-025339c264f2"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
