{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/derekgreene/topic-model-tutorial \n",
    "* https://github.com/derekgreene/topic-model-tutorial/blob/master/topic-modelling-with-scikitlearn.pdf\n",
    "* https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n",
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d80b2206-d953-46b3-b807-2bf5a8557d28"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    def createcands(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        #print(list(group)[::-1])\n",
    "        reversing = [None]\n",
    "        #for el in group:\n",
    "        #    print(el)\n",
    "        for el in group:\n",
    "            if reversing[0] == None:\n",
    "                reversing[0] = el[0]\n",
    "            else:\n",
    "                reversing.insert(0, el[0])\n",
    "        #print(reversing)\n",
    "        for word in reversing:\n",
    "            if current == '':\n",
    "                current = word\n",
    "            else:\n",
    "                current = word + ' ' + current\n",
    "            total.append(current)\n",
    "        #print(list(set(reversing+total)))\n",
    "        return list(set(reversing+total))\n",
    "    \n",
    "    def createcands2(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        for word, pos, chunk in list(group):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            #print(word)\n",
    "            if current == '':\n",
    "                current = word\n",
    "                total.append(word)\n",
    "            else:\n",
    "                current = current+' '+word\n",
    "                total.append(word)\n",
    "        total.append(current)\n",
    "\n",
    "        return list(set(total))\n",
    "            \n",
    "    #candidates = [' '.join(word for word, pos, chunk in group)\n",
    "    candidates = [createcands2(group)\n",
    "                    for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "    #print(candidates)\n",
    "\n",
    "    #return [cand for cand in candidates]\n",
    "    #print(list(itertools.chain.from_iterable(candidates)))\n",
    "    return list(itertools.chain.from_iterable(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        pattern_A01 = re.compile(r'^hey( |$)')\n",
    "        pattern_B01 = re.compile(r'(january|february| march(,|\\.)? |april|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday|morning|afternoon|evening|mont(s|ly)|year(s|ly)?| day(s)?)')\n",
    "        pattern_C01 = re.compile(r'(chance(s)?|opportunit(y|ies))')\n",
    "        \n",
    "        ## Text extraction, REGEXs and Replacements\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text() \\\n",
    "                                                        .lower() \\\n",
    "                                                        .replace('’',\"'\") \\\n",
    "                                                        .replace('‘', \"'\") \\\n",
    "                                                        .replace('´', \"'\") \\\n",
    "                                                        .replace('free code camp', 'fcc') \\\n",
    "                                                        .replace('freecodecamp', 'fcc') \\\n",
    "                                                        .replace('javascript', 'js') \\\n",
    "                                                        .replace('part time', 'parttime') \\\n",
    "                                                        .replace('part-time', 'parttime') \\\n",
    "                                                        .replace('full time', 'fulltime') \\\n",
    "                                                        .replace('full-time', 'fulltime') \\\n",
    "                                                        .replace('web application', 'web-app') \\\n",
    "                                                        .replace('web app', 'web-app') \\\n",
    "                                                        .replace('web development', 'dev') \\\n",
    "                                                        .replace('web-development', 'dev') \\\n",
    "                                                        .replace('developer', 'dev') \\\n",
    "                                                        .replace('web dev', 'dev') \\\n",
    "                                                        .replace('dev position', 'dev job') \\\n",
    "                                                        .replace('dev role', 'dev job') \\\n",
    "                                                        .replace('front end', 'frontend') \\\n",
    "                                                        .replace('front-end','frontend') \\\n",
    "                                                        .replace('back end', 'backend') \\\n",
    "                                                        .replace('back-end','backend') \\\n",
    "                                                        .replace('full stack', 'fullstack') \\\n",
    "                                                        .replace('full-stack','fullstack') \\\n",
    "                                                        .replace('frontend job', 'dev job') \\\n",
    "                                                        .replace('frontend position', 'dev job') \\\n",
    "                                                        .replace('frontend role', 'dev job') \\\n",
    "                                                        .replace('frontend web job', 'dev job') \\\n",
    "                                                        .replace('frontend web position', 'dev job') \\\n",
    "                                                        .replace('frontend web role', 'dev job') \\\n",
    "                                                        .replace('frontend web dev job', 'dev job') \\\n",
    "                                                        .replace('frontend web dev position', 'dev job') \\\n",
    "                                                        .replace('frontend web dev role', 'dev job') \\\n",
    "                                                        .replace('resume', 'cv') \\\n",
    "                                                        .replace('angularjs', 'angular') \\\n",
    "                                                        .replace('angular', 'angularjs') \\\n",
    "                                                        .replace('certification', 'cert') \\\n",
    "                                                        .replace('certificate', 'cert') \\\n",
    "                                                        .replace('machine learning', 'machinelearning') \\\n",
    "                                                        .replace('data science', 'datascience') \\\n",
    "                                                        .replace('self learning', 'self-taught') \\\n",
    "                                                        .replace('self learned', 'self-taught') \\\n",
    "                                                        .replace('self-learning', 'self-taught') \\\n",
    "                                                        .replace('self-learned', 'self-taught') \\\n",
    "                                                        .replace('self taught', 'self-taught') \\\n",
    "                                                        .replace('thanks', 'thank') \\\n",
    "                                                        .replace('thankful', 'thank') \\\n",
    "                                                        .replace('gratitude', 'thank') \\\n",
    "                                                        .replace('many thank', 'thank') \\\n",
    "                                                        .replace('much thank', 'thank') \\\n",
    "                                                        .replace('special thank', 'thank') \\\n",
    "                                                        .replace('big thank', 'thank')\n",
    "                                                        #.replace('app', 'web-app') \\\n",
    "                                                        #.replace('web-dev job', 'dev-job') \\\n",
    "                                                        #.replace('web-dev position', 'web-dev-job') \\\n",
    "                                                        #.replace('web-dev role', 'web-dev-job') \\\n",
    "                                                        #.replace('backend job', 'dev job') \\\n",
    "                                                        #.replace('backend position', 'dev job') \\\n",
    "                                                        #.replace('backend role', 'dev job') \\\n",
    "                                                        #.replace('backend web job', 'dev job') \\\n",
    "                                                        #.replace('backend web position', 'dev job') \\\n",
    "                                                        #.replace('backend web role', 'dev job') \\\n",
    "                                                        #.replace('backend web dev job', 'dev job') \\\n",
    "                                                        #.replace('backend web dev position', 'dev job') \\\n",
    "                                                        #.replace('backend web dev role', 'dev job') \\ \n",
    "                                                        #.replace('dev job', 'dev job') \\\n",
    "                                                        \n",
    "        soup_forumpostTEXT = re.sub(pattern_A01, ' hi ', soup_forumpostTEXT)\n",
    "        soup_forumpostTEXT = soup_forumpostTEXT.replace('fellow camper', '') \\\n",
    "                                    .replace('camper', '') \\\n",
    "                                    .replace('fccers', '') \\\n",
    "                                    .replace('fccer', '') \\\n",
    "                                    .replace('everybody', 'everyone') \\\n",
    "                                    .replace('hello', 'hi') \\\n",
    "                                    .replace('hi everyone', 'hi') \\\n",
    "                                    .replace('hi, everyone', 'hi') \\\n",
    "                                    .replace('hi everybody', 'hi') \\\n",
    "                                    .replace('hi, everybody', 'hi')\n",
    "                                    \n",
    "        soup_forumpostTEXT = re.sub(pattern_B01, ' datetimetoken ', soup_forumpostTEXT)\n",
    "        \n",
    "        soup_forumpostTEXT = re.sub(pattern_C01, ' chance ', soup_forumpostTEXT)\n",
    "        \n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    #print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    #print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                #print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f8e459ff-ae34-48aa-8146-50365df9ea53"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var4: wordimportance_var3 modified to fit keyphrases; changes in the \"opacity\" formula\n",
    "############################\n",
    "def wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=True):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    selectedgrams = None\n",
    "    if unigrams_test:\n",
    "        selectedgrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                         if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                        ])\n",
    "    else:\n",
    "        selectedgrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                        ])        \n",
    "    print('unigrams',len(selectedgrams))\n",
    "    maxdiv = math.log(sorted(selectedgrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "    print('maxdiv', maxdiv)\n",
    "    opacity = collections.defaultdict(float)\n",
    "    #for grams, counts in lemmws_fd.items(): #grams assumes a phrase is possible\n",
    "    for grams, counts in selectedgrams.items():\n",
    "        if grams == '':\n",
    "            opacity[grams] = 0.0\n",
    "            continue\n",
    "        opval = []\n",
    "        #assert len(grams.split()) == 1, print(grams)\n",
    "        for gram in grams.split():\n",
    "            if gram == '':\n",
    "                continue\n",
    "            if gram in list(selectedgrams.keys()):\n",
    "                #if grams == \"new language framework\":\n",
    "                #    print(gram, math.log(selectedgrams[gram]))\n",
    "                opval.append(math.log(selectedgrams[gram]))\n",
    "            else:\n",
    "                opval.append(0.0)\n",
    "        #assert len(opval) != 0, print('grams',grams)\n",
    "        averopval = sum(opval)/len(opval)\n",
    "        if 1 - averopval/maxdiv != 0.0:\n",
    "            opacity[grams] = 1 - averopval/maxdiv\n",
    "        else:\n",
    "            opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "        #if grams == \"new language framework\":\n",
    "        #    print(grams, opacity[grams])\n",
    "    #print('opval',opval[:10])\n",
    "\n",
    "    #sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "    sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(selectedgrams.keys())])\n",
    "    #assert \"new language framework\" not in list(sizing_matrix.keys())\n",
    "    \n",
    "    ## Count lemmatized words/characters per text  \n",
    "    for i,lemmpos_t in enumerate(lemmposrecs):\n",
    "        for k, lemmpos_sts in enumerate(lemmpos_t[3]):\n",
    "            ## Use lemmatized word\n",
    "            #print(lemmpos_sts)\n",
    "            for tk_TUPLE in lemmpos_sts: \n",
    "                lemmw = tk_TUPLE[2]\n",
    "                if lemmw in list(selectedgrams.keys()):\n",
    "                    sizing_matrix[lemmw][i] = sizing_matrix[lemmw][i] + 1\n",
    "\n",
    "        if not unigrams_test:\n",
    "            #print('not unigrams only')\n",
    "            for cand in lemmpos_t[4]: #<--------------------------------- problem!! it counted candidates only once!!!\n",
    "                if len(cand.split()) > 1:\n",
    "                    #assert cand != \"new language framework\", print(cand)\n",
    "                    for w in cand.split():\n",
    "                        sizing_matrix[cand][i] = sizing_matrix[cand][i] + sizing_matrix[w][i]\n",
    "                    sizing_matrix[cand][i] = sizing_matrix[cand][i]/len(cand.split())\n",
    "                        #if cand == \"new language framework\":\n",
    "                        #    print(sizing_matrix[cand][i], sizing_matrix[w][i])\n",
    "\n",
    "\n",
    "    ## Normalization\n",
    "    normalization = dict([(k, (sum(vector)-max(vector)+1)/sum(vector)) if sum(vector) != 0 else (k,0.0) for k, vector in sizing_matrix.items()])\n",
    "    #assert \"new language framework\" not in list(normalization.keys()), normalization[\"new language framework\"]\n",
    "    #normalization = collections.defaultdict(float)\n",
    "    #for k, vector in sizing_matrix.items():\n",
    "    #    if sum(vector) != 0:\n",
    "    #        normalization[k] = (sum(vector)-max(vector))/sum(vector)\n",
    "    #    else:\n",
    "    #        \n",
    "        \n",
    "    \n",
    "    wordimportance = dict([(k, valnorm*opacity[k]) for k, valnorm in normalization.items()])\n",
    "\n",
    "    return wordimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleaningtext(st, unigrams_test = False):\n",
    "    treated_st = []\n",
    "    if not unigrams_test:\n",
    "        for w in st:\n",
    "            treated_st.append(w)\n",
    "    else:\n",
    "        for w in st:\n",
    "            if len(w.split()) == 1:\n",
    "                treated_st.append(w)\n",
    "    countwds = len(treated_st)\n",
    "    return treated_st, countwds\n",
    "\n",
    "def cleaningtext2(st): #discharge phrases\n",
    "    treated_st = []\n",
    "    countwds = len(st)\n",
    "\n",
    "    return treated_st, countwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "59c7646c-3944-4e2c-81a1-1728a02396ae"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## gensim_models2 re-evaluated: modification of gensim_model2's cleanedsts_from_lemmpostxts to fit keyphrases\n",
    "############################\n",
    "\n",
    "def gensim_models2(lemmposrecs, NUM_TOPICS = 15, lemmws_fd = {}, wordimportance = {}, unigrams_test = False, nltk = nltk, gensim = gensim):\n",
    "    \n",
    "    def cleanedsts_from_lemmpostxts2(lemmposrecs, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "        redo_corpus_by_sts = []\n",
    "        for lemmpos_r in lemmposrecs:\n",
    "            candidates = lemmpos_r[4]\n",
    "            treated_lemmcands = cleaningtext(candidates, unigrams_test=unigrams_test)[0] #passing candidates only\n",
    "            redo_corpus_by_sts.append(treated_lemmcands)\n",
    "            #redo_corpus_by_sts.append(candidates)\n",
    "        \n",
    "        #print(len(redo_corpus_by_sts), redo_corpus_by_sts[-1])\n",
    "        return redo_corpus_by_sts\n",
    "        \n",
    "    def basedonBOW(redo_corpus_by_sts):\n",
    "        dictionary = gensim.corpora.Dictionary(redo_corpus_by_sts) #[token for st in redo_corpus_by_sts for token in st]\n",
    "        corpus = [dictionary.doc2bow(text) for text in redo_corpus_by_sts]\n",
    "        return corpus, dictionary\n",
    "    \n",
    "    def basedonTFIDF(corpus):\n",
    "        return gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    def basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance):\n",
    "        \n",
    "        def metriccalc(w):\n",
    "            if w in wordimportance:\n",
    "                return 1.0+2.0**float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        def metriccalc2(w):\n",
    "            if w in wordimportance:\n",
    "                return float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        corpus = []\n",
    "        for sts in redo_corpus_by_sts:\n",
    "            st = []\n",
    "            for w in sts:\n",
    "                st.append((dictionary.token2id[w], metriccalc2(w)))\n",
    "            corpus.append(st)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    redo_corpus_by_sts  = cleanedsts_from_lemmpostxts2(lemmposrecs)\n",
    "    \n",
    "    corpus, dictionary = basedonBOW(redo_corpus_by_sts)\n",
    "    if wordimportance == {'tfidf':True}:\n",
    "        tfidf = basedonTFIDF(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "    if wordimportance != {} and wordimportance != {'tfidf':True}:\n",
    "        corpus = basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance)\n",
    "    \n",
    "    \n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, minimum_probability=0.005, iterations=100, per_word_topics = True, minimum_phi_value = 0.001, id2word=dictionary)\n",
    "    lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "   \n",
    "    return lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e294b910-1a25-4e48-abb5-37239f441f2e"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d236a59f-ff21-406d-b1df-9436aedbdb11"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of treated posts (len(count)) :: 53\n",
      "lemm errs: 19460\n"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, \n",
    "#                                                                                NUM_TOPICS=20,\n",
    "#                                                                                wordimportance = {'tfidf':True})\n",
    "\n",
    "#lda_model.print_topics(num_words=15)\n",
    "\n",
    "#[' '.join([l for wr in rec[3] for w,_,l,pos in wr]) for rec in lemmposrecs]\n",
    "#[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs]\n",
    "#[' '.join([cand for cand in rec[4]]) for rec in lemmposrecs]\n",
    "##https://stackoverflow.com/questions/46282473/error-while-identify-the-coherence-value-from-lda-model\n",
    "#texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "#texts\n",
    "\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                                                  texts=texts,\n",
    "#                                                                  #corpus=corpus,\n",
    "#                                                                  window_size=20,\n",
    "#                                                                  dictionary=dictionary, \n",
    "#                                                                  coherence='c_uci')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, \n",
    "#                                                   texts=texts, \n",
    "#                                                   dictionary=dictionary,\n",
    "#                                                   coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "## Compute Coherence Score using UMass\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "#                                     corpus = corpus,\n",
    "#                                     dictionary=dictionary, \n",
    "#                                     coherence=\"u_mass\")\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#dictionary\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TOPICS = range(2,61,2)\n",
    "cv_coherence_values = []\n",
    "for numtopics in TOPICS:\n",
    "    print('\\n\\nFor NUM_TOPICS:', numtopics)\n",
    "    lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=numtopics, wordimportance = {'tfidf':True})\n",
    "    umass_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "                                     corpus = corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"u_mass\")\n",
    "    umass_coherence_lda = umass_coherence_model_lda.get_coherence()\n",
    "    print('U-Mass Coherence Score: ', umass_coherence_lda)\n",
    "    texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "    cv_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     texts=texts,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"c_v\")\n",
    "    cv_coherence_lda = cv_coherence_model_lda.get_coherence()\n",
    "    cv_coherence_values.append(cv_coherence_lda)\n",
    "    print('C_V Coherence Score: ', cv_coherence_lda)\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.quora.com/Can-I-combine-LSI-and-K-means-for-text-document-clustering-Are-there-any-sources-to-learn-about-it\n",
    "#http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##https://stackoverflow.com/questions/14261903/how-can-i-open-the-interactive-matplotlib-window-in-ipython-notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "limit=61; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, cv_coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=40, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.print_topics(num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=25, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.128*\"project\" + 0.103*\"company\" + 0.100*\"people\" + 0.099*\"interview\" + 0.097*\"lot\" + 0.095*\"way\" + 0.095*\"code\" + 0.090*\"cv\" + 0.089*\"skill\" + 0.086*\"time\" + 0.084*\"app\" + 0.081*\"experience\" + 0.080*\"recruiter\" + 0.080*\"good\" + 0.077*\"question\"'),\n",
       " (1,\n",
       "  '-0.205*\"design\" + -0.147*\"graphic\" + 0.118*\"success\" + 0.104*\"backend\" + 0.096*\"mern stack\" + 0.096*\"mern\" + -0.091*\"role\" + 0.090*\"group\" + -0.084*\"time http\" + 0.082*\"hard work\" + -0.082*\"http\" + -0.078*\"support\" + -0.078*\"html\" + 0.077*\"long\" + 0.076*\"source\"'),\n",
       " (2,\n",
       "  '-0.126*\"resource\" + -0.119*\"success\" + -0.116*\"story\" + -0.104*\"lawyer\" + 0.094*\"cv\" + -0.094*\"science\" + -0.086*\"group\" + -0.085*\"current\" + 0.083*\"company\" + -0.078*\"article\" + -0.076*\"topic doesnt belong\" + -0.076*\"belong\" + -0.076*\"part learning process\" + -0.076*\"doesnt\" + -0.076*\"school kid\"'),\n",
       " (3,\n",
       "  '-0.132*\"app\" + -0.121*\"logic\" + 0.118*\"role\" + -0.115*\"study\" + -0.102*\"adwords\" + 0.097*\"technology\" + 0.095*\"lawyer\" + -0.091*\"salary\" + 0.090*\"current\" + 0.089*\"student\" + -0.087*\"skill\" + 0.086*\"qa\" + -0.082*\"css js\" + -0.079*\"math\" + -0.077*\"thought\"'),\n",
       " (4,\n",
       "  '-0.127*\"day\" + 0.122*\"internship\" + -0.108*\"current\" + 0.103*\"jquery\" + -0.101*\"role\" + 0.093*\"knowledge\" + 0.088*\"success\" + -0.087*\"lawyer\" + 0.086*\"fulltime\" + 0.085*\"share\" + 0.084*\"test\" + 0.081*\"thank much fcc\" + 0.081*\"journey datetimetoken\" + 0.081*\"previous programming knowledge\" + 0.081*\"first job frontend email dev digital marketing agency\"'),\n",
       " (5,\n",
       "  '-0.128*\"group\" + -0.112*\"design\" + 0.105*\"dev job\" + -0.101*\"graphic\" + 0.097*\"freelance\" + 0.086*\"upwork\" + -0.079*\"challenge\" + -0.075*\"role\" + 0.072*\"dream\" + -0.072*\"day\" + -0.072*\"person\" + 0.072*\"article\" + -0.071*\"thats\" + 0.069*\"css js\" + 0.067*\"idea\"'),\n",
       " (6,\n",
       "  '-0.207*\"internship\" + -0.155*\"day\" + -0.105*\"bar\" + -0.103*\"technology\" + 0.101*\"yesterday\" + -0.096*\"software\" + -0.096*\"udemy\" + 0.096*\"dev post yesterday\" + 0.096*\"experience real time project\" + 0.096*\"happy part platform\" + 0.096*\"interview call\" + 0.096*\"portfolio employer\" + -0.090*\"drunk\" + -0.090*\"reply\" + -0.083*\"single company\"'),\n",
       " (7,\n",
       "  '-0.142*\"internship\" + 0.117*\"day\" + 0.101*\"qa\" + -0.098*\"lawyer\" + -0.095*\"people\" + 0.094*\"single\" + -0.091*\"others\" + 0.090*\"design\" + -0.087*\"little bit code free time\" + -0.087*\"gaining\" + -0.087*\"helpful tip\" + -0.087*\"internship couple freelance project\" + -0.087*\"many many job application\" + -0.087*\"whoo\" + -0.087*\"development knowledge gaining experience fulltime job\"'),\n",
       " (8,\n",
       "  '0.124*\"internship\" + -0.111*\"graduation\" + -0.110*\"email\" + -0.080*\"optimistic\" + -0.080*\"story life\" + -0.080*\"something stuck\" + -0.080*\"promising career\" + -0.080*\"promising\" + -0.080*\"politics university\" + -0.080*\"politics\" + -0.080*\"lucky\" + -0.080*\"minute datetimetoken\" + -0.080*\"first dev job small agency\" + -0.080*\"insurmountable\" + -0.080*\"first time\"'),\n",
       " (9,\n",
       "  '-0.141*\"quincy\" + -0.135*\"qa\" + -0.129*\"thats\" + -0.105*\"right\" + -0.101*\"analyst\" + 0.088*\"employer\" + -0.085*\"frontend dev\" + -0.085*\"assignment\" + 0.078*\"happy\" + 0.075*\"portfolio employer\" + 0.075*\"interview call\" + 0.075*\"dev post yesterday\" + 0.075*\"happy part platform\" + 0.075*\"experience real time project\" + 0.074*\"graphic\"'),\n",
       " (10,\n",
       "  '-0.161*\"lawyer\" + -0.160*\"day\" + 0.118*\"internship\" + 0.092*\"fullstack\" + 0.087*\"self-taught\" + 0.087*\"skill\" + -0.087*\"medium article\" + 0.085*\"idea\" + 0.084*\"future\" + 0.083*\"quincy\" + 0.081*\"pay\" + 0.081*\"datetimetoken time\" + 0.081*\"idea type question\" + 0.081*\"interview frontend dev job today\" + 0.081*\"self-taught fcc\"'),\n",
       " (11,\n",
       "  '-0.170*\"lawyer\" + 0.104*\"intro\" + 0.097*\"path\" + 0.095*\"interview process\" + 0.090*\"email\" + -0.090*\"design\" + -0.090*\"article\" + -0.088*\"medium article\" + -0.085*\"ideas.ataccama.com\" + -0.085*\"satisfied\" + -0.085*\"satisfied current job\" + 0.079*\"thats\" + 0.079*\"college\" + 0.075*\"right\" + 0.073*\"frontend dev\"'),\n",
       " (12,\n",
       "  '-0.122*\"lawyer\" + -0.113*\"jquery\" + 0.096*\"student\" + -0.095*\"github\" + 0.093*\"lol\" + -0.089*\"datetimetoken half\" + 0.079*\"venezuela\" + -0.078*\"failure\" + -0.075*\"internship\" + 0.073*\"way\" + 0.072*\"freelance\" + -0.070*\"article\" + -0.069*\"half\" + -0.068*\"angularjs\" + 0.068*\"upwork\"'),\n",
       " (13,\n",
       "  '-0.135*\"larson\" + -0.135*\"thank much fcc\" + -0.135*\"amazing platform community\" + -0.135*\"previous programming knowledge\" + -0.135*\"first job frontend email dev digital marketing agency\" + -0.135*\"journey datetimetoken\" + -0.100*\"previous\" + -0.093*\"agency\" + 0.093*\"article share story resource\" + 0.093*\"belong\" + 0.093*\"community thank\" + 0.093*\"contributor\" + 0.093*\"doesnt\" + 0.093*\"part learning process\" + 0.093*\"school kid\"'),\n",
       " (14,\n",
       "  '-0.225*\"lawyer\" + -0.133*\"xml\" + -0.133*\"xsl\" + -0.133*\"first period\" + -0.133*\"first frontend dev job\" + -0.133*\"backend cert\" + -0.133*\"second week new place\" + -0.126*\"backend\" + -0.126*\"venezuela\" + -0.112*\"satisfied current job\" + -0.112*\"ideas.ataccama.com\" + -0.112*\"satisfied\" + -0.106*\"call\" + -0.105*\"interview call\" + -0.105*\"dev post yesterday\"'),\n",
       " (15,\n",
       "  '0.102*\"freelance\" + 0.097*\"venezuela\" + -0.090*\"half\" + -0.077*\"agency\" + -0.077*\"within\" + 0.074*\"design\" + 0.073*\"hard\" + -0.070*\"student\" + 0.069*\"hard work\" + -0.068*\"bachelor software technology\" + -0.068*\"cv/cv\" + -0.068*\"lot interest technology roadmap\" + -0.068*\"roadmap\" + -0.068*\"single company\" + -0.068*\"student dev within datetimetoken company\"'),\n",
       " (16,\n",
       "  '-0.160*\"day\" + 0.120*\"logic\" + 0.113*\"math\" + -0.113*\"app\" + -0.100*\"adwords\" + -0.091*\"datetimetoken half\" + -0.079*\"group\" + 0.079*\"lawyer\" + -0.077*\"failure\" + -0.076*\"half\" + 0.076*\"advance\" + 0.076*\"trainer\" + 0.076*\"tribute\" + 0.076*\"advance logic\" + 0.071*\"personal\"'),\n",
       " (17,\n",
       "  '-0.165*\"day\" + -0.149*\"lawyer\" + 0.100*\"thank much fcc\" + 0.100*\"previous programming knowledge\" + 0.100*\"larson\" + 0.100*\"journey datetimetoken\" + 0.100*\"first job frontend email dev digital marketing agency\" + 0.100*\"amazing platform community\" + 0.092*\"contributor\" + 0.092*\"topic doesnt belong\" + 0.092*\"belong\" + 0.092*\"article share story resource\" + 0.092*\"community thank\" + 0.092*\"doesnt\" + 0.092*\"school kid\"'),\n",
       " (18,\n",
       "  '-0.119*\"employer\" + 0.117*\"recruiter\" + -0.112*\"experience real time project\" + -0.112*\"happy part platform\" + -0.112*\"interview call\" + -0.112*\"portfolio employer\" + -0.112*\"dev post yesterday\" + -0.102*\"qa\" + 0.097*\"udacity\" + 0.085*\"xml\" + 0.085*\"first period\" + 0.085*\"first frontend dev job\" + 0.085*\"second week new place\" + 0.085*\"xsl\" + 0.085*\"backend cert\"'),\n",
       " (19,\n",
       "  '0.183*\"day\" + -0.121*\"lawyer\" + 0.109*\"interview frontend dev job today\" + 0.109*\"idea type question\" + 0.109*\"datetimetoken time\" + 0.109*\"self-taught fcc\" + 0.103*\"css js\" + 0.097*\"start\" + 0.087*\"technology\" + 0.085*\"call\" + 0.085*\"experience real time project\" + 0.085*\"dev post yesterday\" + 0.085*\"portfolio employer\" + 0.085*\"happy part platform\" + 0.085*\"interview call\"')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"group\" + 0.002*\"prototype\" + 0.002*\"daughter\" + 0.002*\"portal\" + 0.002*\"good thing\" + 0.001*\"weekend\" + 0.001*\"person\" + 0.001*\"app\" + 0.001*\"create\" + 0.001*\"frustration programming\" + 0.001*\"startup company\" + 0.001*\"web stuff\" + 0.001*\"urban\" + 0.001*\"shy person\" + 0.001*\"gardening\" + 0.001*\"responsible\" + 0.001*\"time work\" + 0.001*\"shy\" + 0.001*\"visualization\" + 0.001*\"tech skill\"'),\n",
       " (1,\n",
       "  '0.001*\"similar\" + 0.001*\"share story\" + 0.001*\"small real-life coding exercise\" + 0.001*\"small remote\" + 0.001*\"solution similar problem\" + 0.001*\"something codepen\" + 0.001*\"something similar product card\" + 0.001*\"technology knew\" + 0.001*\"viewpoint\" + 0.001*\"development knowledge moment\" + 0.001*\"dedication\" + 0.001*\"current job time\" + 0.001*\"criterion\" + 0.001*\"computer science-y stuff\" + 0.001*\"community environment chance\" + 0.001*\"card\" + 0.001*\"awesome community\" + 0.001*\"role frontend design\" + 0.001*\"shopify platform\" + 0.001*\"agency canada\"'),\n",
       " (2,\n",
       "  '0.003*\"qa\" + 0.002*\"analyst\" + 0.002*\"assignment\" + 0.001*\"fullstack\" + 0.001*\"machinelearning\" + 0.001*\"datascience\" + 0.001*\"pay\" + 0.001*\"backend\" + 0.001*\"skill\" + 0.001*\"startup\" + 0.001*\"hackathons\" + 0.001*\"dream job future\" + 0.001*\"node.js\" + 0.001*\"scientist\" + 0.001*\"fcc backend project\" + 0.001*\"hunt\" + 0.001*\"fun\" + 0.001*\"quincy\" + 0.001*\"city\" + 0.001*\"curriculum\"'),\n",
       " (3,\n",
       "  '0.002*\"adwords\" + 0.002*\"jquery\" + 0.002*\"app\" + 0.001*\"ad\" + 0.001*\"script\" + 0.001*\"boss\" + 0.001*\"code\" + 0.001*\"recruiter\" + 0.001*\"company\" + 0.001*\"salary\" + 0.001*\"people\" + 0.001*\"project\" + 0.001*\"web-app\" + 0.001*\"code base\" + 0.001*\"someone experience\" + 0.001*\"entry level dev job\" + 0.001*\"end interview\" + 0.001*\"jquery frontend asp.net visual basic backend\" + 0.001*\"relevant\" + 0.001*\"asp.net\"'),\n",
       " (4,\n",
       "  '0.005*\"day\" + 0.002*\"object\" + 0.001*\"comment\" + 0.001*\"leader\" + 0.001*\"right\" + 0.001*\"gulp\" + 0.001*\"path\" + 0.001*\"employee\" + 0.001*\"additional\" + 0.001*\"accomplishment\" + 0.001*\"blog\" + 0.001*\"much\" + 0.001*\"framework\" + 0.001*\"client\" + 0.001*\"github\" + 0.001*\"thought\" + 0.001*\"js\" + 0.001*\"environment\" + 0.001*\"project\" + 0.001*\"countless number applicant\"'),\n",
       " (5,\n",
       "  '0.003*\"venezuela\" + 0.002*\"freelance\" + 0.001*\"first dev job age\" + 0.001*\"food home\" + 0.001*\"freelance job week\" + 0.001*\"freelance summer\" + 0.001*\"frontend backend cert\" + 0.001*\"js technology deep way\" + 0.001*\"local dev job\" + 0.001*\"night hour spent reading coding\" + 0.001*\"post fcc solid path\" + 0.001*\"call guy\" + 0.001*\"obligation\" + 0.001*\"spent\" + 0.001*\"fcc datetimetoken\" + 0.001*\"deep\" + 0.001*\"hard time\" + 0.001*\"interview next datetimetoken\" + 0.001*\"age\" + 0.001*\"food\"'),\n",
       " (6,\n",
       "  '0.001*\"firm\" + 0.001*\"resource\" + 0.001*\"everything\" + 0.001*\"goal\" + 0.001*\"fact\" + 0.001*\"passionate\" + 0.001*\"approach\" + 0.001*\"education\" + 0.001*\"entire life field\" + 0.001*\"belgium\" + 0.001*\"tutorials…\" + 0.001*\"job search\" + 0.001*\"entry point\" + 0.001*\"glad\" + 0.001*\"economics\" + 0.001*\"lucas\" + 0.001*\"learning consult different resource\" + 0.001*\"confidence enough knowledge\" + 0.001*\"hands-on\" + 0.001*\"glad share story\"'),\n",
       " (7,\n",
       "  '0.002*\"success\" + 0.002*\"additional library\" + 0.002*\"post long time\" + 0.002*\"friend local fcc group everyone\" + 0.002*\"job fulltime dev\" + 0.002*\"thank success story\" + 0.002*\"success story fcc forum\" + 0.002*\"frontend cert along udemy course\" + 0.002*\"proud\" + 0.002*\"na\" + 0.002*\"story\" + 0.001*\"next month\" + 0.001*\"mern\" + 0.001*\"library\" + 0.001*\"success story\" + 0.001*\"mern stack\" + 0.001*\"along\" + 0.001*\"holiday\" + 0.001*\"hard work\" + 0.001*\"additional\"'),\n",
       " (8,\n",
       "  '0.001*\"big thank\" + 0.001*\"fcc one\" + 0.001*\"first interview hour\" + 0.001*\"flaw\" + 0.001*\"edit:3.\" + 0.001*\"supportive community\" + 0.001*\"edit:3. interview\" + 0.001*\"much detail\" + 0.001*\"list stuff\" + 0.001*\"supportive\" + 0.001*\"\\'yes\" + 0.001*\"test well.and\" + 0.001*\"true\" + 0.001*\"fcc flaw\" + 0.001*\"well.and\" + 0.001*\"zeppelin\" + 0.001*\"course true question\" + 0.001*\"sake\" + 0.001*\"thank good luck\" + 0.001*\"strength\"'),\n",
       " (9,\n",
       "  '0.002*\"github\" + 0.002*\"test\" + 0.002*\"lol\" + 0.002*\"source\" + 0.001*\"intro\" + 0.001*\"studying\" + 0.001*\"parent\" + 0.001*\"hi guy\" + 0.001*\"open\" + 0.001*\"nice\" + 0.001*\"work github\" + 0.001*\"league\" + 0.001*\"league good open source project\" + 0.001*\"lot freedom\" + 0.001*\"make sure\" + 0.001*\"github link\" + 0.001*\"job startup parent company city\" + 0.001*\"new app ground\" + 0.001*\"make\" + 0.001*\"project github account\"'),\n",
       " (10,\n",
       "  '0.002*\"energy\" + 0.002*\"huge\" + 0.001*\"recruitment\" + 0.001*\"texas\" + 0.001*\"udacity\" + 0.001*\"json\" + 0.001*\"way\" + 0.001*\"phone\" + 0.001*\"lead\" + 0.001*\"portfolio\" + 0.001*\"next datetimetoken\" + 0.001*\"code\" + 0.001*\"forum\" + 0.001*\"factory\" + 0.001*\"“to\" + 0.001*\"time energy\" + 0.001*\"work coding factory huge team\" + 0.001*\"frontend/fullstack dev job phone\" + 0.001*\"“to learn\" + 0.001*\"first job application\"'),\n",
       " (11,\n",
       "  '0.002*\"idea type question\" + 0.002*\"interview frontend dev job today\" + 0.002*\"datetimetoken time\" + 0.002*\"self-taught fcc\" + 0.002*\"thats\" + 0.002*\"css js\" + 0.002*\"math\" + 0.002*\"logic\" + 0.002*\"start\" + 0.002*\"self-taught\" + 0.002*\"skill\" + 0.002*\"frontend dev\" + 0.001*\"type\" + 0.001*\"right\" + 0.001*\"event\" + 0.001*\"weekly\" + 0.001*\"trainer\" + 0.001*\"tribute\" + 0.001*\"advance\" + 0.001*\"advance logic\"'),\n",
       " (12,\n",
       "  '0.003*\"interview call\" + 0.003*\"dev post yesterday\" + 0.003*\"experience real time project\" + 0.003*\"portfolio employer\" + 0.003*\"happy part platform\" + 0.002*\"happy\" + 0.001*\"yesterday\" + 0.001*\"first frontend dev job\" + 0.001*\"first period\" + 0.001*\"backend cert\" + 0.001*\"employer\" + 0.001*\"call\" + 0.001*\"platform\" + 0.001*\"xml\" + 0.001*\"real\" + 0.001*\"xsl\" + 0.001*\"second week new place\" + 0.001*\"period\" + 0.001*\"place\" + 0.001*\"post\"'),\n",
       " (13,\n",
       "  '0.001*\"cover\" + 0.001*\"letter\" + 0.001*\"candidate\" + 0.001*\"structure\" + 0.001*\"cv\" + 0.001*\"manager\" + 0.001*\"dom\" + 0.001*\"company\" + 0.001*\"cover letter\" + 0.001*\"others\" + 0.001*\"boilerplate\" + 0.001*\"framework\" + 0.001*\"algorithms\" + 0.001*\"problem\" + 0.001*\"people\" + 0.001*\"data\" + 0.001*\"hr\" + 0.001*\"failure\" + 0.001*\"react\" + 0.001*\"code\"'),\n",
       " (14,\n",
       "  '0.002*\"fullstack dev job offer\" + 0.002*\"codecs\" + 0.002*\"organisation\" + 0.001*\"quincy\" + 0.001*\"mean\" + 0.001*\"fullstack\" + 0.001*\"stack\" + 0.001*\"people across globe\" + 0.001*\"quora\" + 0.001*\"me.2\" + 0.001*\"master signal processing\" + 0.001*\"mean stack us\" + 0.001*\"home organisation\" + 0.001*\"remote work\" + 0.001*\"good concepts.3\" + 0.001*\"use google\" + 0.001*\"signal\" + 0.001*\"us\" + 0.001*\"great community\" + 0.001*\"india\"'),\n",
       " (15,\n",
       "  '0.000*\"partnership\" + 0.000*\"presentation communication skill\" + 0.000*\"project management\" + 0.000*\"progression\" + 0.000*\"patient\" + 0.000*\"non-technical\" + 0.000*\"outside\" + 0.000*\"order\" + 0.000*\"open new challenge new approach\" + 0.000*\"numerous interview offer\" + 0.000*\"numerous\" + 0.000*\"npm\" + 0.000*\"nothing month\" + 0.000*\"nothing\" + 0.000*\"nonsense\" + 0.000*\"none skills/technologies\" + 0.000*\"none\" + 0.000*\"people tech\" + 0.000*\"progression chance\" + 0.000*\"presentation\"'),\n",
       " (16,\n",
       "  '0.000*\"partnership\" + 0.000*\"presentation communication skill\" + 0.000*\"project management\" + 0.000*\"progression\" + 0.000*\"patient\" + 0.000*\"non-technical\" + 0.000*\"outside\" + 0.000*\"order\" + 0.000*\"open new challenge new approach\" + 0.000*\"numerous interview offer\" + 0.000*\"numerous\" + 0.000*\"npm\" + 0.000*\"nothing month\" + 0.000*\"nothing\" + 0.000*\"nonsense\" + 0.000*\"none skills/technologies\" + 0.000*\"none\" + 0.000*\"people tech\" + 0.000*\"progression chance\" + 0.000*\"presentation\"'),\n",
       " (17,\n",
       "  '0.002*\"upwork\" + 0.002*\"lot interest technology roadmap\" + 0.002*\"single company\" + 0.002*\"bachelor software technology\" + 0.002*\"cv/cv\" + 0.002*\"roadmap\" + 0.002*\"student dev within datetimetoken company\" + 0.002*\"role\" + 0.002*\"technology\" + 0.002*\"science\" + 0.002*\"bright\" + 0.002*\"good kind resource\" + 0.002*\"much time learning\" + 0.002*\"armenia\" + 0.002*\"month graduation\" + 0.002*\"political science\" + 0.002*\"codacademy\" + 0.002*\"bright datetimetoken\" + 0.002*\"local code camp armenia\" + 0.002*\"dev offer\"'),\n",
       " (18,\n",
       "  '0.001*\"junior\" + 0.001*\"london\" + 0.001*\"role\" + 0.001*\"decision\" + 0.001*\"ability\" + 0.001*\"building\" + 0.001*\"informed\" + 0.001*\"take\" + 0.001*\"final interview\" + 0.001*\"progression\" + 0.001*\"access\" + 0.001*\"box\" + 0.001*\"factor\" + 0.001*\"final\" + 0.001*\"progression chance\" + 0.001*\"senior\" + 0.001*\"etc\" + 0.001*\"skill\" + 0.001*\"sale\" + 0.001*\"method\"'),\n",
       " (19,\n",
       "  '0.002*\"datetimetoken half\" + 0.002*\"failure\" + 0.002*\"sleep\" + 0.002*\"dream\" + 0.002*\"response\" + 0.001*\"half\" + 0.001*\"mind\" + 0.001*\"posting\" + 0.001*\"datetimetoken old united state degree\" + 0.001*\"dev fcc”\" + 0.001*\"education background\" + 0.001*\"sacrifice\" + 0.001*\"fcc”\" + 0.001*\"interview offer\" + 0.001*\"job posting\" + 0.001*\"united\" + 0.001*\"last project\" + 0.001*\"datetimetoken life\" + 0.001*\"“how\" + 0.001*\"sacrifice life\"'),\n",
       " (20,\n",
       "  '0.001*\"universe\" + 0.001*\"rail\" + 0.001*\"webmaster\" + 0.001*\"graduate\" + 0.001*\"english\" + 0.001*\"fresh\" + 0.001*\"puzzle\" + 0.001*\"salary\" + 0.001*\"confidence\" + 0.001*\"passion\" + 0.001*\"love\" + 0.001*\"interviewer\" + 0.001*\"dev job\" + 0.001*\"recruiter\" + 0.001*\"level\" + 0.001*\"article\" + 0.001*\"second\" + 0.001*\"line\" + 0.001*\"entry level salary\" + 0.001*\"urge\"'),\n",
       " (21,\n",
       "  '0.002*\"time http\" + 0.002*\"http\" + 0.002*\"many many job application\" + 0.002*\"gaining\" + 0.002*\"development knowledge gaining experience fulltime job\" + 0.002*\"share others\" + 0.002*\"helpful tip\" + 0.002*\"internship couple freelance project\" + 0.002*\"little bit code free time\" + 0.002*\"whoo\" + 0.002*\"fulltime commitment\" + 0.001*\"graphic design datetimetoken\" + 0.001*\"subject\" + 0.001*\"second week new place\" + 0.001*\"share\" + 0.001*\"xsl\" + 0.001*\"helpful\" + 0.001*\"xml\" + 0.001*\"minute datetimetoken\" + 0.001*\"politics university\"'),\n",
       " (22,\n",
       "  '0.004*\"lawyer\" + 0.002*\"design\" + 0.002*\"journey datetimetoken\" + 0.002*\"previous programming knowledge\" + 0.002*\"thank much fcc\" + 0.002*\"first job frontend email dev digital marketing agency\" + 0.002*\"larson\" + 0.002*\"amazing platform community\" + 0.002*\"satisfied\" + 0.002*\"satisfied current job\" + 0.002*\"ideas.ataccama.com\" + 0.002*\"graphic\" + 0.002*\"medium article\" + 0.002*\"closure\" + 0.002*\"previous\" + 0.002*\"marketing\" + 0.002*\"digital\" + 0.001*\"look\" + 0.001*\"medium\" + 0.001*\"meetup\"'),\n",
       " (23,\n",
       "  '0.001*\"degree\" + 0.001*\"thing lot\" + 0.001*\"university\" + 0.001*\"current\" + 0.001*\"self-taught\" + 0.001*\"country\" + 0.001*\"internet\" + 0.001*\"lot\" + 0.001*\"without\" + 0.001*\"resource\" + 0.001*\"science\" + 0.001*\"field\" + 0.001*\"ireland\" + 0.001*\"lot different frontend project\" + 0.001*\"-of-fcc-gave-me-my-first-job-as-a-frontend-dev\" + 0.001*\"i\\'am\" + 0.001*\"ticket\" + 0.001*\"kind mentality internet place\" + 0.001*\"pragmatic\" + 0.001*\"“our policy\"'),\n",
       " (24,\n",
       "  '0.002*\"internship\" + 0.002*\"part learning process\" + 0.002*\"contributor\" + 0.002*\"belong\" + 0.002*\"school kid\" + 0.002*\"topic doesnt belong\" + 0.002*\"doesnt\" + 0.002*\"article share story resource\" + 0.002*\"community thank\" + 0.001*\"college\" + 0.001*\"python\" + 0.001*\"resource\" + 0.001*\"way\" + 0.001*\"dev job\" + 0.001*\"tracker\" + 0.001*\"college degree\" + 0.001*\"topic\" + 0.001*\"kid\" + 0.001*\"student\" + 0.001*\"angel.co\"')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=25, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dictionary.id2token[2298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "adwords 1581 0.00198948\n",
      "designer 1588 0.00131095\n",
      "boss 1584 0.00129881\n",
      "json 2959 0.00122844\n",
      "udacity 3334 0.00122844\n",
      "script 707 0.00120071\n",
      "app 323 0.00116174\n",
      "lead 3734 0.00113772\n",
      "linkedin 2648 0.000888094\n",
      "recruiter 685 0.000848897\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "happy part platform 1924 0.00196795\n",
      "portfolio employer 1926 0.00196795\n",
      "experience real time project 1923 0.00196795\n",
      "interview call 1925 0.00196795\n",
      "dev post yesterday 1922 0.00196795\n",
      "share 721 0.00169308\n",
      "topic doesnt belong 1935 0.00164048\n",
      "contributor 1930 0.00164048\n",
      "belong 1928 0.00164048\n",
      "doesnt 1931 0.00164048\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "path 64 0.00197845\n",
      "venezuela 130 0.0019701\n",
      "freelance 102 0.0014727\n",
      "intro 227 0.00133973\n",
      "object 634 0.00123151\n",
      "obligation 120 0.00109938\n",
      "freelance summer 104 0.00109938\n",
      "local dev job 116 0.00109938\n",
      "js technology deep way 114 0.00109938\n",
      "first dev job age 99 0.00109938\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "first frontend dev job 291 0.00200805\n",
      "backend cert 289 0.00200805\n",
      "first period 292 0.00200805\n",
      "xsl 298 0.00200805\n",
      "xml 297 0.00200805\n",
      "second week new place 296 0.00200805\n",
      "lol 959 0.00160576\n",
      "period 265 0.00152079\n",
      "place 294 0.0013932\n",
      "first 98 0.00132815\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "design 926 0.00223611\n",
      "graphic 941 0.00186303\n",
      "larson 2195 0.0015164\n",
      "previous programming knowledge 2196 0.0015164\n",
      "journey datetimetoken 2194 0.0015164\n",
      "thank much fcc 2197 0.0015164\n",
      "first job frontend email dev digital marketing agency 2193 0.0015164\n",
      "amazing platform community 2192 0.0015164\n",
      "http 2942 0.00145263\n",
      "digital 423 0.00144621\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "jquery 1730 0.0017115\n",
      "ad 310 0.00107257\n",
      "memphis 2106 0.000999778\n",
      "code base 2091 0.000999778\n",
      "hope project 2099 0.000999778\n",
      "person phone 2108 0.000999778\n",
      "link relevant project 2105 0.000999778\n",
      "ad stackoverflow company memphis 2087 0.000999778\n",
      "job future 2102 0.000999778\n",
      "call next datetimetoken 2089 0.000999778\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "lawyer 2262 0.00340731\n",
      "satisfied 2264 0.00182208\n",
      "satisfied current job 2265 0.00182208\n",
      "ideas.ataccama.com 2261 0.00182208\n",
      "medium article 2263 0.00154533\n",
      "student 2023 0.00118331\n",
      "medium 1680 0.00117948\n",
      "look 1049 0.00117948\n",
      "article 331 0.00116455\n",
      "current 395 0.00110668\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "day 2127 0.00346424\n",
      "skill 725 0.00148931\n",
      "group 943 0.00138864\n",
      "backend 89 0.00131238\n",
      "team 282 0.00128138\n",
      "prototype 1907 0.00127369\n",
      "data 399 0.00118936\n",
      "app 323 0.00117979\n",
      "people 65 0.00113764\n",
      "python 1739 0.00113115\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "success 742 0.00186696\n",
      "math 3648 0.00177609\n",
      "logic 1678 0.00172157\n",
      "job fulltime dev 1634 0.00130533\n",
      "thank success story 1642 0.00130533\n",
      "na 1638 0.00130533\n",
      "success story fcc forum 1641 0.00130533\n",
      "frontend cert along udemy course 1632 0.00130533\n",
      "post long time 1639 0.00130533\n",
      "proud 1640 0.00130533\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "junior 151 0.000966064\n",
      "london 2501 0.000869345\n",
      "role 859 0.000797953\n",
      "building 919 0.000791621\n",
      "ability 2400 0.000791621\n",
      "decision 413 0.000791621\n",
      "informed 2481 0.000748526\n",
      "progression 2544 0.000748526\n",
      "take 2586 0.000748526\n",
      "access 2402 0.000748526\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "#https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 2915\n",
      "maxdiv 7.202661196523238\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=25, wordimportance = wordimportance, unigrams_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.277*\"company\" + 0.223*\"interview\" + 0.213*\"project\" + 0.205*\"job\" + 0.185*\"time\" + 0.142*\"people\" + 0.138*\"day\" + 0.132*\"thing\" + 0.127*\"experience\" + 0.124*\"way\" + 0.124*\"code\" + 0.121*\"cv\" + 0.115*\"lot\" + 0.108*\"many\" + 0.106*\"month\" + 0.104*\"dev\" + 0.104*\"question\" + 0.097*\"fcc\" + 0.097*\"something\" + 0.094*\"challenge\"'),\n",
       " (1,\n",
       "  '0.164*\"fcc\" + 0.149*\"job\" + 0.139*\"recruiter\" + 0.129*\"week\" + 0.117*\"backend\" + 0.110*\"end\" + 0.109*\"project\" + 0.101*\"year\" + -0.101*\"cv\" + 0.100*\"day\" + -0.096*\"thing\" + 0.087*\"guy\" + 0.084*\"call\" + 0.084*\"level\" + 0.083*\"story\" + -0.082*\"code\" + -0.082*\"github\" + 0.078*\"agency\" + -0.076*\"problem\" + 0.076*\"salary\"'),\n",
       " (2,\n",
       "  '-0.225*\"dev\" + -0.220*\"job\" + 0.187*\"company\" + -0.169*\"fcc\" + -0.159*\"skill\" + -0.148*\"frontend\" + -0.143*\"lot\" + -0.135*\"year\" + -0.104*\"good\" + 0.103*\"recruiter\" + -0.103*\"course\" + -0.099*\"thank\" + 0.096*\"call\" + -0.093*\"resource\" + -0.091*\"opportunity\" + 0.084*\"level\" + -0.081*\"couple\" + -0.079*\"professional\" + 0.078*\"day\" + -0.077*\"month\"'),\n",
       " (3,\n",
       "  '0.163*\"dev\" + -0.155*\"junior\" + -0.118*\"opportunity\" + -0.115*\"role\" + -0.100*\"couple\" + -0.097*\"senior\" + -0.095*\"decision\" + -0.094*\"skill\" + -0.093*\"etc\" + -0.090*\"sale\" + -0.086*\"tech\" + -0.084*\"technology\" + -0.083*\"ability\" + -0.081*\"professional\" + -0.080*\"message\" + -0.080*\"technical\" + -0.080*\"way\" + -0.080*\"building\" + -0.077*\"employer\" + 0.077*\"thing\"'),\n",
       " (4,\n",
       "  '-0.146*\"lot\" + -0.140*\"fcc\" + 0.122*\"email\" + -0.107*\"year\" + 0.098*\"interview\" + -0.092*\"backend\" + 0.091*\"week\" + -0.091*\"degree\" + 0.090*\"learning\" + 0.078*\"salary\" + -0.077*\"dream\" + -0.076*\"fullstack\" + -0.076*\"field\" + 0.075*\"rate\" + 0.075*\"environment\" + 0.074*\"parttime\" + -0.074*\"coding\" + -0.074*\"startup\" + 0.072*\"gig\" + 0.071*\"marketing\"'),\n",
       " (5,\n",
       "  '0.147*\"fcc\" + -0.131*\"something\" + -0.121*\"lead\" + -0.115*\"designer\" + -0.114*\"someone\" + -0.109*\"time\" + -0.108*\"fact\" + -0.102*\"udacity\" + -0.101*\"tip\" + -0.101*\"track\" + -0.100*\"try\" + -0.099*\"json\" + -0.099*\"life\" + -0.098*\"portfolio\" + -0.094*\"js\" + -0.093*\"top\" + -0.092*\"linkedin\" + -0.090*\"area\" + -0.087*\"basic\" + 0.085*\"job\"'),\n",
       " (6,\n",
       "  '-0.191*\"degree\" + -0.183*\"course\" + 0.163*\"skill\" + -0.153*\"lot\" + -0.148*\"current\" + 0.138*\"fcc\" + -0.127*\"resource\" + -0.125*\"science\" + -0.113*\"chance\" + 0.112*\"good\" + -0.103*\"college\" + -0.103*\"computer\" + 0.100*\"dev\" + -0.097*\"university\" + -0.095*\"little\" + 0.094*\"city\" + 0.093*\"startup\" + -0.093*\"programming\" + 0.089*\"backend\" + -0.087*\"year\"'),\n",
       " (7,\n",
       "  '-0.166*\"lot\" + 0.160*\"dev\" + -0.153*\"skill\" + -0.125*\"backend\" + 0.118*\"js\" + -0.112*\"country\" + 0.105*\"language\" + -0.105*\"pay\" + 0.103*\"html\" + 0.101*\"community\" + -0.100*\"degree\" + -0.099*\"everyone\" + -0.096*\"good\" + -0.095*\"experience\" + -0.095*\"internet\" + 0.093*\"hi\" + 0.090*\"day\" + -0.087*\"fullstack\" + 0.086*\"first\" + -0.086*\"field\"'),\n",
       " (8,\n",
       "  '-0.157*\"language\" + -0.149*\"web\" + 0.141*\"project\" + -0.138*\"python\" + -0.129*\"team\" + 0.128*\"dev\" + -0.125*\"skill\" + 0.120*\"phone\" + -0.106*\"friend\" + -0.102*\"data\" + 0.101*\"thing\" + -0.093*\"website\" + 0.093*\"communication\" + -0.091*\"programming\" + -0.089*\"part\" + 0.086*\"online\" + 0.085*\"world\" + 0.083*\"class\" + 0.079*\"detail\" + 0.079*\"way\"'),\n",
       " (9,\n",
       "  '-0.181*\"design\" + 0.146*\"team\" + -0.138*\"basic\" + 0.137*\"week\" + -0.136*\"position\" + -0.111*\"month\" + -0.106*\"html\" + 0.102*\"international\" + -0.100*\"css\" + -0.099*\"new\" + -0.098*\"graphic\" + 0.093*\"interest\" + 0.089*\"python\" + -0.088*\"day\" + -0.084*\"challenge\" + -0.081*\"js\" + 0.079*\"way\" + 0.079*\"large\" + -0.075*\"support\" + 0.074*\"computer\"'),\n",
       " (10,\n",
       "  '-0.162*\"path\" + 0.141*\"dev\" + 0.141*\"confidence\" + 0.129*\"salary\" + 0.106*\"year\" + -0.105*\"project\" + 0.100*\"article\" + -0.098*\"good\" + -0.096*\"test\" + -0.094*\"much\" + 0.094*\"graduate\" + -0.092*\"december\" + 0.092*\"line\" + -0.089*\"community\" + -0.089*\"career\" + 0.089*\"html\" + 0.088*\"fresh\" + -0.087*\"js\" + -0.084*\"month\" + 0.082*\"interviewer\"'),\n",
       " (11,\n",
       "  '0.142*\"js\" + -0.133*\"job\" + 0.132*\"project\" + 0.130*\"path\" + -0.127*\"design\" + -0.116*\"learning\" + 0.114*\"career\" + -0.100*\"fulltime\" + -0.098*\"new\" + 0.097*\"much\" + 0.096*\"foundation\" + -0.088*\"coding\" + 0.084*\"right\" + 0.083*\"employee\" + 0.082*\"additional\" + -0.082*\"challenge\" + -0.081*\"bachelor\" + -0.081*\"role\" + -0.081*\"living\" + 0.081*\"strong\"'),\n",
       " (12,\n",
       "  '0.128*\"js\" + -0.126*\"goal\" + -0.126*\"resource\" + 0.122*\"fcc\" + -0.120*\"knowledge\" + -0.118*\"dev\" + 0.115*\"something\" + -0.110*\"process\" + 0.108*\"project\" + -0.106*\"app\" + -0.102*\"college\" + -0.096*\"passionate\" + -0.095*\"time\" + -0.094*\"learning\" + -0.091*\"study\" + 0.088*\"design\" + -0.084*\"everything\" + 0.082*\"position\" + 0.076*\"online\" + -0.074*\"entire\"'),\n",
       " (13,\n",
       "  '0.214*\"design\" + 0.146*\"support\" + 0.138*\"graphic\" + -0.132*\"community\" + -0.121*\"day\" + 0.117*\"basic\" + 0.107*\"role\" + 0.105*\"language\" + -0.104*\"book\" + 0.095*\"tutorial\" + 0.093*\"engineering\" + 0.090*\"fulltime\" + -0.089*\"something\" + -0.087*\"git\" + -0.086*\"beta\" + -0.084*\"coding\" + -0.083*\"function\" + -0.083*\"heart\" + 0.082*\"photoshop\" + 0.080*\"environment\"'),\n",
       " (14,\n",
       "  '0.139*\"day\" + -0.136*\"fulltime\" + 0.123*\"app\" + -0.121*\"couple\" + 0.121*\"project\" + -0.106*\"summer\" + 0.106*\"website\" + -0.106*\"learning\" + 0.105*\"skill\" + 0.104*\"thought\" + -0.103*\"takeaway\" + -0.101*\"profile\" + -0.099*\"client\" + -0.098*\"california\" + 0.088*\"study\" + -0.086*\"year\" + 0.084*\"exercise\" + -0.083*\"gig\" + 0.083*\"networking\" + 0.082*\"knowledge\"'),\n",
       " (15,\n",
       "  '0.174*\"year\" + 0.163*\"fcc\" + 0.151*\"interview\" + -0.146*\"dev\" + 0.131*\"employer\" + 0.116*\"career\" + -0.111*\"app\" + 0.101*\"php\" + 0.099*\"december\" + -0.097*\"client\" + 0.096*\"class\" + -0.095*\"study\" + -0.094*\"resource\" + 0.093*\"path\" + 0.087*\"in-person\" + 0.086*\"science\" + -0.082*\"regular\" + -0.081*\"journey\" + -0.080*\"experience\" + 0.079*\"intro\"'),\n",
       " (16,\n",
       "  '-0.149*\"test\" + 0.125*\"science\" + 0.125*\"role\" + -0.113*\"class\" + 0.106*\"current\" + -0.105*\"position\" + -0.097*\"css\" + 0.096*\"environment\" + 0.095*\"way\" + -0.094*\"lot\" + -0.089*\"design\" + 0.088*\"goal\" + 0.087*\"employee\" + 0.085*\"project\" + -0.083*\"html\" + 0.083*\"forum\" + -0.082*\"college\" + -0.081*\"december\" + 0.081*\"support\" + -0.080*\"study\"'),\n",
       " (17,\n",
       "  '0.178*\"day\" + 0.164*\"frontend\" + 0.106*\"course\" + 0.105*\"profile\" + -0.104*\"goal\" + 0.104*\"gig\" + -0.101*\"people\" + -0.100*\"passionate\" + 0.095*\"way\" + -0.093*\"session\" + -0.092*\"night\" + -0.090*\"resource\" + 0.088*\"year\" + 0.083*\"fulltime\" + 0.083*\"cert\" + 0.082*\"tuesday\" + -0.082*\"approach\" + 0.081*\"contract\" + -0.081*\"fact\" + 0.080*\"client\"'),\n",
       " (18,\n",
       "  '-0.146*\"people\" + 0.132*\"course\" + -0.131*\"app\" + 0.120*\"year\" + -0.101*\"group\" + -0.095*\"luck\" + -0.095*\"skill\" + -0.094*\"networking\" + -0.087*\"website\" + -0.087*\"mobile\" + 0.083*\"day\" + -0.081*\"weekly\" + 0.080*\"frontend\" + 0.080*\"story\" + -0.079*\"couple\" + -0.079*\"lot\" + -0.078*\"strong\" + -0.078*\"profile\" + -0.077*\"internet\" + 0.077*\"offer\"'),\n",
       " (19,\n",
       "  '0.164*\"role\" + 0.153*\"college\" + 0.149*\"programming\" + 0.111*\"current\" + -0.107*\"frontend\" + 0.101*\"class\" + -0.101*\"course\" + 0.100*\"support\" + 0.099*\"question\" + 0.097*\"aspect\" + 0.086*\"challenge\" + -0.085*\"day\" + 0.083*\"cert\" + -0.083*\"project\" + 0.083*\"situation\" + -0.082*\"skype\" + 0.080*\"infrastructure\" + 0.079*\"engineering\" + 0.078*\"dev\" + -0.077*\"position\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"edx\" + 0.006*\"//manuelbasanta.github.io/\" + 0.006*\"aproach\" + 0.006*\"awsome\" + 0.006*\"increadible\" + 0.006*\"chellanges\" + 0.006*\"cs50\" + 0.005*\"someone\" + 0.005*\"quincy\" + 0.005*\"august\" + 0.005*\"angularjs\" + 0.004*\"thank\" + 0.004*\"website\" + 0.004*\"reason\" + 0.004*\"right\" + 0.004*\"encouragement\" + 0.004*\"helpful\" + 0.004*\"https\" + 0.004*\"week\" + 0.004*\"frontend\"'),\n",
       " (1,\n",
       "  '0.006*\"hi\" + 0.006*\"local\" + 0.006*\"someone\" + 0.005*\"scratch\" + 0.005*\"new\" + 0.004*\"bright\" + 0.004*\"armenia\" + 0.004*\"administrator\" + 0.004*\"circle\" + 0.004*\"wednesday\" + 0.004*\"visualization\" + 0.004*\"3-year-old\" + 0.004*\"codacademy\" + 0.004*\"urban\" + 0.004*\"gardening\" + 0.004*\"io\" + 0.004*\"lynday.com\" + 0.004*\"clerk\" + 0.004*\"firebase\" + 0.004*\"5-6am\"'),\n",
       " (2,\n",
       "  '0.002*\"fccers\" + 0.002*\"mine\" + 0.002*\"public\" + 0.002*\"yesterday\" + 0.002*\"office\" + 0.002*\"aspect\" + 0.002*\"someone\" + 0.002*\"journey\" + 0.002*\"whole\" + 0.002*\"sense\" + 0.002*\"working\" + 0.002*\"command\" + 0.002*\"brain\" + 0.002*\"css\" + 0.002*\"last\" + 0.002*\"scalable\" + 0.002*\"“thank\" + 0.002*\"starting\" + 0.002*\"mini-lessons\" + 0.002*\"fashion\"'),\n",
       " (3,\n",
       "  '0.004*\"frontend\" + 0.004*\"life\" + 0.004*\"degree\" + 0.004*\"chance\" + 0.003*\"world\" + 0.003*\"year\" + 0.003*\"dev\" + 0.003*\"state\" + 0.003*\"month\" + 0.003*\"part\" + 0.003*\"process\" + 0.003*\"course\" + 0.003*\"fcc\" + 0.003*\"education\" + 0.003*\"lot\" + 0.003*\"basic\" + 0.003*\"couple\" + 0.003*\"goal\" + 0.003*\"experience\" + 0.003*\"entry\"'),\n",
       " (4,\n",
       "  '0.005*\"someone\" + 0.004*\"contribution\" + 0.004*\"month\" + 0.004*\"website\" + 0.004*\"position\" + 0.004*\"workplace\" + 0.004*\"stuck\" + 0.004*\"material\" + 0.004*\"monday\" + 0.004*\"response\" + 0.004*\"curriculum\" + 0.004*\"new\" + 0.004*\"mobile\" + 0.003*\"week\" + 0.003*\"site\" + 0.003*\"java\" + 0.003*\"camper\" + 0.003*\"dev\" + 0.003*\"email\" + 0.003*\"career\"'),\n",
       " (5,\n",
       "  '0.003*\"quincylarson\" + 0.003*\"amazing\" + 0.003*\"game\" + 0.003*\"coding\" + 0.003*\"bit\" + 0.003*\"background\" + 0.003*\"board\" + 0.003*\"dungeon\" + 0.003*\"scores/hundreds\" + 0.002*\"limited\" + 0.002*\"anthropology\" + 0.002*\"position\" + 0.002*\"please\" + 0.002*\"strike\" + 0.002*\"picture\" + 0.002*\"path\" + 0.002*\"non-technical\" + 0.002*\"shout-out\" + 0.002*\"“a-ha”\" + 0.002*\"team\"'),\n",
       " (6,\n",
       "  '0.006*\"hi\" + 0.005*\"language\" + 0.005*\"today\" + 0.005*\"frontend\" + 0.005*\"cert\" + 0.004*\"fcc\" + 0.004*\"css\" + 0.004*\"post\" + 0.004*\"content\" + 0.004*\"bachelor\" + 0.004*\"current\" + 0.004*\"networking\" + 0.004*\"attempt\" + 0.004*\"html\" + 0.004*\"automation\" + 0.004*\"fortune\" + 0.004*\"ideas.ataccama.com\" + 0.004*\"nyc\" + 0.004*\"dead\" + 0.004*\"restful\"'),\n",
       " (7,\n",
       "  '0.006*\"today\" + 0.005*\"big\" + 0.005*\"edit\" + 0.004*\"question\" + 0.004*\"type\" + 0.004*\"dev\" + 0.004*\"idea\" + 0.004*\"example\" + 0.004*\"stuff\" + 0.004*\"flaw\" + 0.004*\"well.and\" + 0.004*\"zeppelin\" + 0.004*\"edit:3.\" + 0.004*\"\\'yes\" + 0.004*\"progressing\" + 0.004*\"sake\" + 0.004*\"strength\" + 0.004*\"frontend\" + 0.004*\"commitment\" + 0.004*\"self\"'),\n",
       " (8,\n",
       "  '0.001*\"pair-program\" + 0.001*\"shop\" + 0.001*\"value\" + 0.001*\"session\" + 0.001*\"non-trivial\" + 0.001*\"free\" + 0.001*\"cycle\" + 0.001*\"dozen\" + 0.001*\"“summary”\" + 0.001*\"round\" + 0.001*\"time/space\" + 0.001*\"facebook\" + 0.001*\"writing\" + 0.001*\"“fourteener”\" + 0.001*\"conversation\" + 0.001*\"java\" + 0.001*\"imposter\" + 0.001*\"legal\" + 0.001*\"evidence\" + 0.001*\"recruiting\"'),\n",
       " (9,\n",
       "  '0.005*\"thank\" + 0.005*\"platform\" + 0.005*\"part\" + 0.005*\"roadmap\" + 0.005*\"cv/cv\" + 0.005*\"day\" + 0.004*\"course\" + 0.004*\"dev\" + 0.004*\"portfolio\" + 0.004*\"single\" + 0.004*\"reply\" + 0.004*\"something\" + 0.004*\"real\" + 0.004*\"bachelor\" + 0.004*\"within\" + 0.004*\"project\" + 0.003*\"p1xt\" + 0.003*\"coffeescript\" + 0.003*\"wall\" + 0.003*\"prototypal\"'),\n",
       " (10,\n",
       "  '0.005*\"easy-to-use\" + 0.005*\"reply\" + 0.005*\"paramount\" + 0.005*\"ever-increasing\" + 0.005*\"pocket\" + 0.005*\"qualified\" + 0.005*\"digital\" + 0.005*\"total\" + 0.004*\"ceo\" + 0.004*\"mail\" + 0.004*\"whole\" + 0.004*\"thursday\" + 0.004*\"invitation\" + 0.004*\"bunch\" + 0.004*\"side\" + 0.004*\"desire\" + 0.004*\"comfort\" + 0.004*\"ok\" + 0.004*\"temporary\" + 0.004*\"latter\"'),\n",
       " (11,\n",
       "  '0.007*\"mern\" + 0.007*\"proud\" + 0.007*\"holiday\" + 0.006*\"local\" + 0.006*\"fulltime\" + 0.005*\"forum\" + 0.005*\"story\" + 0.005*\"journey\" + 0.005*\"cert\" + 0.005*\"hi\" + 0.005*\"udemy\" + 0.005*\"library\" + 0.005*\"along\" + 0.005*\"na\" + 0.005*\"additional\" + 0.004*\"year\" + 0.004*\"long\" + 0.004*\"next\" + 0.004*\"everyone\" + 0.004*\"success\"'),\n",
       " (12,\n",
       "  '0.005*\"knowledge\" + 0.004*\"fulltime\" + 0.004*\"many\" + 0.004*\"bit\" + 0.004*\"share\" + 0.004*\"coding\" + 0.004*\"frontend\" + 0.004*\"project\" + 0.004*\"stuff\" + 0.004*\"community\" + 0.003*\"dev\" + 0.003*\"lot\" + 0.003*\"day\" + 0.003*\"story\" + 0.003*\"thing\" + 0.003*\"phone\" + 0.003*\"time\" + 0.003*\"monster\" + 0.003*\"hi\" + 0.003*\"first\"'),\n",
       " (13,\n",
       "  '0.009*\"obligation\" + 0.008*\"deep\" + 0.008*\"age\" + 0.008*\"food\" + 0.008*\"spent\" + 0.008*\"november\" + 0.008*\"reading\" + 0.008*\"night\" + 0.007*\"solid\" + 0.006*\"summer\" + 0.006*\"home\" + 0.005*\"local\" + 0.005*\"path\" + 0.005*\"freelance\" + 0.005*\"hour\" + 0.005*\"cert\" + 0.005*\"coding\" + 0.005*\"cvs\" + 0.005*\"college\" + 0.004*\"post\"'),\n",
       " (14,\n",
       "  '0.009*\"digital\" + 0.009*\"previous\" + 0.007*\"marketing\" + 0.007*\"larson\" + 0.007*\"agency\" + 0.007*\"hi\" + 0.007*\"programming\" + 0.006*\"february\" + 0.005*\"apprenticeship\" + 0.005*\"offer/trainee\" + 0.005*\"platform\" + 0.005*\"optimisation…\" + 0.005*\"thank\" + 0.005*\"amazing\" + 0.005*\"execution\" + 0.005*\"foreign\" + 0.005*\"arises\" + 0.005*\"commercial\" + 0.005*\"diy-ing\" + 0.005*\"spark\"'),\n",
       " (15,\n",
       "  '0.006*\"concepts.3\" + 0.006*\"quora\" + 0.006*\"postgresql\" + 0.006*\"dd+\" + 0.006*\"atmos\" + 0.006*\"india\" + 0.006*\"processing\" + 0.006*\"audio\" + 0.006*\"\\'why\" + 0.006*\"signal\" + 0.006*\"dolby\" + 0.006*\"\\'how\" + 0.006*\"bottom\" + 0.006*\"cliche\" + 0.006*\"channel\" + 0.006*\"video\" + 0.006*\"me.2\" + 0.005*\"cheer\" + 0.005*\"globe\" + 0.005*\"uk\"'),\n",
       " (16,\n",
       "  '0.004*\"extensive\" + 0.003*\"relief\" + 0.003*\"accelerator\" + 0.003*\"rwd\" + 0.003*\"alphago\" + 0.003*\"man\" + 0.003*\"fccer\" + 0.003*\"lee\" + 0.003*\"non-cs-degree\" + 0.003*\"anywho\" + 0.003*\"designing\" + 0.003*\"progressive\" + 0.003*\"employment\" + 0.003*\"statistic\" + 0.003*\"sedol\" + 0.003*\"knowledge\" + 0.003*\"public\" + 0.003*\"yesterday\" + 0.003*\"mine\" + 0.003*\"hackathon.3\"'),\n",
       " (17,\n",
       "  '0.006*\"community\" + 0.006*\"part\" + 0.005*\"hi\" + 0.005*\"anything\" + 0.005*\"thank\" + 0.005*\"js\" + 0.004*\"resource\" + 0.004*\"large\" + 0.004*\"way\" + 0.004*\"guy\" + 0.004*\"year\" + 0.004*\"topic\" + 0.004*\"life\" + 0.004*\"agency\" + 0.004*\"half\" + 0.004*\"dev\" + 0.004*\"learning\" + 0.004*\"programming\" + 0.004*\"last\" + 0.003*\"huge\"'),\n",
       " (18,\n",
       "  '0.007*\"hi\" + 0.006*\"july\" + 0.006*\"stuff\" + 0.006*\"css\" + 0.006*\"month\" + 0.005*\"week\" + 0.005*\"html\" + 0.005*\"guy\" + 0.004*\"camper\" + 0.004*\"js\" + 0.004*\"clue\" + 0.004*\"tecnology\" + 0.004*\"colt\" + 0.004*\"birthday\" + 0.004*\"tomorrow\" + 0.004*\"steele\" + 0.004*\"teller\" + 0.004*\"untill\" + 0.004*\"stable\" + 0.004*\"region.and\"'),\n",
       " (19,\n",
       "  '0.005*\"programming\" + 0.005*\"basic\" + 0.005*\"website\" + 0.005*\"wordpress\" + 0.005*\"week\" + 0.004*\"salary\" + 0.004*\"anyone\" + 0.004*\"nothing\" + 0.004*\"month\" + 0.004*\"diploma\" + 0.004*\"2-3\" + 0.004*\"relation\" + 0.004*\"dps\" + 0.004*\"flexible\" + 0.004*\"html/css/php\" + 0.004*\"profit\" + 0.004*\"cad\" + 0.004*\"money\" + 0.004*\"computer\" + 0.004*\"personal\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "edx 1802 0.0056916\n",
      "increadible 1803 0.0056916\n",
      "chellanges 1800 0.0056916\n",
      "aproach 1798 0.0056916\n",
      "awsome 1799 0.0056916\n",
      "//manuelbasanta.github.io/ 1797 0.0056916\n",
      "cs50 1801 0.0056916\n",
      "someone 124 0.00521736\n",
      "quincy 53 0.00486393\n",
      "august 247 0.00464831\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "hi 34 0.00565628\n",
      "local 85 0.00561472\n",
      "someone 124 0.00559863\n",
      "scratch 928 0.00505271\n",
      "new 210 0.00456847\n",
      "armenia 997 0.00439323\n",
      "administrator 1101 0.00439323\n",
      "bright 998 0.00439323\n",
      "wednesday 1123 0.00439323\n",
      "circle 1102 0.00439323\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "fccers 1072 0.00225834\n",
      "mine 1083 0.00205679\n",
      "public 453 0.00201412\n",
      "yesterday 602 0.00201329\n",
      "office 185 0.0019428\n",
      "aspect 535 0.00192827\n",
      "someone 124 0.00189928\n",
      "journey 761 0.00184149\n",
      "whole 526 0.00179895\n",
      "sense 812 0.00176095\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "frontend 79 0.00403885\n",
      "life 113 0.0040113\n",
      "degree 144 0.00362329\n",
      "chance 138 0.00356695\n",
      "world 655 0.00348695\n",
      "year 66 0.00345624\n",
      "dev 22 0.00338348\n",
      "state 1277 0.0033541\n",
      "month 43 0.00323479\n",
      "part 47 0.0032098\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "someone 124 0.00469368\n",
      "contribution 1183 0.00444039\n",
      "month 43 0.00438728\n",
      "website 131 0.00438239\n",
      "position 442 0.00423777\n",
      "workplace 834 0.00411286\n",
      "stuck 127 0.00408279\n",
      "material 773 0.00388939\n",
      "monday 117 0.00377313\n",
      "response 806 0.00366699\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "quincylarson 637 0.00325901\n",
      "amazing 230 0.00322437\n",
      "game 351 0.00295513\n",
      "coding 71 0.00292116\n",
      "bit 258 0.00280098\n",
      "background 251 0.00276643\n",
      "board 260 0.00275382\n",
      "dungeon 1463 0.00264847\n",
      "scores/hundreds 1406 0.00251186\n",
      "limited 1371 0.00248978\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "hi 34 0.00632697\n",
      "language 557 0.00539106\n",
      "today 508 0.00522187\n",
      "frontend 79 0.00507057\n",
      "cert 70 0.00473314\n",
      "fcc 26 0.00447411\n",
      "css 733 0.00438637\n",
      "post 90 0.00418708\n",
      "content 540 0.0040567\n",
      "bachelor 539 0.00404557\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "today 508 0.00567963\n",
      "big 664 0.00541987\n",
      "edit 738 0.00455206\n",
      "question 198 0.00449956\n",
      "type 824 0.00448998\n",
      "dev 22 0.00432183\n",
      "idea 110 0.00429205\n",
      "example 327 0.00417698\n",
      "stuff 202 0.00416299\n",
      "flaw 1233 0.0039661\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "pair-program 1598 0.000660506\n",
      "shop 1019 0.00065582\n",
      "value 702 0.000654391\n",
      "session 929 0.000649294\n",
      "non-trivial 1589 0.000647941\n",
      "free 747 0.00064757\n",
      "cycle 1518 0.000643248\n",
      "dozen 309 0.000640647\n",
      "“summary” 1689 0.000640405\n",
      "round 1630 0.000639903\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "thank 58 0.00546569\n",
      "platform 858 0.00528886\n",
      "part 47 0.00493383\n",
      "roadmap 1175 0.00466894\n",
      "cv/cv 1174 0.00466894\n",
      "day 74 0.00463998\n",
      "course 18 0.00422395\n",
      "dev 22 0.0042132\n",
      "portfolio 441 0.00408016\n",
      "single 1176 0.0039907\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 4781\n",
      "maxdiv 7.198931240688173\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = wordimportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.276*\"company\" + 0.220*\"interview\" + 0.212*\"project\" + 0.204*\"job\" + 0.185*\"time\" + 0.141*\"people\" + 0.132*\"thing\" + 0.128*\"datetimetoken\" + 0.127*\"experience\" + 0.124*\"way\" + 0.123*\"code\" + 0.121*\"cv\" + 0.114*\"lot\" + 0.108*\"many\" + 0.105*\"month\" + 0.104*\"question\" + 0.103*\"dev\" + 0.097*\"something\" + 0.096*\"fcc\" + 0.094*\"challenge\"'),\n",
       " (1,\n",
       "  '0.155*\"datetimetoken\" + 0.154*\"fcc\" + 0.140*\"job\" + 0.137*\"recruiter\" + 0.126*\"week\" + 0.113*\"backend\" + 0.108*\"end\" + 0.107*\"project\" + -0.097*\"cv\" + -0.093*\"thing\" + 0.085*\"guy\" + 0.084*\"call\" + 0.083*\"level\" + 0.080*\"story\" + -0.079*\"github\" + -0.079*\"code\" + 0.077*\"agency\" + 0.074*\"interview\" + 0.073*\"salary\" + -0.073*\"problem\"'),\n",
       " (2,\n",
       "  '-0.219*\"dev\" + -0.219*\"job\" + 0.183*\"company\" + -0.162*\"fcc\" + -0.152*\"skill\" + -0.140*\"frontend\" + -0.134*\"lot\" + -0.100*\"chance\" + -0.099*\"good\" + -0.098*\"thank\" + 0.098*\"recruiter\" + -0.097*\"course\" + -0.093*\"resource\" + 0.091*\"call\" + -0.081*\"couple\" + -0.081*\"month\" + 0.079*\"level\" + -0.076*\"professional\" + -0.073*\"css\" + 0.069*\"moment\"'),\n",
       " (3,\n",
       "  '0.135*\"junior\" + -0.131*\"dev\" + 0.101*\"chance\" + 0.099*\"role\" + 0.096*\"skill\" + 0.084*\"couple\" + 0.083*\"senior\" + 0.081*\"decision\" + 0.079*\"etc\" + 0.079*\"sale\" + 0.079*\"professional\" + -0.078*\"salary\" + 0.078*\"tech\" + -0.078*\"thing\" + 0.075*\"life\" + -0.074*\"month\" + 0.073*\"project\" + 0.073*\"technology\" + 0.071*\"ability\" + 0.070*\"employer\"'),\n",
       " (4,\n",
       "  '0.149*\"fcc\" + 0.149*\"lot\" + -0.109*\"interview\" + -0.106*\"email\" + 0.096*\"dev\" + 0.093*\"degree\" + 0.079*\"backend\" + -0.074*\"rate\" + 0.073*\"field\" + -0.073*\"junior\" + 0.072*\"dream\" + -0.071*\"learning\" + -0.071*\"couple\" + 0.070*\"coding\" + 0.069*\"startup\" + -0.069*\"command\" + 0.067*\"city\" + 0.066*\"fullstack\" + -0.065*\"responsive\" + -0.065*\"template\"'),\n",
       " (5,\n",
       "  '-0.138*\"fcc\" + 0.116*\"lead\" + 0.115*\"something\" + 0.109*\"designer\" + 0.108*\"someone\" + 0.102*\"time\" + 0.101*\"tip\" + 0.099*\"fact\" + 0.096*\"track\" + 0.096*\"try\" + 0.095*\"json\" + 0.095*\"udacity\" + 0.092*\"portfolio\" + 0.087*\"linkedin\" + -0.087*\"lot\" + 0.086*\"area\" + -0.084*\"job\" + 0.084*\"life\" + 0.083*\"top\" + 0.080*\"encouragement\"'),\n",
       " (6,\n",
       "  '-0.201*\"degree\" + -0.166*\"lot\" + -0.153*\"course\" + 0.151*\"fcc\" + -0.146*\"current\" + 0.134*\"dev\" + -0.125*\"chance\" + -0.125*\"resource\" + 0.121*\"skill\" + -0.117*\"science\" + -0.105*\"university\" + -0.101*\"college\" + -0.094*\"computer\" + -0.093*\"little\" + 0.090*\"good\" + -0.084*\"life\" + 0.083*\"city\" + 0.080*\"startup\" + -0.075*\"post\" + -0.075*\"\\'real\"'),\n",
       " (7,\n",
       "  '0.180*\"skill\" + -0.173*\"datetimetoken\" + 0.138*\"backend\" + 0.133*\"lot\" + -0.120*\"js\" + 0.115*\"pay\" + 0.114*\"good\" + -0.112*\"dev\" + -0.105*\"language\" + -0.105*\"community\" + 0.101*\"everyone\" + 0.101*\"country\" + -0.101*\"hi\" + 0.100*\"fullstack\" + -0.099*\"programming\" + 0.095*\"experience\" + 0.092*\"startup\" + -0.090*\"html\" + 0.086*\"dream\" + -0.082*\"course\"'),\n",
       " (8,\n",
       "  '-0.144*\"language\" + -0.138*\"python\" + -0.137*\"team\" + -0.137*\"web\" + 0.134*\"dev\" + 0.126*\"project\" + -0.125*\"skill\" + 0.114*\"phone\" + -0.109*\"friend\" + -0.101*\"data\" + 0.094*\"thing\" + -0.090*\"website\" + 0.086*\"communication\" + -0.083*\"part\" + -0.082*\"programming\" + 0.082*\"online\" + -0.079*\"head\" + 0.077*\"world\" + 0.077*\"class\" + -0.076*\"business\"'),\n",
       " (9,\n",
       "  '0.159*\"design\" + -0.142*\"team\" + -0.134*\"week\" + 0.129*\"basic\" + 0.117*\"position\" + 0.108*\"new\" + 0.098*\"html\" + -0.097*\"international\" + -0.092*\"python\" + 0.092*\"challenge\" + -0.090*\"interest\" + 0.090*\"css\" + 0.089*\"month\" + -0.086*\"way\" + 0.086*\"experience\" + 0.081*\"graphic\" + -0.075*\"large\" + -0.071*\"head\" + 0.070*\"living\" + -0.070*\"phone\"'),\n",
       " (10,\n",
       "  '-0.143*\"dev\" + 0.137*\"path\" + -0.136*\"confidence\" + -0.133*\"salary\" + 0.107*\"datetimetoken\" + -0.099*\"graduate\" + -0.097*\"article\" + 0.095*\"month\" + -0.094*\"line\" + -0.092*\"fresh\" + -0.090*\"html\" + -0.090*\"web\" + 0.088*\"test\" + 0.086*\"project\" + 0.085*\"good\" + -0.084*\"interviewer\" + -0.082*\"english\" + 0.082*\"community\" + -0.082*\"attraction\" + -0.082*\"beautiful\"'),\n",
       " (11,\n",
       "  '0.127*\"js\" + 0.127*\"path\" + -0.123*\"job\" + 0.117*\"project\" + 0.112*\"career\" + 0.104*\"much\" + 0.103*\"dev\" + -0.100*\"coding\" + 0.091*\"foundation\" + 0.085*\"time\" + -0.082*\"life\" + 0.081*\"right\" + 0.081*\"additional\" + 0.080*\"object\" + 0.080*\"employee\" + 0.079*\"wife\" + -0.078*\"platform\" + -0.075*\"hour\" + -0.074*\"learning\" + 0.072*\"strong\"'),\n",
       " (12,\n",
       "  '-0.161*\"js\" + -0.139*\"something\" + -0.109*\"project\" + 0.106*\"resource\" + 0.103*\"knowledge\" + -0.096*\"git\" + 0.094*\"app\" + -0.092*\"codepen\" + 0.092*\"job\" + 0.091*\"bachelor\" + 0.091*\"dev\" + 0.088*\"learning\" + 0.086*\"goal\" + -0.082*\"fcc\" + 0.079*\"background\" + 0.078*\"college\" + -0.077*\"path\" + -0.077*\"section\" + 0.075*\"role\" + 0.074*\"different\"'),\n",
       " (13,\n",
       "  '-0.241*\"design\" + -0.149*\"support\" + -0.148*\"graphic\" + -0.113*\"tutorial\" + -0.112*\"role\" + -0.104*\"basic\" + -0.102*\"language\" + 0.096*\"community\" + -0.092*\"engineering\" + -0.087*\"photoshop\" + -0.086*\"position\" + 0.085*\"question\" + -0.084*\"new\" + 0.083*\"resource\" + -0.081*\"seo optimisation…\" + -0.081*\"seo\" + -0.081*\"optimisation…\" + -0.081*\"paid\" + -0.081*\"apprenticeship\" + -0.081*\"spark\"'),\n",
       " (14,\n",
       "  '-0.166*\"fcc\" + -0.142*\"datetimetoken\" + 0.138*\"fulltime\" + 0.127*\"client\" + -0.124*\"project\" + 0.113*\"dev\" + 0.106*\"profile\" + 0.101*\"everything\" + -0.100*\"networking\" + -0.099*\"people\" + 0.094*\"experience\" + 0.090*\"idea\" + -0.090*\"skype\" + 0.086*\"coding\" + 0.085*\"entry\" + 0.085*\"communication\" + -0.083*\"person\" + 0.082*\"upwork\" + 0.080*\"living\" + 0.076*\"summer\"'),\n",
       " (15,\n",
       "  '0.130*\"datetimetoken\" + 0.128*\"couple\" + -0.126*\"app\" + 0.123*\"interview\" + -0.122*\"study\" + 0.102*\"career\" + 0.102*\"learning\" + -0.101*\"dev\" + -0.100*\"thought\" + -0.098*\"website\" + 0.091*\"employer\" + -0.089*\"basic\" + -0.082*\"thank\" + 0.080*\"passion\" + -0.080*\"skill\" + 0.079*\"california\" + 0.079*\"frontend\" + -0.078*\"coursera\" + -0.078*\"knowledge\" + 0.076*\"path\"'),\n",
       " (16,\n",
       "  '-0.136*\"test\" + -0.134*\"datetimetoken\" + 0.110*\"role\" + -0.104*\"css\" + -0.103*\"position\" + -0.103*\"course\" + 0.096*\"science\" + 0.093*\"goal\" + 0.090*\"current\" + -0.088*\"class\" + -0.088*\"html\" + 0.083*\"environment\" + 0.082*\"fact\" + 0.082*\"project\" + 0.080*\"people\" + -0.080*\"interview\" + 0.079*\"advice\" + -0.079*\"design\" + -0.075*\"last\" + 0.074*\"networking\"'),\n",
       " (17,\n",
       "  '-0.139*\"way\" + -0.111*\"frontend\" + 0.099*\"passionate\" + 0.095*\"goal\" + 0.092*\"entire\" + -0.091*\"role\" + 0.090*\"people\" + 0.089*\"session\" + 0.088*\"night\" + -0.088*\"cert\" + 0.085*\"luck\" + -0.084*\"science\" + 0.082*\"process\" + -0.078*\"client\" + -0.076*\"regular\" + -0.074*\"profile\" + 0.074*\"lot\" + 0.072*\"approach\" + -0.071*\"engineering\" + 0.069*\"class\"'),\n",
       " (18,\n",
       "  '-0.140*\"app\" + -0.130*\"people\" + -0.105*\"profile\" + 0.095*\"course\" + -0.092*\"luck\" + -0.091*\"mobile\" + -0.089*\"client\" + -0.089*\"skill\" + -0.085*\"weekly\" + -0.084*\"website\" + -0.083*\"summer\" + -0.083*\"professional\" + -0.080*\"regular\" + 0.079*\"story\" + -0.078*\"upwork\" + -0.078*\"guy\" + -0.075*\"partner\" + -0.075*\"upcoming\" + -0.075*\"peter\" + -0.075*\"objective-c\"'),\n",
       " (19,\n",
       "  '-0.154*\"programming\" + -0.147*\"college\" + 0.144*\"course\" + -0.138*\"class\" + -0.138*\"role\" + 0.121*\"frontend\" + -0.103*\"employer\" + -0.097*\"process\" + -0.095*\"current\" + -0.089*\"career\" + -0.087*\"question\" + -0.086*\"support\" + -0.086*\"path\" + 0.083*\"project\" + 0.080*\"position\" + -0.079*\"intro\" + -0.076*\"aspect\" + -0.075*\"local\" + -0.074*\"people\" + -0.071*\"situation\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"learning\" + 0.003*\"bachelor\" + 0.003*\"cert\" + 0.003*\"curriculum\" + 0.003*\"topic\" + 0.003*\"website\" + 0.002*\"part\" + 0.002*\"community\" + 0.002*\"dev\" + 0.002*\"world\" + 0.002*\"forum\" + 0.002*\"hi\" + 0.002*\"portfolio\" + 0.002*\"kid\" + 0.002*\"way\" + 0.002*\"life\" + 0.002*\"situation\" + 0.002*\"school\" + 0.002*\"networking\" + 0.002*\"lot\"'),\n",
       " (1,\n",
       "  '0.004*\"month\" + 0.003*\"dev\" + 0.003*\"frontend\" + 0.003*\"css\" + 0.003*\"week\" + 0.003*\"hi\" + 0.003*\"course\" + 0.003*\"portfolio\" + 0.003*\"html\" + 0.003*\"time\" + 0.003*\"lot\" + 0.003*\"thank\" + 0.003*\"js\" + 0.002*\"position\" + 0.002*\"experience\" + 0.002*\"datetimetoken\" + 0.002*\"background\" + 0.002*\"video\" + 0.002*\"job\" + 0.002*\"test\"'),\n",
       " (2,\n",
       "  '0.003*\"office\" + 0.003*\"3-\" + 0.003*\"urban\" + 0.003*\"firebase\" + 0.003*\"-old\" + 0.003*\"5-6am\" + 0.003*\"administrator\" + 0.003*\"clerk\" + 0.003*\"circle\" + 0.003*\"scary\" + 0.003*\"responsible\" + 0.003*\"lynday.com\" + 0.003*\"io\" + 0.003*\"visualization\" + 0.003*\"gardening\" + 0.003*\"front\" + 0.003*\"networking\" + 0.003*\"intern\" + 0.003*\"web-based\" + 0.003*\"demo\"'),\n",
       " (3,\n",
       "  '0.003*\"community\" + 0.003*\"strong\" + 0.002*\"career\" + 0.002*\"many\" + 0.002*\"encouragement\" + 0.002*\"help\" + 0.002*\"good\" + 0.002*\"big\" + 0.002*\"month\" + 0.002*\"editor\" + 0.002*\"hi\" + 0.002*\"someone\" + 0.002*\"stuff\" + 0.002*\"15-30\" + 0.002*\"array\" + 0.002*\"son\" + 0.002*\"belief\" + 0.002*\"euler\" + 0.002*\"butt\" + 0.002*\"guiding arrow\"'),\n",
       " (4,\n",
       "  '0.004*\"league\" + 0.004*\"freedom\" + 0.003*\"ground\" + 0.003*\"spent\" + 0.003*\"mern\" + 0.003*\"studying\" + 0.003*\"parent\" + 0.003*\"single\" + 0.003*\"account\" + 0.003*\"everyday\" + 0.003*\"mern stack\" + 0.003*\"programming\" + 0.003*\"stuff\" + 0.003*\"lot freedom\" + 0.002*\"new app ground\" + 0.002*\"source\" + 0.002*\"link\" + 0.002*\"hi\" + 0.002*\"long\" + 0.002*\"bar\"'),\n",
       " (5,\n",
       "  '0.004*\"programming\" + 0.004*\"website\" + 0.003*\"chellanges\" + 0.003*\"edx\" + 0.003*\"cs50 edx\" + 0.003*\"aproach\" + 0.003*\"awsome\" + 0.003*\"//manuelbasanta.github.io/\" + 0.003*\"increadible\" + 0.003*\"cs50\" + 0.003*\"issue\" + 0.003*\"frontend\" + 0.003*\"entry\" + 0.003*\"week\" + 0.003*\"thank\" + 0.003*\"computer\" + 0.002*\"hope\" + 0.002*\"codehour\" + 0.002*\"gamer\" + 0.002*\"hadi partovi\"'),\n",
       " (6,\n",
       "  '0.005*\"hi\" + 0.004*\"book\" + 0.003*\"dev\" + 0.003*\"life\" + 0.003*\"education\" + 0.003*\"article\" + 0.003*\"part\" + 0.003*\"fcc\" + 0.003*\"advice\" + 0.003*\"community\" + 0.003*\"satisfied\" + 0.003*\"ideas.ataccama.com\" + 0.003*\"goal\" + 0.003*\"entry\" + 0.003*\"lucas\" + 0.003*\"on-the-job\" + 0.003*\"belgium\" + 0.003*\"hands-on\" + 0.003*\"sme\" + 0.003*\"tutorials…\"'),\n",
       " (7,\n",
       "  '0.004*\"room\" + 0.003*\"promising\" + 0.003*\"insurmountable\" + 0.003*\"politics\" + 0.003*\"optimistic\" + 0.003*\"hybrid\" + 0.003*\"site\" + 0.003*\"apps\" + 0.003*\"process\" + 0.003*\"training\" + 0.003*\"someone\" + 0.003*\"stuck\" + 0.003*\"graduation\" + 0.003*\"career\" + 0.003*\"thing\" + 0.003*\"web-apps hybrid mobile apps\" + 0.003*\"politics university\" + 0.003*\"insurmountable task\" + 0.003*\"month\" + 0.003*\"reading\"'),\n",
       " (8,\n",
       "  '0.004*\"thank\" + 0.004*\"java\" + 0.004*\"curriculum\" + 0.003*\"cert\" + 0.003*\"position\" + 0.003*\"toe\" + 0.003*\"tac\" + 0.003*\"xsl\" + 0.003*\"ever-increasing\" + 0.003*\"qualified\" + 0.003*\"pocket\" + 0.003*\"relax\" + 0.003*\"ceo\" + 0.003*\"paycheck\" + 0.003*\"mail\" + 0.003*\"easy-to-use\" + 0.003*\"xml\" + 0.003*\"total\" + 0.003*\"paramount\" + 0.003*\"tic\"'),\n",
       " (9,\n",
       "  '0.003*\"wonderful\" + 0.002*\"windows/linux\" + 0.002*\"distributed system\" + 0.002*\"analyst/it\" + 0.002*\"dead horse\" + 0.002*\"content\" + 0.002*\"apis\" + 0.002*\"distributed\" + 0.002*\"juice\" + 0.002*\"transition\" + 0.002*\"nyc\" + 0.002*\"networking\" + 0.002*\"specialist\" + 0.002*\"enterprise\" + 0.002*\"mixed windows/linux environment\" + 0.002*\"mixed\" + 0.002*\"fortune\" + 0.002*\"threw\" + 0.002*\"automation\" + 0.002*\"aws/azure\"'),\n",
       " (10,\n",
       "  '0.001*\"solid\" + 0.001*\"“cvs”\" + 0.001*\"9-10\" + 0.001*\"webdev\" + 0.001*\"competition\" + 0.001*\"eye\" + 0.001*\"bootcamp\" + 0.001*\"impression\" + 0.001*\"text\" + 0.001*\"bachelor\" + 0.001*\"domain\" + 0.001*\"daily\" + 0.001*\"skill\" + 0.001*\"direct\" + 0.001*\"screens/interviews\" + 0.001*\"amongst\" + 0.001*\"experienced\" + 0.001*\"meeting…\" + 0.001*\"experience\" + 0.001*\"grateful\"'),\n",
       " (11,\n",
       "  '0.003*\"journey\" + 0.003*\"community\" + 0.003*\"programming\" + 0.003*\"hi\" + 0.003*\"knowledge\" + 0.003*\"thank\" + 0.003*\"frontend\" + 0.003*\"larson\" + 0.002*\"position\" + 0.002*\"brazil\" + 0.002*\"platform\" + 0.002*\"amazing\" + 0.002*\"vuejs\" + 0.002*\"btter\" + 0.002*\"previous\" + 0.002*\"motivation\" + 0.002*\"title\" + 0.002*\"competent\" + 0.002*\"requested\" + 0.002*\"dev\"'),\n",
       " (12,\n",
       "  '0.001*\"article\" + 0.001*\"medium\" + 0.001*\"bit\" + 0.001*\"description\" + 0.001*\"numerous\" + 0.001*\"fundamental\" + 0.001*\"command\" + 0.001*\"glance\" + 0.001*\"bootcamps\" + 0.001*\"patient\" + 0.001*\"template\" + 0.001*\"recent\" + 0.001*\"rate\" + 0.001*\"thank”\" + 0.001*\"agency\" + 0.001*\"tailor\" + 0.001*\"history\" + 0.001*\"reception\" + 0.001*\"most/all\" + 0.001*\"figure\"'),\n",
       " (13,\n",
       "  '0.004*\"thank\" + 0.004*\"course\" + 0.004*\"example\" + 0.003*\"dev\" + 0.003*\"month\" + 0.003*\"fcc\" + 0.003*\"story\" + 0.003*\"knowledge\" + 0.003*\"holiday\" + 0.002*\"reply\" + 0.002*\"next\" + 0.002*\"cv/cv\" + 0.002*\"zeppelin\" + 0.002*\"well.and\" + 0.002*\"strength\" + 0.002*\"\\'yes\" + 0.002*\"progressing\" + 0.002*\"roadmap\" + 0.002*\"sake\" + 0.002*\"flaw\"'),\n",
       " (14,\n",
       "  '0.004*\"bit\" + 0.003*\"response\" + 0.003*\"community\" + 0.003*\"platform\" + 0.003*\"many\" + 0.003*\"everyone\" + 0.003*\"free\" + 0.003*\"knowledge\" + 0.003*\"fulltime\" + 0.002*\"project\" + 0.002*\"whoo\" + 0.002*\"gaining\" + 0.002*\"development\" + 0.002*\"coding\" + 0.002*\"remote\" + 0.002*\"share\" + 0.002*\"code\" + 0.002*\"computer\" + 0.002*\"hi\" + 0.002*\"little\"'),\n",
       " (15,\n",
       "  '0.005*\"codacademy\" + 0.005*\"bright\" + 0.005*\"armenia\" + 0.004*\"political\" + 0.004*\"graduation\" + 0.004*\"camp\" + 0.004*\"profession\" + 0.004*\"political science\" + 0.004*\"local code camp armenia\" + 0.003*\"science\" + 0.003*\"hi\" + 0.003*\"someone\" + 0.003*\"local\" + 0.003*\"resource\" + 0.003*\"good kind resource\" + 0.003*\"success\" + 0.003*\"learning\" + 0.003*\"kind\" + 0.003*\"story\" + 0.002*\"thank\"'),\n",
       " (16,\n",
       "  '0.001*\"computer\" + 0.001*\"opposite\" + 0.001*\"noobie\" + 0.001*\"achievement\" + 0.001*\"unnecessary\" + 0.001*\"niche\" + 0.001*\"hi\" + 0.001*\"min\" + 0.001*\"activity\" + 0.001*\"bill\" + 0.001*\"money\" + 0.001*\"traffic jam\" + 0.001*\"codecademy\" + 0.001*\"full\" + 0.001*\"wtf\" + 0.001*\"data-vis\" + 0.001*\"omg\" + 0.001*\"horrible hourly wage\" + 0.001*\"diploma\" + 0.001*\"matter preference\"'),\n",
       " (17,\n",
       "  '0.003*\"cert\" + 0.003*\"post\" + 0.002*\"coding\" + 0.002*\"js\" + 0.002*\"vue\" + 0.002*\"web\" + 0.002*\"fulltime\" + 0.002*\"way\" + 0.002*\"huge\" + 0.002*\"thank\" + 0.002*\"css\" + 0.002*\"frontend\" + 0.002*\"guy\" + 0.002*\"month\" + 0.002*\"fcc\" + 0.002*\"dev\" + 0.002*\"prospect\" + 0.002*\"line\" + 0.002*\"datetimetoken\" + 0.002*\"freelance\"'),\n",
       " (18,\n",
       "  '0.004*\"type\" + 0.003*\"js\" + 0.003*\"idea type question\" + 0.003*\"question\" + 0.003*\"css\" + 0.003*\"today\" + 0.003*\"idea\" + 0.002*\"css js\" + 0.002*\"meetups\" + 0.002*\"self-taught fcc\" + 0.002*\"liberal\" + 0.002*\"self-taught\" + 0.002*\"dev\" + 0.002*\"prospective\" + 0.002*\"lifelong\" + 0.002*\"“who\" + 0.002*\"indeed.com\" + 0.002*\"california\" + 0.002*\"culture\" + 0.002*\"\\'you\"'),\n",
       " (19,\n",
       "  '0.001*\"con\" + 0.001*\"editor\" + 0.001*\"description\" + 0.001*\"detailed\" + 0.001*\"mongodb\" + 0.001*\"previous\" + 0.001*\"tldr\" + 0.001*\"full\" + 0.001*\"command\" + 0.001*\"service\" + 0.001*\"result\" + 0.001*\"unnecessary\" + 0.001*\"whole\" + 0.001*\"word\" + 0.001*\"field\\\\attribute object\" + 0.001*\"css\\\\sass\" + 0.001*\"suit\" + 0.001*\"a.\" + 0.001*\"negotations\" + 0.001*\"controller\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09731554182749776"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordimportance[\"new language framework\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "learning 234 0.0033871\n",
      "bachelor 809 0.0027781\n",
      "cert 92 0.00274221\n",
      "curriculum 24 0.00263891\n",
      "topic 870 0.00253989\n",
      "website 176 0.00251779\n",
      "part 63 0.00248143\n",
      "community 18 0.00247603\n",
      "dev 28 0.00239804\n",
      "world 997 0.0023418\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "month 57 0.00370682\n",
      "dev 28 0.00341686\n",
      "frontend 105 0.00336777\n",
      "css 1136 0.00335696\n",
      "week 86 0.00335513\n",
      "hi 45 0.00318371\n",
      "course 21 0.00300169\n",
      "portfolio 658 0.00290928\n",
      "html 1674 0.00275316\n",
      "time 129 0.00273274\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "office 258 0.00340783\n",
      "3- 1866 0.00318432\n",
      "urban 1917 0.00318432\n",
      "circle 1872 0.00318432\n",
      "visualization 1918 0.00318432\n",
      "5-6am 1868 0.00318432\n",
      "administrator 1870 0.00318432\n",
      "lynday.com 1898 0.00318432\n",
      "clerk 1873 0.00318432\n",
      "io 1893 0.00318432\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "community 18 0.00279095\n",
      "strong 1089 0.00275801\n",
      "career 136 0.00246965\n",
      "many 248 0.00238684\n",
      "encouragement 1030 0.00235367\n",
      "help 217 0.00227805\n",
      "good 38 0.00225959\n",
      "big 1011 0.00220758\n",
      "month 57 0.00214844\n",
      "editor 432 0.00214575\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "league 3428 0.00380957\n",
      "freedom 2891 0.00370656\n",
      "ground 1038 0.00346023\n",
      "spent 125 0.00346023\n",
      "mern 1636 0.00346023\n",
      "studying 1751 0.00337368\n",
      "parent 260 0.00325588\n",
      "single 2039 0.00325588\n",
      "account 2043 0.00320934\n",
      "everyday 2131 0.0031109\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "programming 270 0.00385196\n",
      "website 176 0.00361431\n",
      "chellanges 3492 0.00296206\n",
      "edx 3497 0.00296206\n",
      "increadible 3500 0.00296206\n",
      "//manuelbasanta.github.io/ 3488 0.00296206\n",
      "awsome 3490 0.00296206\n",
      "cs50 3493 0.00296206\n",
      "cs50 edx 3494 0.00296206\n",
      "aproach 3489 0.00296206\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "hi 45 0.00472743\n",
      "book 10 0.00353858\n",
      "dev 28 0.0033609\n",
      "life 152 0.0031648\n",
      "education 933 0.00313279\n",
      "article 331 0.0030498\n",
      "part 63 0.00303785\n",
      "fcc 33 0.00302045\n",
      "advice 3 0.00278877\n",
      "community 18 0.0027447\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "room 1548 0.00402109\n",
      "promising 162 0.00335481\n",
      "insurmountable 146 0.00335481\n",
      "politics 159 0.00335481\n",
      "optimistic 158 0.00335481\n",
      "hybrid 143 0.00335481\n",
      "site 164 0.00335204\n",
      "apps 135 0.00311228\n",
      "process 161 0.00310681\n",
      "training 769 0.00310518\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "thank 79 0.00396618\n",
      "java 1413 0.00386912\n",
      "curriculum 24 0.00367006\n",
      "cert 92 0.00333247\n",
      "position 659 0.00327616\n",
      "tac 2082 0.00311668\n",
      "xsl 298 0.00311668\n",
      "toe 2085 0.00311668\n",
      "pocket 1425 0.00311668\n",
      "qualified 1428 0.00311668\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "wonderful 874 0.00260431\n",
      "windows/linux 873 0.00246\n",
      "distributed system 822 0.00235325\n",
      "analyst/it 800 0.00234173\n",
      "dead horse 817 0.00225644\n",
      "content 813 0.00219174\n",
      "apis 801 0.00218539\n",
      "distributed 821 0.00213612\n",
      "juice 839 0.00212735\n",
      "transition 871 0.00210223\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "k-means to approximate the number of topics before trying a more elaborate form\n",
    "Check and improve previous work:\n",
    "* https://github.com/evaristoc/fccgitterDataScience/blob/master/Identifying%20Relevant%20Topics%20in%20a%20Chatroom.ipynb\n",
    "* https://stackoverflow.com/questions/24816912/number-of-latent-semantic-indexing-topics\n",
    "* https://stackoverflow.com/questions/9582291/how-do-we-decide-the-number-of-dimensions-for-latent-semantic-analysis/9759218#9759218\n",
    "\n",
    "Also check:\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/ (good example but conceptually a bit wrong)\n",
    "* https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "* http://hojunhao.github.io/sgparliament/LDA.html\n",
    "* https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process\n",
    "* https://nlpforhackers.io/recipe-text-clustering/\n",
    "* https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling\n",
    "* http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=3&lambda=1&term=\n",
    "* https://stackoverflow.com/questions/50106516/k-means-for-topic-modelling-elbow-method\n",
    "* https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html\n",
    "* http://wdsinet.org/Annual_Meetings/2016_Proceedings/papers/Paper45.pdf\n",
    "* http://ramet.elte.hu/~podani/Methods.htm\n",
    "* https://hk.saowen.com/a/edc29232eae094158f66e8ff3f08d6f35b8a2a45d628fce8917d2dce6f94282e\n",
    "* https://rare-technologies.com/validating-gensims-topic-coherence-pipeline/\n",
    "* http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html\n",
    "* https://www.searchenginejournal.com/latent-semantic-indexing-wont-help-seo/240705/\n",
    "* https://www.quora.com/What-is-topic-coherence + http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "* https://stackoverflow.com/questions/50340657/pyldavis-with-mallet-lda-implementation-ldamallet-object-has-no-attribute-inf\n",
    "* https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 8\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "K = list(range(1, n_components+1))\n",
    "KM = [KMeans(n_clusters=k).fit(X) for k in K]\n",
    "centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [numpy.argmin(D,axis=1) for D in D_k]\n",
    "dist = [numpy.min(D,axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "\n",
    "kIdx = 8-1\n",
    "\n",
    "# elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(K, avgWithinSS, 'b*-')\n",
    "ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for KMeans clustering')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "true_k = 8\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "newX = numpy.array(pandas.concat([pandas.DataFrame(X),datadf_foran['timestamp_norm'].reset_index()['timestamp_norm']],axis=1))\n",
    "km.fit(newX)\n",
    "\n",
    "print()\n",
    "\n",
    "labels = [x for x in range(datadf_foran.shape[0])]\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(newX, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "fig_clusters = plt.figure()\n",
    "fig_clusters.suptitle('Clusters over first 2 Components')\n",
    "ax = fig_clusters.add_subplot(111)\n",
    "ax.set_xlabel('Component I')\n",
    "ax.set_ylabel('Component II')\n",
    "plt.scatter(newX[:,0],newX[:,1], c=km.fit_predict(newX))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727"
    }
   },
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "038497e7-3cf4-4c64-867c-1bc637cad5e5"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## raw_lda_frankjupyter2: modified to fit allrecordsPreparation2\n",
    "############################\n",
    "def raw_lda_frankjupyter2(lemmposrecs, wordimportance, metmodel=2, STOPWORDS=nltk.corpus.stopwords.words('english')):\n",
    "    '''\n",
    "    description: modified model based on https://www.frankcleary.com/svd/ for a more raw construction of a lda\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    def metriccalc(st, normalizer, wordimportance):\n",
    "        '''\n",
    "        description:\n",
    "        text normalization based on ALL characters in the sentence; why? Example: if two writers wrote 20 words, 2 of them very important, but one of them wrote half of characters stopwords, those 2 words wouldnt be penalized accordingly for this writer: the other wrote more important content\n",
    "        '''       \n",
    "        metfuncs = [\n",
    "            lambda w: math.pow(0.1+float(wordimportance[w]),textbow[w]/normalizer) if w in list(wordimportance.keys()) else 0.0, #a sort of idf-normalization based on number of words in the text: the more the words in a text, the more important\n",
    "            lambda w: float(wordimportance[w])*textbow[w] if w in list(wordimportance.keys()) else 0.0, #good but ignore those words with worimportance too low or 0 but that are frequent in text\n",
    "            lambda w: 1.0+2.0**float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- probably the best one; because it is not normilized this indicator would simply say that if it has the word at least once is already on topic\n",
    "            lambda w: float(wordimportance[w]) + textbow[w]/normalizer if w in list(wordimportance.keys()) else 0.0,\n",
    "            lambda w: (1.0+textbow[w]/normalizer)*float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- apparently the second best with the adhoc word ranking\n",
    "            lambda w: normalizer/textbow[w]*wordimportance[w]/sum(list(wordimportance.values())), #tfidf-ish\n",
    "            lambda w: float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0\n",
    "        ]\n",
    "        likedict = collections.defaultdict(float)\n",
    "        textbow = collections.Counter(st)\n",
    "        for w in st:\n",
    "            likedict[w] = metfuncs[metmodel](w)\n",
    "        return likedict\n",
    "\n",
    "    #redo_corpus_by_sts = []\n",
    "    words_df = pandas.DataFrame()\n",
    "    textreference = {}\n",
    "    \n",
    "    \n",
    "    for textindex, lemmpos_r in enumerate(lemmposrecs):\n",
    "        lemmpos_t = lemmpos_r[3]\n",
    "        #print('lemmpos_t', len(lemmpos_t))\n",
    "        lemm_sts = ''\n",
    "        for lemmpos_TUPLE in lemmpos_t:\n",
    "            w = lemmpos_TUPLE[2]\n",
    "            lemm_sts = lemm_sts + w + ' '\n",
    "        sts = lemm_sts.split('.')\n",
    "        for stindex, lemmpos_st in enumerate(sts):\n",
    "            treated_st, lensts = cleaningtext(lemmpos_st.split(), STOPWORDS=STOPWORDS)        \n",
    "            #print('treated_st', lensts)\n",
    "            if len(treated_st) > 3:\n",
    "                likedict = metriccalc(treated_st, lensts, wordimportance)\n",
    "                st_df = pandas.DataFrame.from_dict(likedict, orient='index')\n",
    "                textindexing = str(textindex)+'_'+str(stindex)\n",
    "                st_df.columns = [textindexing]\n",
    "                textreference[textindexing] = {}\n",
    "                textreference[textindexing]['treated_st'] = treated_st\n",
    "                #st_df.columns = [str(count)]\n",
    "                words_df = words_df.join(st_df, how='outer', )\n",
    "    \n",
    "    words_df = words_df.fillna(0)\n",
    "    print(\"Number of unique words: %s\" % len(words_df))\n",
    "    print(words_df.head(10))\n",
    "    #print(words_df.sort(columns=words_df.columns[0], ascending=False).head(10))\n",
    "    \n",
    "    return words_df, textreference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e3fa11e-feb2-487d-9d53-43dc3deb650c": {
     "id": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "prev": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "regions": {
      "07b1e699-5143-43f7-a938-d61bc27e8e62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e1c251e9-1608-43d5-ba5a-e27bb09739f5",
        "part": "whole"
       },
       "id": "07b1e699-5143-43f7-a938-d61bc27e8e62"
      }
     }
    },
    "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5": {
     "id": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "prev": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "regions": {
      "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727",
        "part": "whole"
       },
       "id": "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0"
      }
     }
    },
    "1cf808ac-39f2-4009-8112-3ef3a17b4af6": {
     "id": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "prev": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "regions": {
      "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5992afc-b214-46ae-8f12-1026835184cd",
        "part": "whole"
       },
       "id": "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8"
      }
     }
    },
    "1f7a3d0c-f066-482e-8b40-cf107299d110": {
     "id": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "prev": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "regions": {
      "1badc1e9-6ff7-4358-9493-665026267eab": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d236a59f-ff21-406d-b1df-9436aedbdb11",
        "part": "whole"
       },
       "id": "1badc1e9-6ff7-4358-9493-665026267eab"
      }
     }
    },
    "3659283d-893a-48fa-9115-7dd64d2aed00": {
     "id": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "prev": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "regions": {
      "835576ab-c1f9-4bfa-af97-ed77dbde5925": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e294b910-1a25-4e48-abb5-37239f441f2e",
        "part": "whole"
       },
       "id": "835576ab-c1f9-4bfa-af97-ed77dbde5925"
      }
     }
    },
    "395c02de-c982-4ca8-a48e-10a79828812d": {
     "id": "395c02de-c982-4ca8-a48e-10a79828812d",
     "prev": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "regions": {
      "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3cda7d21-4a2d-4d01-ac74-10e33c83629e",
        "part": "whole"
       },
       "id": "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7"
      }
     }
    },
    "3f4f1ce3-4872-48cc-8c96-31c19dba5a74": {
     "id": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "prev": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "regions": {
      "7fb22b2c-4bff-4c35-86aa-40952a6b9b24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5bb4cd0-4cb1-493f-99e0-e75214f084a7",
        "part": "whole"
       },
       "id": "7fb22b2c-4bff-4c35-86aa-40952a6b9b24"
      }
     }
    },
    "45ee2b8f-c113-44cf-a993-6a20d43e99f5": {
     "id": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "prev": "532186bd-d619-49da-be76-e28eb7db691a",
     "regions": {
      "e4a63835-d541-4073-bee5-0017da9a065b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8e459ff-ae34-48aa-8146-50365df9ea53",
        "part": "whole"
       },
       "id": "e4a63835-d541-4073-bee5-0017da9a065b"
      }
     }
    },
    "532186bd-d619-49da-be76-e28eb7db691a": {
     "id": "532186bd-d619-49da-be76-e28eb7db691a",
     "prev": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "regions": {
      "2b7cff32-e764-4b30-8301-0cb4bb2268dc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384",
        "part": "whole"
       },
       "id": "2b7cff32-e764-4b30-8301-0cb4bb2268dc"
      }
     }
    },
    "99fa0d29-85d3-4e57-a540-74b76519f4ac": {
     "id": "99fa0d29-85d3-4e57-a540-74b76519f4ac",
     "prev": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "regions": {
      "afebcb71-18ec-40b4-9e65-8f01e4c70d57": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "038497e7-3cf4-4c64-867c-1bc637cad5e5",
        "part": "whole"
       },
       "id": "afebcb71-18ec-40b4-9e65-8f01e4c70d57"
      }
     }
    },
    "a59429ee-539c-4c02-bfc8-8582111bf455": {
     "id": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "prev": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "regions": {
      "34164544-7832-445e-a820-4ef4dbc522ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a",
        "part": "whole"
       },
       "id": "34164544-7832-445e-a820-4ef4dbc522ac"
      }
     }
    },
    "aa51d16f-a5b4-460f-91fa-569faac9a6fe": {
     "id": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "prev": null,
     "regions": {
      "e46d430e-2c0b-416c-bac3-86a2d27711c1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "560a94b7-6c8e-465f-96fa-fcf2c3859245",
        "part": "whole"
       },
       "id": "e46d430e-2c0b-416c-bac3-86a2d27711c1"
      }
     }
    },
    "ac6a71ca-a1d6-4449-9ee7-87ff28115363": {
     "id": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "prev": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "regions": {
      "ce9295e6-aa18-4ceb-a277-fb19127d89ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67bf390c-9e34-4764-a6de-ab9d3082090b",
        "part": "whole"
       },
       "id": "ce9295e6-aa18-4ceb-a277-fb19127d89ed"
      }
     }
    },
    "baa3a1da-cdb2-4638-8bec-2986349ac603": {
     "id": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "prev": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "regions": {
      "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59c7646c-3944-4e2c-81a1-1728a02396ae",
        "part": "whole"
       },
       "id": "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce"
      }
     }
    },
    "ca7a6c34-27e7-4921-9da2-43a92faf8e8e": {
     "id": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "prev": "395c02de-c982-4ca8-a48e-10a79828812d",
     "regions": {
      "6414ffa6-ea4d-40fe-9d90-5b0816ff0789": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c1fd569-b2c4-423d-a598-1466fc992a49",
        "part": "whole"
       },
       "id": "6414ffa6-ea4d-40fe-9d90-5b0816ff0789"
      }
     }
    },
    "dae9886f-f215-4b89-aefb-8831c7f2ddfc": {
     "id": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "prev": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "regions": {
      "5eeb753c-493e-4bba-b5ab-025339c264f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d80b2206-d953-46b3-b807-2bf5a8557d28",
        "part": "whole"
       },
       "id": "5eeb753c-493e-4bba-b5ab-025339c264f2"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
