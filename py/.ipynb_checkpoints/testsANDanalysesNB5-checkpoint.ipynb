{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/derekgreene/topic-model-tutorial \n",
    "* https://github.com/derekgreene/topic-model-tutorial/blob/master/topic-modelling-with-scikitlearn.pdf\n",
    "* https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n",
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d80b2206-d953-46b3-b807-2bf5a8557d28"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    def createcands(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        #print(list(group)[::-1])\n",
    "        reversing = [None]\n",
    "        #for el in group:\n",
    "        #    print(el)\n",
    "        for el in group:\n",
    "            if reversing[0] == None:\n",
    "                reversing[0] = el[0]\n",
    "            else:\n",
    "                reversing.insert(0, el[0])\n",
    "        #print(reversing)\n",
    "        for word in reversing:\n",
    "            if current == '':\n",
    "                current = word\n",
    "            else:\n",
    "                current = word + ' ' + current\n",
    "            total.append(current)\n",
    "        #print(list(set(reversing+total)))\n",
    "        return list(set(reversing+total))\n",
    "    \n",
    "    def createcands2(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        for word, pos, chunk in list(group):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            #print(word)\n",
    "            if current == '':\n",
    "                current = word\n",
    "                total.append(word)\n",
    "            else:\n",
    "                current = current+' '+word\n",
    "                total.append(word)\n",
    "        total.append(current)\n",
    "\n",
    "        return list(set(total))\n",
    "            \n",
    "    #candidates = [' '.join(word for word, pos, chunk in group)\n",
    "    candidates = [createcands2(group)\n",
    "                    for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "    #print(candidates)\n",
    "\n",
    "    #return [cand for cand in candidates]\n",
    "    #print(list(itertools.chain.from_iterable(candidates)))\n",
    "    return list(itertools.chain.from_iterable(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        pattern_A01 = re.compile(r'^hey( |$)')\n",
    "        pattern_B01 = re.compile(r'(january|february| march(,|\\.)? |april|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday|morning|afternoon|evening|mont(s|ly|\\.|,)|year(s|ly|\\.|,)?| day(s|\\.|,)?)')\n",
    "        pattern_C01 = re.compile(r'(chance(s)?|opportunit(y|ies))')\n",
    "        pattern_D01 = re.compile(r'(http?s?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?',re.I)\n",
    "        \n",
    "        ## Text extraction, REGEXs and Replacements\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text() \\\n",
    "                                                        .lower() \\\n",
    "                                                        .replace('’',\"'\") \\\n",
    "                                                        .replace('‘', \"'\") \\\n",
    "                                                        .replace('´', \"'\") \\\n",
    "                                                        .replace('free code camp', 'fcc') \\\n",
    "                                                        .replace('freecodecamp', 'fcc') \\\n",
    "                                                        .replace('javascript', 'js') \\\n",
    "                                                        .replace('part time', 'parttime') \\\n",
    "                                                        .replace('part-time', 'parttime') \\\n",
    "                                                        .replace('full time', 'fulltime') \\\n",
    "                                                        .replace('full-time', 'fulltime') \\\n",
    "                                                        .replace('web application', 'web-app') \\\n",
    "                                                        .replace('web app', 'web-app') \\\n",
    "                                                        .replace('web development', 'dev') \\\n",
    "                                                        .replace('web-development', 'dev') \\\n",
    "                                                        .replace('developer', 'dev') \\\n",
    "                                                        .replace('web dev', 'dev') \\\n",
    "                                                        .replace('dev position', 'dev job') \\\n",
    "                                                        .replace('dev role', 'dev job') \\\n",
    "                                                        .replace('front end', 'frontend') \\\n",
    "                                                        .replace('front-end','frontend') \\\n",
    "                                                        .replace('back end', 'backend') \\\n",
    "                                                        .replace('back-end','backend') \\\n",
    "                                                        .replace('full stack', 'fullstack') \\\n",
    "                                                        .replace('full-stack','fullstack') \\\n",
    "                                                        .replace('frontend job', 'dev job') \\\n",
    "                                                        .replace('frontend position', 'dev job') \\\n",
    "                                                        .replace('frontend role', 'dev job') \\\n",
    "                                                        .replace('frontend web job', 'dev job') \\\n",
    "                                                        .replace('frontend web position', 'dev job') \\\n",
    "                                                        .replace('frontend web role', 'dev job') \\\n",
    "                                                        .replace('frontend web dev job', 'dev job') \\\n",
    "                                                        .replace('frontend web dev position', 'dev job') \\\n",
    "                                                        .replace('frontend web dev role', 'dev job') \\\n",
    "                                                        .replace('resume', 'cv') \\\n",
    "                                                        .replace('angularjs', 'angular') \\\n",
    "                                                        .replace('angular', 'angularjs') \\\n",
    "                                                        .replace('certification', 'cert') \\\n",
    "                                                        .replace('certificate', 'cert') \\\n",
    "                                                        .replace('machine learning', 'machinelearning') \\\n",
    "                                                        .replace('data science', 'datascience') \\\n",
    "                                                        .replace('self learning', 'self-taught') \\\n",
    "                                                        .replace('self learned', 'self-taught') \\\n",
    "                                                        .replace('self-learning', 'self-taught') \\\n",
    "                                                        .replace('self-learned', 'self-taught') \\\n",
    "                                                        .replace('self taught', 'self-taught') \\\n",
    "                                                        .replace('thanks', 'thank') \\\n",
    "                                                        .replace('thankful', 'thank') \\\n",
    "                                                        .replace('gratitude', 'thank') \\\n",
    "                                                        .replace('many thank', 'thank') \\\n",
    "                                                        .replace('much thank', 'thank') \\\n",
    "                                                        .replace('special thank', 'thank') \\\n",
    "                                                        .replace('big thank', 'thank')\n",
    "                                                        #.replace('app', 'web-app') \\\n",
    "                                                        #.replace('web-dev job', 'dev-job') \\\n",
    "                                                        #.replace('web-dev position', 'web-dev-job') \\\n",
    "                                                        #.replace('web-dev role', 'web-dev-job') \\\n",
    "                                                        #.replace('backend job', 'dev job') \\\n",
    "                                                        #.replace('backend position', 'dev job') \\\n",
    "                                                        #.replace('backend role', 'dev job') \\\n",
    "                                                        #.replace('backend web job', 'dev job') \\\n",
    "                                                        #.replace('backend web position', 'dev job') \\\n",
    "                                                        #.replace('backend web role', 'dev job') \\\n",
    "                                                        #.replace('backend web dev job', 'dev job') \\\n",
    "                                                        #.replace('backend web dev position', 'dev job') \\\n",
    "                                                        #.replace('backend web dev role', 'dev job') \\ \n",
    "                                                        #.replace('dev job', 'dev job') \\\n",
    "                                                        \n",
    "        soup_forumpostTEXT = re.sub(pattern_A01, ' hi ', soup_forumpostTEXT)\n",
    "        soup_forumpostTEXT = soup_forumpostTEXT.replace('fellow camper', '') \\\n",
    "                                    .replace('camper', '') \\\n",
    "                                    .replace('fccers', '') \\\n",
    "                                    .replace('fccer', '') \\\n",
    "                                    .replace('everybody', 'everyone') \\\n",
    "                                    .replace('hello', 'hi') \\\n",
    "                                    .replace('hi everyone', 'hi') \\\n",
    "                                    .replace('hi, everyone', 'hi') \\\n",
    "                                    .replace('hi everybody', 'hi') \\\n",
    "                                    .replace('hi, everybody', 'hi')\n",
    "                                    \n",
    "        soup_forumpostTEXT = re.sub(pattern_B01, ' datetimetoken ', soup_forumpostTEXT)\n",
    "        \n",
    "        soup_forumpostTEXT = re.sub(pattern_C01, ' chance ', soup_forumpostTEXT)\n",
    "                \n",
    "        soup_forumpostTEXT = re.sub(pattern_D01, ' thiswasalink ', soup_forumpostTEXT)\n",
    "        \n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':','\\\\'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    #print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    #print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                #print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var5: wordimportance_var3 modified to explore mutual information hypothesis and coherence analysis\n",
    "############################\n",
    "def wordimportance_var5(lemmposrecs, lemmws_fd):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    selectedgrams = None\n",
    "\n",
    "    selectedgrams = dict([(grams, count) \n",
    "                     for grams, count in lemmws_fd.items() \n",
    "                     if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                    ])\n",
    "\n",
    "    print('unigrams',len(selectedgrams))\n",
    "    maxdiv = math.log(sorted(selectedgrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "    print('maxdiv', maxdiv)\n",
    "    opacity = collections.defaultdict(float)\n",
    "    #for grams, counts in lemmws_fd.items(): #grams assumes a phrase is possible\n",
    "    for grams, counts in selectedgrams.items():\n",
    "        if grams == '':\n",
    "            opacity[grams] = 0.0\n",
    "            continue\n",
    "        opval = []\n",
    "        #assert len(grams.split()) == 1, print(grams)\n",
    "        for gram in grams.split():\n",
    "            if gram == '':\n",
    "                continue\n",
    "            if gram in list(selectedgrams.keys()):\n",
    "                #if grams == \"new language framework\":\n",
    "                #    print(gram, math.log(selectedgrams[gram]))\n",
    "                opval.append(math.log(selectedgrams[gram]))\n",
    "            else:\n",
    "                opval.append(0.0)\n",
    "        #assert len(opval) != 0, print('grams',grams)\n",
    "        averopval = sum(opval)/len(opval)\n",
    "        if 1 - averopval/maxdiv != 0.0:\n",
    "            opacity[grams] = 1 - averopval/maxdiv\n",
    "        else:\n",
    "            opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "        #if grams == \"new language framework\":\n",
    "        #    print(grams, opacity[grams])\n",
    "    #print('opval',opval[:10])\n",
    "\n",
    "    #sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "    sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(selectedgrams.keys())])\n",
    "    #assert \"new language framework\" not in list(sizing_matrix.keys())\n",
    "\n",
    "    print('lenselectedgrams',len(selectedgrams))\n",
    "    indexing_matrix = dict([(i, k) for i, k in enumerate(list(selectedgrams.keys()))])\n",
    "    inverse_index_matrix = dict([(k,i) for i, k in indexing_matrix.items()])\n",
    "    \n",
    "    pseudo_mi = numpy.zeros([len(selectedgrams),len(selectedgrams),len(lemmposrecs)])\n",
    "    \n",
    "    ## Count lemmatized words/characters per text\n",
    "    for i,lemmpos_t in enumerate(lemmposrecs): \n",
    "        file = [] #file reconstruction\n",
    "        for k, lemmpos_sts in enumerate(lemmpos_t[3]):\n",
    "            ## Use lemmatized word\n",
    "            #print(lemmpos_sts)\n",
    "            for tk_TUPLE in lemmpos_sts: \n",
    "                file.append(tk_TUPLE[2])\n",
    "                \n",
    "        ##fill importance by word\n",
    "        for j, currenttoken in enumerate(file):\n",
    "            currentindex = inverse_index_matrix[currenttoken]\n",
    "            file_section = file[j:]\n",
    "            search = 10\n",
    "            if len(file_section) < 10:\n",
    "                search = len(file_section)\n",
    "            for s in range(search):\n",
    "                nexttoken = file[j+s]\n",
    "                nextindex = inverse_index_matrix[nexttoken]\n",
    "                pseudo_mi[currentindex,nextindex,i] = pseudo_mi[currentindex,nextindex,i]+opacity[nexttoken]/(s/2+1)\n",
    "                \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    return pseudo_mi, indexing_matrix, inverse_index_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of treated posts (len(count)) :: 53\n",
      "lemm errs: 19265\n"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 2839\n",
      "maxdiv 7.181591944611865\n",
      "lenselectedgrams 2839\n"
     ]
    }
   ],
   "source": [
    "pseudo_mi, indexing_matrix, inverse_index_matrix  = wordimportance_var5(lemmposrecs, lemmws_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2839, 2839, 53)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_mi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,  128,  214,  448, 1701, 2339, 2494, 2543, 2626, 2711]),\n",
       " array([17, 17, 17, 17, 17, 17, 17, 17, 17, 17]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.where(pseudo_mi[1,:,:] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cms'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cms\n",
      "the\n",
      "their\n",
      "interview\n",
      "receive\n",
      "in\n",
      "when\n",
      "info\n",
      "i\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#which are the word that are more related to 'dependency'?\n",
    "for wi in numpy.where(pseudo_mi[1,:,:] > 0)[0]:\n",
    "    print(indexing_matrix[wi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 17\n",
      "cms 1.0\n",
      "TEXT 17\n",
      "the 0.0114404305862\n",
      "TEXT 17\n",
      "their 0.149095662873\n",
      "TEXT 17\n",
      "interview 0.0551320060678\n",
      "TEXT 17\n",
      "receive 0.24301398334\n",
      "TEXT 17\n",
      "in 0.0389889884733\n",
      "TEXT 17\n",
      "when 0.209368946183\n",
      "TEXT 17\n",
      "info 0.25\n",
      "TEXT 17\n",
      "i 0.00051060364096\n",
      "TEXT 17\n",
      ". 0.497569912209\n"
     ]
    }
   ],
   "source": [
    "#how do they related to that word? Ie how 'strong' was that relation?\n",
    "checkdata = []\n",
    "for IND in range(len(numpy.where(pseudo_mi[1,:,:] > 0)[0])):\n",
    "    print('TEXT',numpy.where(pseudo_mi[1,:,:] > 0)[1][IND])\n",
    "    print(indexing_matrix[numpy.where(pseudo_mi[1,:,:] > 0)[0][IND]],pseudo_mi[1,numpy.where(pseudo_mi[1,:,:] > 0)[0][IND],numpy.where(pseudo_mi[1,:,:] > 0)[1][IND]])\n",
    "    checkdata.append((indexing_matrix[numpy.where(pseudo_mi[1,:,:] > 0)[0][IND]],pseudo_mi[1,numpy.where(pseudo_mi[1,:,:] > 0)[0][IND],numpy.where(pseudo_mi[1,:,:] > 0)[1][IND]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cms', 1.0),\n",
       " ('.', 0.4975699122087105),\n",
       " ('info', 0.25),\n",
       " ('receive', 0.24301398334040816),\n",
       " ('when', 0.20936894618277807),\n",
       " ('their', 0.14909566287292672),\n",
       " ('interview', 0.055132006067774138),\n",
       " ('in', 0.03898898847330548),\n",
       " ('the', 0.011440430586176609),\n",
       " ('i', 0.00051060364096002788)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(checkdata,key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super\n",
      "TEXT:  39 ; word:  week ( 0.18 )\n",
      "TEXT:  39 ; word:  to ( 0.03 )\n",
      "TEXT:  39 ; word:  this ( 0.11 )\n",
      "TEXT:  39 ; word:  the ( 0.01 )\n",
      "TEXT:  39 ; word:  that ( 0.09 )\n",
      "TEXT:  39 ; word:  super ( 4.50 )\n",
      "TEXT:  39 ; word:  story ( 0.18 )\n",
      "TEXT:  39 ; word:  so ( 0.05 )\n",
      "TEXT:  39 ; word:  slept ( 0.33 )\n",
      "TEXT:  39 ; word:  send ( 0.10 )\n",
      "\n",
      "\n",
      "cms\n",
      "TEXT:  17 ; word:  when ( 0.20 )\n",
      "TEXT:  17 ; word:  their ( 0.14 )\n",
      "TEXT:  17 ; word:  the ( 0.01 )\n",
      "TEXT:  17 ; word:  receive ( 0.24 )\n",
      "TEXT:  17 ; word:  interview ( 0.05 )\n",
      "TEXT:  17 ; word:  info ( 0.25 )\n",
      "TEXT:  17 ; word:  in ( 0.03 )\n",
      "TEXT:  17 ; word:  i ( 0.00 )\n",
      "TEXT:  17 ; word:  cms ( 1.0 )\n",
      "TEXT:  17 ; word:  . ( 0.49 )\n",
      "\n",
      "\n",
      "engineering\n",
      "TEXT:  22 ; word:  your ( 0.06 )\n",
      "TEXT:  23 ; word:  you ( 0.02 )\n",
      "TEXT:  22 ; word:  will ( 0.13 )\n",
      "TEXT:  22 ; word:  who ( 0.23 )\n",
      "TEXT:  35 ; word:  up ( 0.10 )\n",
      "TEXT:  23 ; word:  the ( 0.02 )\n",
      "TEXT:  35 ; word:  that ( 0.03 )\n",
      "TEXT:  35 ; word:  tell ( 0.23 )\n",
      "TEXT:  23 ; word:  so ( 0.06 )\n",
      "TEXT:  28 ; word:  sale ( 0.14 )\n",
      "\n",
      "\n",
      "reality\n",
      "TEXT:  8 ; word:  yourself ( 0.10 )\n",
      "TEXT:  8 ; word:  you ( 0.03 )\n",
      "TEXT:  8 ; word:  that ( 0.05 )\n",
      "TEXT:  8 ; word:  sell ( 0.16 )\n",
      "TEXT:  8 ; word:  reality ( 1.0 )\n",
      "TEXT:  8 ; word:  of ( 0.09 )\n",
      "TEXT:  8 ; word:  job ( 0.11 )\n",
      "TEXT:  8 ; word:  hunting ( 0.4 )\n",
      "TEXT:  8 ; word:  be ( 0.03 )\n",
      "\n",
      "\n",
      "discourage\n",
      "TEXT:  22 ; word:  you ( 0.06 )\n",
      "TEXT:  23 ; word:  you ( 0.02 )\n",
      "TEXT:  31 ; word:  you ( 0.06 )\n",
      "TEXT:  45 ; word:  you ( 0.05 )\n",
      "TEXT:  22 ; word:  without ( 0.10 )\n",
      "TEXT:  22 ; word:  to ( 0.01 )\n",
      "TEXT:  45 ; word:  think ( 0.21 )\n",
      "TEXT:  45 ; word:  there ( 0.08 )\n",
      "TEXT:  22 ; word:  the ( 0.01 )\n",
      "TEXT:  23 ; word:  that ( 0.04 )\n",
      "\n",
      "\n",
      "shift\n",
      "TEXT:  39 ; word:  web-apps ( 0.24 )\n",
      "TEXT:  39 ; word:  to ( 0.01 )\n",
      "TEXT:  39 ; word:  shift ( 1.0 )\n",
      "TEXT:  39 ; word:  lesson ( 0.12 )\n",
      "TEXT:  39 ; word:  js ( 0.16 )\n",
      "TEXT:  39 ; word:  focus ( 0.40 )\n",
      "TEXT:  39 ; word:  back ( 0.26 )\n",
      "TEXT:  39 ; word:  : ( 0.06 )\n",
      "TEXT:  39 ; word:  2 ( 0.11 )\n",
      "TEXT:  39 ; word:  ( ( 0.07 )\n",
      "\n",
      "\n",
      "open-university\n",
      "TEXT:  12 ; word:  open-university ( 1.0 )\n",
      "TEXT:  12 ; word:  it ( 0.04 )\n",
      "TEXT:  12 ; word:  i ( 0.00 )\n",
      "TEXT:  12 ; word:  for ( 0.11 )\n",
      "TEXT:  12 ; word:  finish ( 0.08 )\n",
      "TEXT:  12 ; word:  datetimetoken ( 0.08 )\n",
      "TEXT:  12 ; word:  course ( 0.12 )\n",
      "TEXT:  12 ; word:  a ( 0.02 )\n",
      "TEXT:  12 ; word:  1 ( 0.27 )\n",
      "TEXT:  12 ; word:  . ( 0.16 )\n",
      "\n",
      "\n",
      "s\n",
      "TEXT:  39 ; word:  your ( 0.06 )\n",
      "TEXT:  1 ; word:  write ( 0.16 )\n",
      "TEXT:  4 ; word:  who ( 0.18 )\n",
      "TEXT:  33 ; word:  who ( 0.31 )\n",
      "TEXT:  39 ; word:  who ( 0.13 )\n",
      "TEXT:  8 ; word:  weekend ( 0.40 )\n",
      "TEXT:  24 ; word:  want ( 0.12 )\n",
      "TEXT:  34 ; word:  wake ( 0.17 )\n",
      "TEXT:  34 ; word:  up ( 0.08 )\n",
      "TEXT:  6 ; word:  to ( 0.01 )\n",
      "\n",
      "\n",
      "ignore\n",
      "TEXT:  36 ; word:  your ( 0.05 )\n",
      "TEXT:  36 ; word:  time ( 0.08 )\n",
      "TEXT:  36 ; word:  they ( 0.09 )\n",
      "TEXT:  36 ; word:  submit ( 0.18 )\n",
      "TEXT:  8 ; word:  specific ( 0.22 )\n",
      "TEXT:  36 ; word:  spam ( 0.20 )\n",
      "TEXT:  36 ; word:  recruiter ( 0.26 )\n",
      "TEXT:  8 ; word:  no ( 0.23 )\n",
      "TEXT:  8 ; word:  london ( 0.20 )\n",
      "TEXT:  36 ; word:  indian ( 0.66 )\n",
      "\n",
      "\n",
      "effective\n",
      "TEXT:  36 ; word:  with ( 0.16 )\n",
      "TEXT:  36 ; word:  this ( 0.13 )\n",
      "TEXT:  36 ; word:  oldno ( 0.18 )\n",
      "TEXT:  36 ; word:  me ( 0.07 )\n",
      "TEXT:  36 ; word:  effective ( 1.0 )\n",
      "TEXT:  36 ; word:  datetimetoken ( 0.05 )\n",
      "TEXT:  36 ; word:  about ( 0.10 )\n",
      "TEXT:  36 ; word:  : ( 0.09 )\n",
      "TEXT:  36 ; word:  23 ( 0.20 )\n",
      "TEXT:  36 ; word:  . ( 0.29 )\n",
      "\n",
      "\n",
      "web-apps\n",
      "TEXT:  39 ; word:  you ( 0.03 )\n",
      "TEXT:  39 ; word:  what ( 0.08 )\n",
      "TEXT:  39 ; word:  web-apps ( 1.69 )\n",
      "TEXT:  52 ; word:  web-apps ( 0.84 )\n",
      "TEXT:  52 ; word:  truly ( 0.07 )\n",
      "TEXT:  39 ; word:  training ( 0.28 )\n",
      "TEXT:  39 ; word:  thiswasalink ( 0.09 )\n",
      "TEXT:  39 ; word:  their ( 0.13 )\n",
      "TEXT:  39 ; word:  pay ( 0.22 )\n",
      "TEXT:  52 ; word:  mobile ( 0.32 )\n",
      "\n",
      "\n",
      "yet\n",
      "TEXT:  2 ; word:  you ( 0.02 )\n",
      "TEXT:  39 ; word:  you ( 0.06 )\n",
      "TEXT:  2 ; word:  yet ( 0.72 )\n",
      "TEXT:  20 ; word:  yet ( 0.72 )\n",
      "TEXT:  22 ; word:  yet ( 0.72 )\n",
      "TEXT:  37 ; word:  yet ( 0.72 )\n",
      "TEXT:  39 ; word:  yet ( 1.45 )\n",
      "TEXT:  46 ; word:  yet ( 0.72 )\n",
      "TEXT:  22 ; word:  with ( 0.06 )\n",
      "TEXT:  37 ; word:  with ( 0.06 )\n",
      "\n",
      "\n",
      "monitor\n",
      "TEXT:  18 ; word:  such ( 0.38 )\n",
      "TEXT:  18 ; word:  monitor ( 1.0 )\n",
      "TEXT:  18 ; word:  however ( 0.12 )\n",
      "TEXT:  18 ; word:  guy ( 0.15 )\n",
      "TEXT:  18 ; word:  cool ( 0.21 )\n",
      "TEXT:  18 ; word:  a ( 0.02 )\n",
      "TEXT:  18 ; word:  . ( 0.16 )\n",
      "TEXT:  18 ; word:  , ( 0.00 )\n",
      "TEXT:  18 ; word:  ) ( 0.07 )\n",
      "TEXT:  18 ; word:  ( ( 0.19 )\n",
      "\n",
      "\n",
      "-according\n",
      "TEXT:  39 ; word:  to ( 0.03 )\n",
      "TEXT:  39 ; word:  subsequently ( 0.25 )\n",
      "TEXT:  39 ; word:  on- ( 0.33 )\n",
      "TEXT:  39 ; word:  make ( 0.09 )\n",
      "TEXT:  39 ; word:  impression ( 0.18 )\n",
      "TEXT:  39 ; word:  feedback ( 0.45 )\n",
      "TEXT:  39 ; word:  besides ( 0.12 )\n",
      "TEXT:  39 ; word:  an ( 0.08 )\n",
      "TEXT:  39 ; word:  -according ( 1.0 )\n",
      "TEXT:  39 ; word:  ( ( 0.05 )\n",
      "\n",
      "\n",
      "culmination\n",
      "TEXT:  8 ; word:  work ( 0.07 )\n",
      "TEXT:  8 ; word:  two ( 0.20 )\n",
      "TEXT:  8 ; word:  over ( 0.27 )\n",
      "TEXT:  8 ; word:  on ( 0.04 )\n",
      "TEXT:  8 ; word:  of ( 0.14 )\n",
      "TEXT:  8 ; word:  datetimetoken ( 0.08 )\n",
      "TEXT:  8 ; word:  culmination ( 1.0 )\n",
      "TEXT:  8 ; word:  and ( 0.01 )\n",
      "TEXT:  8 ; word:  ( ( 0.06 )\n",
      "\n",
      "\n",
      "cup\n",
      "TEXT:  39 ; word:  tea ( 0.5 )\n",
      "TEXT:  39 ; word:  quit ( 0.20 )\n",
      "TEXT:  39 ; word:  of ( 0.09 )\n",
      "TEXT:  39 ; word:  month ( 0.08 )\n",
      "TEXT:  39 ; word:  i ( 0.00 )\n",
      "TEXT:  39 ; word:  cup ( 1.0 )\n",
      "TEXT:  39 ; word:  and ( 0.02 )\n",
      "TEXT:  39 ; word:  after ( 0.09 )\n",
      "TEXT:  39 ; word:  4 ( 0.12 )\n",
      "TEXT:  39 ; word:  . ( 0.13 )\n",
      "\n",
      "\n",
      "piece\n",
      "TEXT:  5 ; word:  “a-ha” ( 0.22 )\n",
      "TEXT:  16 ; word:  you ( 0.02 )\n",
      "TEXT:  22 ; word:  you ( 0.03 )\n",
      "TEXT:  16 ; word:  would ( 0.15 )\n",
      "TEXT:  22 ; word:  work ( 0.44 )\n",
      "TEXT:  16 ; word:  with ( 0.05 )\n",
      "TEXT:  39 ; word:  wing ( 0.25 )\n",
      "TEXT:  17 ; word:  virtual ( 0.4 )\n",
      "TEXT:  5 ; word:  together ( 0.37 )\n",
      "TEXT:  22 ; word:  together ( 0.51 )\n",
      "\n",
      "\n",
      "related\n",
      "TEXT:  22 ; word:  your ( 0.10 )\n",
      "TEXT:  22 ; word:  you ( 0.02 )\n",
      "TEXT:  22 ; word:  work ( 0.38 )\n",
      "TEXT:  22 ; word:  time ( 0.08 )\n",
      "TEXT:  22 ; word:  the ( 0.02 )\n",
      "TEXT:  22 ; word:  that ( 0.04 )\n",
      "TEXT:  22 ; word:  some ( 0.11 )\n",
      "TEXT:  22 ; word:  showcase ( 0.17 )\n",
      "TEXT:  22 ; word:  related ( 2.54 )\n",
      "TEXT:  22 ; word:  question ( 0.21 )\n",
      "\n",
      "\n",
      "strangle\n",
      "TEXT:  45 ; word:  time ( 0.08 )\n",
      "TEXT:  45 ; word:  the ( 0.01 )\n",
      "TEXT:  45 ; word:  strangle ( 1.0 )\n",
      "TEXT:  45 ; word:  spend ( 0.23 )\n",
      "TEXT:  45 ; word:  so ( 0.09 )\n",
      "TEXT:  45 ; word:  on ( 0.05 )\n",
      "TEXT:  45 ; word:  much ( 0.12 )\n",
      "TEXT:  45 ; word:  me ( 0.17 )\n",
      "TEXT:  45 ; word:  for ( 0.08 )\n",
      "TEXT:  45 ; word:  computer ( 0.11 )\n",
      "\n",
      "\n",
      "var\n",
      "TEXT:  46 ; word:  var ( 1.0 )\n",
      "TEXT:  46 ; word:  i++ ( 0.18 )\n",
      "TEXT:  46 ; word:  i ( 0.00 )\n",
      "TEXT:  46 ; word:  = ( 0.36 )\n",
      "TEXT:  46 ; word:  < ( 0.22 )\n",
      "TEXT:  46 ; word:  ; ( 0.34 )\n",
      "TEXT:  46 ; word:  5 ( 0.13 )\n",
      "TEXT:  46 ; word:  0 ( 0.32 )\n",
      "\n",
      "\n",
      "thiswasalink\n",
      "TEXT:  46 ; word:  } ( 0.25 )\n",
      "TEXT:  1 ; word:  your ( 0.07 )\n",
      "TEXT:  1 ; word:  you ( 0.05 )\n",
      "TEXT:  16 ; word:  you ( 0.02 )\n",
      "TEXT:  29 ; word:  you ( 0.05 )\n",
      "TEXT:  39 ; word:  you ( 0.02 )\n",
      "TEXT:  17 ; word:  xmas ( 0.22 )\n",
      "TEXT:  17 ; word:  write ( 0.16 )\n",
      "TEXT:  29 ; word:  would ( 0.15 )\n",
      "TEXT:  51 ; word:  work ( 0.09 )\n",
      "\n",
      "\n",
      "educational\n",
      "TEXT:  39 ; word:  wholly ( 0.23 )\n",
      "TEXT:  39 ; word:  i ( 0.00 )\n",
      "TEXT:  39 ; word:  have ( 0.04 )\n",
      "TEXT:  39 ; word:  educational ( 1.0 )\n",
      "TEXT:  39 ; word:  different ( 0.20 )\n",
      "TEXT:  39 ; word:  degree ( 0.09 )\n",
      "TEXT:  39 ; word:  be ( 0.02 )\n",
      "TEXT:  39 ; word:  background ( 0.41 )\n",
      "TEXT:  39 ; word:  a ( 0.01 )\n",
      "TEXT:  39 ; word:  . ( 0.21 )\n",
      "\n",
      "\n",
      "actionable\n",
      "TEXT:  22 ; word:  you ( 0.04 )\n",
      "TEXT:  22 ; word:  to ( 0.02 )\n",
      "TEXT:  22 ; word:  that ( 0.04 )\n",
      "TEXT:  22 ; word:  help ( 0.16 )\n",
      "TEXT:  22 ; word:  get ( 0.06 )\n",
      "TEXT:  22 ; word:  first ( 0.08 )\n",
      "TEXT:  22 ; word:  elusive ( 0.22 )\n",
      "TEXT:  22 ; word:  dev ( 0.06 )\n",
      "TEXT:  22 ; word:  advice ( 0.40 )\n",
      "TEXT:  22 ; word:  actionable ( 1.0 )\n",
      "\n",
      "\n",
      "listing\n",
      "TEXT:  8 ; word:  to ( 0.01 )\n",
      "TEXT:  8 ; word:  the ( 0.01 )\n",
      "TEXT:  8 ; word:  path ( 0.18 )\n",
      "TEXT:  8 ; word:  out ( 0.08 )\n",
      "TEXT:  8 ; word:  my ( 0.06 )\n",
      "TEXT:  8 ; word:  listing ( 1.0 )\n",
      "TEXT:  8 ; word:  list ( 0.15 )\n",
      "TEXT:  8 ; word:  learning ( 0.17 )\n",
      "TEXT:  8 ; word:  in ( 0.08 )\n",
      "TEXT:  8 ; word:  early_on ( 0.53 )\n",
      "\n",
      "\n",
      "30+\n",
      "TEXT:  25 ; word:  that ( 0.04 )\n",
      "TEXT:  25 ; word:  long ( 0.12 )\n",
      "TEXT:  25 ; word:  land ( 0.18 )\n",
      "TEXT:  25 ; word:  job ( 0.14 )\n",
      "TEXT:  25 ; word:  i ( 0.00 )\n",
      "TEXT:  25 ; word:  development ( 0.10 )\n",
      "TEXT:  25 ; word:  before ( 0.20 )\n",
      "TEXT:  25 ; word:  await ( 0.2 )\n",
      "TEXT:  25 ; word:  application ( 0.26 )\n",
      "TEXT:  25 ; word:  30+ ( 1.0 )\n",
      "\n",
      "\n",
      "assist\n",
      "TEXT:  22 ; word:  the ( 0.02 )\n",
      "TEXT:  22 ; word:  rest ( 0.18 )\n",
      "TEXT:  22 ; word:  others ( 0.25 )\n",
      "TEXT:  22 ; word:  of ( 0.02 )\n",
      "TEXT:  22 ; word:  myself ( 0.33 )\n",
      "TEXT:  22 ; word:  in ( 0.05 )\n",
      "TEXT:  22 ; word:  bootstrapping ( 0.28 )\n",
      "TEXT:  22 ; word:  assist ( 1.0 )\n",
      "TEXT:  22 ; word:  and ( 0.03 )\n",
      "\n",
      "\n",
      "multi-user\n",
      "TEXT:  0 ; word:  tracker ( 0.45 )\n",
      "TEXT:  0 ; word:  time ( 0.23 )\n",
      "TEXT:  0 ; word:  that ( 0.06 )\n",
      "TEXT:  0 ; word:  multi-user ( 1.0 )\n",
      "TEXT:  0 ; word:  have ( 0.06 )\n",
      "TEXT:  0 ; word:  functionality ( 0.17 )\n",
      "TEXT:  0 ; word:  for ( 0.03 )\n",
      "TEXT:  0 ; word:  different ( 0.15 )\n",
      "TEXT:  0 ; word:  app ( 0.20 )\n",
      "TEXT:  0 ; word:  admins ( 0.18 )\n",
      "\n",
      "\n",
      "tea\n",
      "TEXT:  39 ; word:  tea ( 1.0 )\n",
      "TEXT:  39 ; word:  take ( 0.07 )\n",
      "TEXT:  39 ; word:  quit ( 0.29 )\n",
      "TEXT:  39 ; word:  month ( 0.10 )\n",
      "TEXT:  39 ; word:  i ( 0.00 )\n",
      "TEXT:  39 ; word:  and ( 0.04 )\n",
      "TEXT:  39 ; word:  after ( 0.13 )\n",
      "TEXT:  39 ; word:  4 ( 0.16 )\n",
      "TEXT:  39 ; word:  . ( 0.16 )\n",
      "\n",
      "\n",
      "member\n",
      "TEXT:  0 ; word:  well ( 0.09 )\n",
      "TEXT:  0 ; word:  review ( 0.26 )\n",
      "TEXT:  0 ; word:  project ( 0.07 )\n",
      "TEXT:  0 ; word:  on-point ( 0.5 )\n",
      "TEXT:  0 ; word:  my ( 0.04 )\n",
      "TEXT:  0 ; word:  member ( 1.0 )\n",
      "TEXT:  0 ; word:  in ( 0.07 )\n",
      "TEXT:  0 ; word:  equally ( 0.09 )\n",
      "TEXT:  0 ; word:  be ( 0.03 )\n",
      "TEXT:  0 ; word:  , ( 0.00 )\n",
      "\n",
      "\n",
      "mid-weight\n",
      "TEXT:  8 ; word:  will ( 0.08 )\n",
      "TEXT:  8 ; word:  to ( 0.00 )\n",
      "TEXT:  8 ; word:  position ( 0.36 )\n",
      "TEXT:  8 ; word:  plenty ( 0.30 )\n",
      "TEXT:  8 ; word:  open ( 0.11 )\n",
      "TEXT:  8 ; word:  of ( 0.04 )\n",
      "TEXT:  8 ; word:  mid-weight ( 1.0 )\n",
      "TEXT:  8 ; word:  company ( 0.09 )\n",
      "TEXT:  8 ; word:  be ( 0.01 )\n",
      "TEXT:  8 ; word:  . ( 0.37 )\n",
      "\n",
      "\n",
      "job\n",
      "TEXT:  48 ; word:  ” ( 0.40 )\n",
      "TEXT:  41 ; word:  “our ( 0.5 )\n",
      "TEXT:  17 ; word:  ~5 ( 0.2 )\n",
      "TEXT:  17 ; word:  ~22 ( 0.5 )\n",
      "TEXT:  8 ; word:  yourself ( 0.12 )\n",
      "TEXT:  22 ; word:  yourself ( 0.16 )\n",
      "TEXT:  5 ; word:  your ( 0.07 )\n",
      "TEXT:  8 ; word:  your ( 0.05 )\n",
      "TEXT:  13 ; word:  your ( 0.06 )\n",
      "TEXT:  22 ; word:  your ( 0.15 )\n",
      "\n",
      "\n",
      "reign\n",
      "TEXT:  18 ; word:  write ( 0.11 )\n",
      "TEXT:  18 ; word:  reign ( 1.0 )\n",
      "TEXT:  18 ; word:  myself ( 0.14 )\n",
      "TEXT:  18 ; word:  express ( 0.28 )\n",
      "TEXT:  18 ; word:  could ( 0.18 )\n",
      "TEXT:  18 ; word:  code ( 0.08 )\n",
      "TEXT:  18 ; word:  by ( 0.11 )\n",
      "TEXT:  18 ; word:  and ( 0.03 )\n",
      "TEXT:  18 ; word:  . ( 0.13 )\n",
      "TEXT:  18 ; word:  , ( 0.00 )\n",
      "\n",
      "\n",
      "9-5\n",
      "TEXT:  36 ; word:  with ( 0.16 )\n",
      "TEXT:  36 ; word:  to ( 0.00 )\n",
      "TEXT:  36 ; word:  pay ( 0.22 )\n",
      "TEXT:  36 ; word:  good ( 0.21 )\n",
      "TEXT:  36 ; word:  go ( 0.08 )\n",
      "TEXT:  36 ; word:  benefit ( 0.24 )\n",
      "TEXT:  36 ; word:  and ( 0.02 )\n",
      "TEXT:  36 ; word:  a ( 0.01 )\n",
      "TEXT:  36 ; word:  9-5 ( 1.0 )\n",
      "TEXT:  36 ; word:  . ( 0.18 )\n",
      "\n",
      "\n",
      "on-site\n",
      "TEXT:  22 ; word:  you ( 0.05 )\n",
      "TEXT:  22 ; word:  though ( 0.24 )\n",
      "TEXT:  22 ; word:  the ( 0.01 )\n",
      "TEXT:  22 ; word:  spend ( 0.14 )\n",
      "TEXT:  22 ; word:  on-site ( 1.80 )\n",
      "TEXT:  22 ; word:  interview ( 0.40 )\n",
      "TEXT:  22 ; word:  every ( 0.18 )\n",
      "TEXT:  22 ; word:  different ( 0.13 )\n",
      "TEXT:  22 ; word:  datetimetoken ( 0.05 )\n",
      "TEXT:  22 ; word:  company ( 0.09 )\n",
      "\n",
      "\n",
      "initially\n",
      "TEXT:  15 ; word:  send ( 0.14 )\n",
      "TEXT:  15 ; word:  out ( 0.08 )\n",
      "TEXT:  15 ; word:  of ( 0.02 )\n",
      "TEXT:  15 ; word:  job ( 0.08 )\n",
      "TEXT:  15 ; word:  initially ( 1.0 )\n",
      "TEXT:  15 ; word:  be ( 0.01 )\n",
      "TEXT:  15 ; word:  application ( 0.17 )\n",
      "TEXT:  15 ; word:  23 ( 0.45 )\n",
      "TEXT:  15 ; word:  . ( 0.16 )\n",
      "TEXT:  15 ; word:  , ( 0.00 )\n",
      "\n",
      "\n",
      "dig\n",
      "TEXT:  35 ; word:  view ( 0.16 )\n",
      "TEXT:  35 ; word:  the ( 0.02 )\n",
      "TEXT:  35 ; word:  project ( 0.12 )\n",
      "TEXT:  35 ; word:  new ( 0.10 )\n",
      "TEXT:  35 ; word:  into ( 0.37 )\n",
      "TEXT:  35 ; word:  dig ( 1.0 )\n",
      "TEXT:  35 ; word:  create ( 0.18 )\n",
      "TEXT:  35 ; word:  a ( 0.01 )\n",
      "TEXT:  35 ; word:  ) ( 0.05 )\n",
      "TEXT:  35 ; word:  ( ( 0.09 )\n",
      "\n",
      "\n",
      "informed\n",
      "TEXT:  8 ; word:  you ( 0.03 )\n",
      "TEXT:  8 ; word:  which ( 0.24 )\n",
      "TEXT:  8 ; word:  to ( 0.00 )\n",
      "TEXT:  8 ; word:  these ( 0.18 )\n",
      "TEXT:  8 ; word:  the ( 0.01 )\n",
      "TEXT:  8 ; word:  possible ( 0.31 )\n",
      "TEXT:  8 ; word:  my ( 0.03 )\n",
      "TEXT:  8 ; word:  main ( 0.15 )\n",
      "TEXT:  8 ; word:  informed ( 1.80 )\n",
      "TEXT:  8 ; word:  go ( 0.08 )\n",
      "\n",
      "\n",
      "dom\n",
      "TEXT:  22 ; word:  wo ( 0.16 )\n",
      "TEXT:  22 ; word:  will ( 0.09 )\n",
      "TEXT:  22 ; word:  up ( 0.08 )\n",
      "TEXT:  22 ; word:  to ( 0.03 )\n",
      "TEXT:  22 ; word:  this ( 0.11 )\n",
      "TEXT:  22 ; word:  the ( 0.01 )\n",
      "TEXT:  22 ; word:  so ( 0.05 )\n",
      "TEXT:  22 ; word:  scale ( 0.15 )\n",
      "TEXT:  22 ; word:  react ( 0.17 )\n",
      "TEXT:  22 ; word:  on ( 0.09 )\n",
      "\n",
      "\n",
      "importantly\n",
      "TEXT:  22 ; word:  where ( 0.11 )\n",
      "TEXT:  44 ; word:  to ( 0.00 )\n",
      "TEXT:  22 ; word:  that ( 0.06 )\n",
      "TEXT:  44 ; word:  remote ( 0.28 )\n",
      "TEXT:  22 ; word:  know ( 0.09 )\n",
      "TEXT:  22 ; word:  importantly ( 0.90 )\n",
      "TEXT:  44 ; word:  importantly ( 0.90 )\n",
      "TEXT:  22 ; word:  i ( 0.00 )\n",
      "TEXT:  44 ; word:  i ( 0.00 )\n",
      "TEXT:  22 ; word:  how ( 0.07 )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#can we do that for all?\n",
    "for mw in range(len(indexing_matrix)-2800):\n",
    "    checkdata = []\n",
    "    for IND in range(len(numpy.where(pseudo_mi[mw,:,:] > 0)[0])):\n",
    "        #print('TEXT',numpy.where(pseudo_mi[1,:,:] > 0)[1][IND])\n",
    "        #print(indexing_matrix[numpy.where(pseudo_mi[1,:,:] > 0)[0][IND]],pseudo_mi[1,numpy.where(pseudo_mi[1,:,:] > 0)[0][IND],numpy.where(pseudo_mi[1,:,:] > 0)[1][IND]])\n",
    "        checkdata.append((numpy.where(pseudo_mi[mw,:,:] > 0)[1][IND],indexing_matrix[numpy.where(pseudo_mi[mw,:,:] > 0)[0][IND]],pseudo_mi[mw,numpy.where(pseudo_mi[mw,:,:] > 0)[0][IND],numpy.where(pseudo_mi[mw,:,:] > 0)[1][IND]]))\n",
    "    print(indexing_matrix[mw])\n",
    "    for data in sorted(checkdata,key=lambda x: x[2], reverse=True)[:10]:\n",
    "            print('TEXT: ',data[0],'; word: ', data[1], '(',str(data[2])[:4],')')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e3fa11e-feb2-487d-9d53-43dc3deb650c": {
     "id": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "prev": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "regions": {
      "07b1e699-5143-43f7-a938-d61bc27e8e62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e1c251e9-1608-43d5-ba5a-e27bb09739f5",
        "part": "whole"
       },
       "id": "07b1e699-5143-43f7-a938-d61bc27e8e62"
      }
     }
    },
    "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5": {
     "id": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "prev": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "regions": {
      "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727",
        "part": "whole"
       },
       "id": "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0"
      }
     }
    },
    "1cf808ac-39f2-4009-8112-3ef3a17b4af6": {
     "id": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "prev": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "regions": {
      "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5992afc-b214-46ae-8f12-1026835184cd",
        "part": "whole"
       },
       "id": "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8"
      }
     }
    },
    "1f7a3d0c-f066-482e-8b40-cf107299d110": {
     "id": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "prev": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "regions": {
      "1badc1e9-6ff7-4358-9493-665026267eab": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d236a59f-ff21-406d-b1df-9436aedbdb11",
        "part": "whole"
       },
       "id": "1badc1e9-6ff7-4358-9493-665026267eab"
      }
     }
    },
    "3659283d-893a-48fa-9115-7dd64d2aed00": {
     "id": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "prev": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "regions": {
      "835576ab-c1f9-4bfa-af97-ed77dbde5925": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e294b910-1a25-4e48-abb5-37239f441f2e",
        "part": "whole"
       },
       "id": "835576ab-c1f9-4bfa-af97-ed77dbde5925"
      }
     }
    },
    "395c02de-c982-4ca8-a48e-10a79828812d": {
     "id": "395c02de-c982-4ca8-a48e-10a79828812d",
     "prev": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "regions": {
      "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3cda7d21-4a2d-4d01-ac74-10e33c83629e",
        "part": "whole"
       },
       "id": "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7"
      }
     }
    },
    "3f4f1ce3-4872-48cc-8c96-31c19dba5a74": {
     "id": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "prev": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "regions": {
      "7fb22b2c-4bff-4c35-86aa-40952a6b9b24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5bb4cd0-4cb1-493f-99e0-e75214f084a7",
        "part": "whole"
       },
       "id": "7fb22b2c-4bff-4c35-86aa-40952a6b9b24"
      }
     }
    },
    "45ee2b8f-c113-44cf-a993-6a20d43e99f5": {
     "id": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "prev": "532186bd-d619-49da-be76-e28eb7db691a",
     "regions": {
      "e4a63835-d541-4073-bee5-0017da9a065b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8e459ff-ae34-48aa-8146-50365df9ea53",
        "part": "whole"
       },
       "id": "e4a63835-d541-4073-bee5-0017da9a065b"
      }
     }
    },
    "532186bd-d619-49da-be76-e28eb7db691a": {
     "id": "532186bd-d619-49da-be76-e28eb7db691a",
     "prev": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "regions": {
      "2b7cff32-e764-4b30-8301-0cb4bb2268dc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384",
        "part": "whole"
       },
       "id": "2b7cff32-e764-4b30-8301-0cb4bb2268dc"
      }
     }
    },
    "99fa0d29-85d3-4e57-a540-74b76519f4ac": {
     "id": "99fa0d29-85d3-4e57-a540-74b76519f4ac",
     "prev": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "regions": {
      "afebcb71-18ec-40b4-9e65-8f01e4c70d57": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "038497e7-3cf4-4c64-867c-1bc637cad5e5",
        "part": "whole"
       },
       "id": "afebcb71-18ec-40b4-9e65-8f01e4c70d57"
      }
     }
    },
    "a59429ee-539c-4c02-bfc8-8582111bf455": {
     "id": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "prev": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "regions": {
      "34164544-7832-445e-a820-4ef4dbc522ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a",
        "part": "whole"
       },
       "id": "34164544-7832-445e-a820-4ef4dbc522ac"
      }
     }
    },
    "aa51d16f-a5b4-460f-91fa-569faac9a6fe": {
     "id": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "prev": null,
     "regions": {
      "e46d430e-2c0b-416c-bac3-86a2d27711c1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "560a94b7-6c8e-465f-96fa-fcf2c3859245",
        "part": "whole"
       },
       "id": "e46d430e-2c0b-416c-bac3-86a2d27711c1"
      }
     }
    },
    "ac6a71ca-a1d6-4449-9ee7-87ff28115363": {
     "id": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "prev": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "regions": {
      "ce9295e6-aa18-4ceb-a277-fb19127d89ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67bf390c-9e34-4764-a6de-ab9d3082090b",
        "part": "whole"
       },
       "id": "ce9295e6-aa18-4ceb-a277-fb19127d89ed"
      }
     }
    },
    "baa3a1da-cdb2-4638-8bec-2986349ac603": {
     "id": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "prev": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "regions": {
      "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59c7646c-3944-4e2c-81a1-1728a02396ae",
        "part": "whole"
       },
       "id": "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce"
      }
     }
    },
    "ca7a6c34-27e7-4921-9da2-43a92faf8e8e": {
     "id": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "prev": "395c02de-c982-4ca8-a48e-10a79828812d",
     "regions": {
      "6414ffa6-ea4d-40fe-9d90-5b0816ff0789": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c1fd569-b2c4-423d-a598-1466fc992a49",
        "part": "whole"
       },
       "id": "6414ffa6-ea4d-40fe-9d90-5b0816ff0789"
      }
     }
    },
    "dae9886f-f215-4b89-aefb-8831c7f2ddfc": {
     "id": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "prev": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "regions": {
      "5eeb753c-493e-4bba-b5ab-025339c264f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d80b2206-d953-46b3-b807-2bf5a8557d28",
        "part": "whole"
       },
       "id": "5eeb753c-493e-4bba-b5ab-025339c264f2"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
