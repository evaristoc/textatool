{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/derekgreene/topic-model-tutorial \n",
    "* https://github.com/derekgreene/topic-model-tutorial/blob/master/topic-modelling-with-scikitlearn.pdf\n",
    "* https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n",
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d80b2206-d953-46b3-b807-2bf5a8557d28"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    def createcands(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        #print(list(group)[::-1])\n",
    "        reversing = [None]\n",
    "        #for el in group:\n",
    "        #    print(el)\n",
    "        for el in group:\n",
    "            if reversing[0] == None:\n",
    "                reversing[0] = el[0]\n",
    "            else:\n",
    "                reversing.insert(0, el[0])\n",
    "        #print(reversing)\n",
    "        for word in reversing:\n",
    "            if current == '':\n",
    "                current = word\n",
    "            else:\n",
    "                current = word + ' ' + current\n",
    "            total.append(current)\n",
    "        #print(list(set(reversing+total)))\n",
    "        return list(set(reversing+total))\n",
    "    \n",
    "    def createcands2(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        for word, pos, chunk in list(group):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            #print(word)\n",
    "            if current == '':\n",
    "                current = word\n",
    "                total.append(word)\n",
    "            else:\n",
    "                current = current+' '+word\n",
    "                total.append(word)\n",
    "        total.append(current)\n",
    "\n",
    "        return list(set(total))\n",
    "            \n",
    "    #candidates = [' '.join(word for word, pos, chunk in group)\n",
    "    candidates = [createcands2(group)\n",
    "                    for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "    #print(candidates)\n",
    "\n",
    "    #return [cand for cand in candidates]\n",
    "    #print(list(itertools.chain.from_iterable(candidates)))\n",
    "    return list(itertools.chain.from_iterable(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        pattern_A01 = re.compile(r'^hey( |$)')\n",
    "        pattern_B01 = re.compile(r'(january|february| march(,|\\.)? |april|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday|morning|afternoon|evening|mont(s|ly)|year(s|ly)?| day(s)?)')\n",
    "        pattern_C01 = re.compile(r'(chance(s)?|opportunit(y|ies))')\n",
    "        \n",
    "        ## Text extraction, REGEXs and Replacements\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text() \\\n",
    "                                                        .lower() \\\n",
    "                                                        .replace('’',\"'\") \\\n",
    "                                                        .replace('‘', \"'\") \\\n",
    "                                                        .replace('´', \"'\") \\\n",
    "                                                        .replace('free code camp', 'fcc') \\\n",
    "                                                        .replace('freecodecamp', 'fcc') \\\n",
    "                                                        .replace('javascript', 'js') \\\n",
    "                                                        .replace('part time', 'parttime') \\\n",
    "                                                        .replace('part-time', 'parttime') \\\n",
    "                                                        .replace('full time', 'fulltime') \\\n",
    "                                                        .replace('full-time', 'fulltime') \\\n",
    "                                                        .replace('web application', 'web-app') \\\n",
    "                                                        .replace('web app', 'web-app') \\\n",
    "                                                        .replace('web development', 'dev') \\\n",
    "                                                        .replace('web-development', 'dev') \\\n",
    "                                                        .replace('developer', 'dev') \\\n",
    "                                                        .replace('web dev', 'dev') \\\n",
    "                                                        .replace('dev position', 'dev job') \\\n",
    "                                                        .replace('dev role', 'dev job') \\\n",
    "                                                        .replace('front end', 'frontend') \\\n",
    "                                                        .replace('front-end','frontend') \\\n",
    "                                                        .replace('back end', 'backend') \\\n",
    "                                                        .replace('back-end','backend') \\\n",
    "                                                        .replace('full stack', 'fullstack') \\\n",
    "                                                        .replace('full-stack','fullstack') \\\n",
    "                                                        .replace('frontend job', 'dev job') \\\n",
    "                                                        .replace('frontend position', 'dev job') \\\n",
    "                                                        .replace('frontend role', 'dev job') \\\n",
    "                                                        .replace('frontend web job', 'dev job') \\\n",
    "                                                        .replace('frontend web position', 'dev job') \\\n",
    "                                                        .replace('frontend web role', 'dev job') \\\n",
    "                                                        .replace('frontend web dev job', 'dev job') \\\n",
    "                                                        .replace('frontend web dev position', 'dev job') \\\n",
    "                                                        .replace('frontend web dev role', 'dev job') \\\n",
    "                                                        .replace('resume', 'cv') \\\n",
    "                                                        .replace('angularjs', 'angular') \\\n",
    "                                                        .replace('angular', 'angularjs') \\\n",
    "                                                        .replace('certification', 'cert') \\\n",
    "                                                        .replace('certificate', 'cert') \\\n",
    "                                                        .replace('machine learning', 'machinelearning') \\\n",
    "                                                        .replace('data science', 'datascience') \\\n",
    "                                                        .replace('self learning', 'self-taught') \\\n",
    "                                                        .replace('self learned', 'self-taught') \\\n",
    "                                                        .replace('self-learning', 'self-taught') \\\n",
    "                                                        .replace('self-learned', 'self-taught') \\\n",
    "                                                        .replace('self taught', 'self-taught') \\\n",
    "                                                        .replace('thanks', 'thank') \\\n",
    "                                                        .replace('thankful', 'thank') \\\n",
    "                                                        .replace('gratitude', 'thank') \\\n",
    "                                                        .replace('many thank', 'thank') \\\n",
    "                                                        .replace('much thank', 'thank') \\\n",
    "                                                        .replace('special thank', 'thank') \\\n",
    "                                                        .replace('big thank', 'thank')\n",
    "                                                        #.replace('app', 'web-app') \\\n",
    "                                                        #.replace('web-dev job', 'dev-job') \\\n",
    "                                                        #.replace('web-dev position', 'web-dev-job') \\\n",
    "                                                        #.replace('web-dev role', 'web-dev-job') \\\n",
    "                                                        #.replace('backend job', 'dev job') \\\n",
    "                                                        #.replace('backend position', 'dev job') \\\n",
    "                                                        #.replace('backend role', 'dev job') \\\n",
    "                                                        #.replace('backend web job', 'dev job') \\\n",
    "                                                        #.replace('backend web position', 'dev job') \\\n",
    "                                                        #.replace('backend web role', 'dev job') \\\n",
    "                                                        #.replace('backend web dev job', 'dev job') \\\n",
    "                                                        #.replace('backend web dev position', 'dev job') \\\n",
    "                                                        #.replace('backend web dev role', 'dev job') \\ \n",
    "                                                        #.replace('dev job', 'dev job') \\\n",
    "                                                        \n",
    "        soup_forumpostTEXT = re.sub(pattern_A01, ' hi ', soup_forumpostTEXT)\n",
    "        soup_forumpostTEXT = soup_forumpostTEXT.replace('fellow camper', '') \\\n",
    "                                    .replace('camper', '') \\\n",
    "                                    .replace('fccers', '') \\\n",
    "                                    .replace('fccer', '') \\\n",
    "                                    .replace('everybody', 'everyone') \\\n",
    "                                    .replace('hello', 'hi') \\\n",
    "                                    .replace('hi everyone', 'hi') \\\n",
    "                                    .replace('hi, everyone', 'hi') \\\n",
    "                                    .replace('hi everybody', 'hi') \\\n",
    "                                    .replace('hi, everybody', 'hi')\n",
    "                                    \n",
    "        soup_forumpostTEXT = re.sub(pattern_B01, ' datetimetoken ', soup_forumpostTEXT)\n",
    "        \n",
    "        soup_forumpostTEXT = re.sub(pattern_C01, ' chance ', soup_forumpostTEXT)\n",
    "        \n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    #print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    #print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                #print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f8e459ff-ae34-48aa-8146-50365df9ea53"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var4: wordimportance_var3 modified to fit keyphrases; changes in the \"opacity\" formula\n",
    "############################\n",
    "def wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=True):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    selectedgrams = None\n",
    "    if unigrams_test:\n",
    "        selectedgrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                         if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                        ])\n",
    "    else:\n",
    "        selectedgrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                        ])        \n",
    "    print('unigrams',len(selectedgrams))\n",
    "    maxdiv = math.log(sorted(selectedgrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "    print('maxdiv', maxdiv)\n",
    "    opacity = collections.defaultdict(float)\n",
    "    #for grams, counts in lemmws_fd.items(): #grams assumes a phrase is possible\n",
    "    for grams, counts in selectedgrams.items():\n",
    "        if grams == '':\n",
    "            opacity[grams] = 0.0\n",
    "            continue\n",
    "        opval = []\n",
    "        #assert len(grams.split()) == 1, print(grams)\n",
    "        for gram in grams.split():\n",
    "            if gram == '':\n",
    "                continue\n",
    "            if gram in list(selectedgrams.keys()):\n",
    "                #if grams == \"new language framework\":\n",
    "                #    print(gram, math.log(selectedgrams[gram]))\n",
    "                opval.append(math.log(selectedgrams[gram]))\n",
    "            else:\n",
    "                opval.append(0.0)\n",
    "        #assert len(opval) != 0, print('grams',grams)\n",
    "        averopval = sum(opval)/len(opval)\n",
    "        if 1 - averopval/maxdiv != 0.0:\n",
    "            opacity[grams] = 1 - averopval/maxdiv\n",
    "        else:\n",
    "            opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "        #if grams == \"new language framework\":\n",
    "        #    print(grams, opacity[grams])\n",
    "    #print('opval',opval[:10])\n",
    "\n",
    "    #sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "    sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(selectedgrams.keys())])\n",
    "    #assert \"new language framework\" not in list(sizing_matrix.keys())\n",
    "    \n",
    "    ## Count lemmatized words/characters per text  \n",
    "    for i,lemmpos_t in enumerate(lemmposrecs):\n",
    "        for k, lemmpos_sts in enumerate(lemmpos_t[3]):\n",
    "            ## Use lemmatized word\n",
    "            #print(lemmpos_sts)\n",
    "            for tk_TUPLE in lemmpos_sts: \n",
    "                lemmw = tk_TUPLE[2]\n",
    "                if lemmw in list(selectedgrams.keys()):\n",
    "                    sizing_matrix[lemmw][i] = sizing_matrix[lemmw][i] + 1\n",
    "\n",
    "        if not unigrams_test:\n",
    "            #print('not unigrams only')\n",
    "            for cand in lemmpos_t[4]: #<--------------------------------- problem!! it counted candidates only once!!!\n",
    "                if len(cand.split()) > 1:\n",
    "                    #assert cand != \"new language framework\", print(cand)\n",
    "                    for w in cand.split():\n",
    "                        sizing_matrix[cand][i] = sizing_matrix[cand][i] + sizing_matrix[w][i]\n",
    "                    sizing_matrix[cand][i] = sizing_matrix[cand][i]/len(cand.split())\n",
    "                        #if cand == \"new language framework\":\n",
    "                        #    print(sizing_matrix[cand][i], sizing_matrix[w][i])\n",
    "\n",
    "\n",
    "    ## Normalization\n",
    "    normalization = dict([(k, (sum(vector)-max(vector)+1)/sum(vector)) if sum(vector) != 0 else (k,0.0) for k, vector in sizing_matrix.items()])\n",
    "    #assert \"new language framework\" not in list(normalization.keys()), normalization[\"new language framework\"]\n",
    "    #normalization = collections.defaultdict(float)\n",
    "    #for k, vector in sizing_matrix.items():\n",
    "    #    if sum(vector) != 0:\n",
    "    #        normalization[k] = (sum(vector)-max(vector))/sum(vector)\n",
    "    #    else:\n",
    "    #        \n",
    "        \n",
    "    \n",
    "    wordimportance = dict([(k, valnorm*opacity[k]) for k, valnorm in normalization.items()])\n",
    "\n",
    "    return wordimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleaningtext(st, unigrams_test = False):\n",
    "    treated_st = []\n",
    "    if not unigrams_test:\n",
    "        for w in st:\n",
    "            treated_st.append(w)\n",
    "    else:\n",
    "        for w in st:\n",
    "            if len(w.split()) == 1:\n",
    "                treated_st.append(w)\n",
    "    countwds = len(treated_st)\n",
    "    return treated_st, countwds\n",
    "\n",
    "def cleaningtext2(st): #discharge phrases\n",
    "    treated_st = []\n",
    "    countwds = len(st)\n",
    "\n",
    "    return treated_st, countwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "59c7646c-3944-4e2c-81a1-1728a02396ae"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## gensim_models2 re-evaluated: modification of gensim_model2's cleanedsts_from_lemmpostxts to fit keyphrases\n",
    "############################\n",
    "\n",
    "def gensim_models2(lemmposrecs, NUM_TOPICS = 15, lemmws_fd = {}, wordimportance = {}, unigrams_test = False, nltk = nltk, gensim = gensim):\n",
    "    \n",
    "    def cleanedsts_from_lemmpostxts2(lemmposrecs, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "        redo_corpus_by_sts = []\n",
    "        for lemmpos_r in lemmposrecs:\n",
    "            candidates = lemmpos_r[4]\n",
    "            treated_lemmcands = cleaningtext(candidates, unigrams_test=unigrams_test)[0] #passing candidates only\n",
    "            redo_corpus_by_sts.append(treated_lemmcands)\n",
    "            #redo_corpus_by_sts.append(candidates)\n",
    "        \n",
    "        #print(len(redo_corpus_by_sts), redo_corpus_by_sts[-1])\n",
    "        return redo_corpus_by_sts\n",
    "        \n",
    "    def basedonBOW(redo_corpus_by_sts):\n",
    "        dictionary = gensim.corpora.Dictionary(redo_corpus_by_sts) #[token for st in redo_corpus_by_sts for token in st]\n",
    "        corpus = [dictionary.doc2bow(text) for text in redo_corpus_by_sts]\n",
    "        return corpus, dictionary\n",
    "    \n",
    "    def basedonTFIDF(corpus):\n",
    "        return gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    def basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance):\n",
    "        \n",
    "        def metriccalc(w):\n",
    "            if w in wordimportance:\n",
    "                return 1.0+2.0**float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        def metriccalc2(w):\n",
    "            if w in wordimportance:\n",
    "                return float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        corpus = []\n",
    "        for sts in redo_corpus_by_sts:\n",
    "            st = []\n",
    "            for w in sts:\n",
    "                st.append((dictionary.token2id[w], metriccalc2(w)))\n",
    "            corpus.append(st)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    redo_corpus_by_sts  = cleanedsts_from_lemmpostxts2(lemmposrecs)\n",
    "    \n",
    "    corpus, dictionary = basedonBOW(redo_corpus_by_sts)\n",
    "    if wordimportance == {'tfidf':True}:\n",
    "        tfidf = basedonTFIDF(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "    if wordimportance != {} and wordimportance != {'tfidf':True}:\n",
    "        corpus = basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance)\n",
    "    \n",
    "    \n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, minimum_probability=0.005, iterations=100, per_word_topics = True, minimum_phi_value = 0.001, id2word=dictionary)\n",
    "    lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "   \n",
    "    return lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e294b910-1a25-4e48-abb5-37239f441f2e"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d236a59f-ff21-406d-b1df-9436aedbdb11"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of treated posts (len(count)) :: 53\n",
      "lemm errs: 19460\n"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, \n",
    "#                                                                                NUM_TOPICS=20,\n",
    "#                                                                                wordimportance = {'tfidf':True})\n",
    "\n",
    "#lda_model.print_topics(num_words=15)\n",
    "\n",
    "#[' '.join([l for wr in rec[3] for w,_,l,pos in wr]) for rec in lemmposrecs]\n",
    "#[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs]\n",
    "#[' '.join([cand for cand in rec[4]]) for rec in lemmposrecs]\n",
    "##https://stackoverflow.com/questions/46282473/error-while-identify-the-coherence-value-from-lda-model\n",
    "#texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "#texts\n",
    "\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                                                  texts=texts,\n",
    "#                                                                  #corpus=corpus,\n",
    "#                                                                  window_size=20,\n",
    "#                                                                  dictionary=dictionary, \n",
    "#                                                                  coherence='c_uci')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, \n",
    "#                                                   texts=texts, \n",
    "#                                                   dictionary=dictionary,\n",
    "#                                                   coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "## Compute Coherence Score using UMass\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "#                                     corpus = corpus,\n",
    "#                                     dictionary=dictionary, \n",
    "#                                     coherence=\"u_mass\")\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#dictionary\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TOPICS = range(2,61,2)\n",
    "cv_coherence_values = []\n",
    "for numtopics in TOPICS:\n",
    "    print('\\n\\nFor NUM_TOPICS:', numtopics)\n",
    "    lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=numtopics, wordimportance = {'tfidf':True})\n",
    "    umass_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "                                     corpus = corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"u_mass\")\n",
    "    umass_coherence_lda = umass_coherence_model_lda.get_coherence()\n",
    "    print('U-Mass Coherence Score: ', umass_coherence_lda)\n",
    "    texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "    cv_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     texts=texts,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"c_v\")\n",
    "    cv_coherence_lda = cv_coherence_model_lda.get_coherence()\n",
    "    cv_coherence_values.append(cv_coherence_lda)\n",
    "    print('C_V Coherence Score: ', cv_coherence_lda)\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.quora.com/Can-I-combine-LSI-and-K-means-for-text-document-clustering-Are-there-any-sources-to-learn-about-it\n",
    "#http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##https://stackoverflow.com/questions/14261903/how-can-i-open-the-interactive-matplotlib-window-in-ipython-notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "limit=61; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, cv_coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=40, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.print_topics(num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=25, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.128*\"project\" + 0.103*\"company\" + 0.100*\"people\" + 0.099*\"interview\" + 0.097*\"lot\" + 0.095*\"way\" + 0.095*\"code\" + 0.090*\"cv\" + 0.089*\"skill\" + 0.086*\"time\" + 0.084*\"app\" + 0.081*\"experience\" + 0.080*\"recruiter\" + 0.080*\"good\" + 0.077*\"question\"'),\n",
       " (1,\n",
       "  '-0.205*\"design\" + -0.147*\"graphic\" + 0.118*\"success\" + 0.104*\"backend\" + 0.096*\"mern stack\" + 0.096*\"mern\" + -0.091*\"role\" + 0.090*\"group\" + -0.084*\"time http\" + 0.082*\"hard work\" + -0.082*\"http\" + -0.078*\"support\" + -0.078*\"html\" + 0.077*\"long\" + 0.076*\"source\"'),\n",
       " (2,\n",
       "  '-0.126*\"resource\" + -0.119*\"success\" + -0.116*\"story\" + -0.104*\"lawyer\" + 0.094*\"cv\" + -0.094*\"science\" + -0.086*\"group\" + -0.085*\"current\" + 0.083*\"company\" + -0.078*\"article\" + -0.076*\"topic doesnt belong\" + -0.076*\"belong\" + -0.076*\"part learning process\" + -0.076*\"doesnt\" + -0.076*\"school kid\"'),\n",
       " (3,\n",
       "  '-0.132*\"app\" + -0.121*\"logic\" + 0.118*\"role\" + -0.115*\"study\" + -0.102*\"adwords\" + 0.097*\"technology\" + 0.095*\"lawyer\" + -0.091*\"salary\" + 0.090*\"current\" + 0.089*\"student\" + -0.087*\"skill\" + 0.086*\"qa\" + -0.082*\"css js\" + -0.079*\"math\" + -0.077*\"thought\"'),\n",
       " (4,\n",
       "  '-0.127*\"day\" + 0.122*\"internship\" + -0.108*\"current\" + 0.103*\"jquery\" + -0.101*\"role\" + 0.093*\"knowledge\" + 0.088*\"success\" + -0.087*\"lawyer\" + 0.086*\"fulltime\" + 0.085*\"share\" + 0.084*\"test\" + 0.081*\"thank much fcc\" + 0.081*\"journey datetimetoken\" + 0.081*\"previous programming knowledge\" + 0.081*\"first job frontend email dev digital marketing agency\"'),\n",
       " (5,\n",
       "  '-0.128*\"group\" + -0.112*\"design\" + 0.105*\"dev job\" + -0.101*\"graphic\" + 0.097*\"freelance\" + 0.086*\"upwork\" + -0.079*\"challenge\" + -0.075*\"role\" + 0.072*\"dream\" + -0.072*\"day\" + -0.072*\"person\" + 0.072*\"article\" + -0.071*\"thats\" + 0.069*\"css js\" + 0.067*\"idea\"'),\n",
       " (6,\n",
       "  '-0.207*\"internship\" + -0.155*\"day\" + -0.105*\"bar\" + -0.103*\"technology\" + 0.101*\"yesterday\" + -0.096*\"software\" + -0.096*\"udemy\" + 0.096*\"dev post yesterday\" + 0.096*\"experience real time project\" + 0.096*\"happy part platform\" + 0.096*\"interview call\" + 0.096*\"portfolio employer\" + -0.090*\"drunk\" + -0.090*\"reply\" + -0.083*\"single company\"'),\n",
       " (7,\n",
       "  '-0.142*\"internship\" + 0.117*\"day\" + 0.101*\"qa\" + -0.098*\"lawyer\" + -0.095*\"people\" + 0.094*\"single\" + -0.091*\"others\" + 0.090*\"design\" + -0.087*\"little bit code free time\" + -0.087*\"gaining\" + -0.087*\"helpful tip\" + -0.087*\"internship couple freelance project\" + -0.087*\"many many job application\" + -0.087*\"whoo\" + -0.087*\"development knowledge gaining experience fulltime job\"'),\n",
       " (8,\n",
       "  '0.124*\"internship\" + -0.111*\"graduation\" + -0.110*\"email\" + -0.080*\"optimistic\" + -0.080*\"story life\" + -0.080*\"something stuck\" + -0.080*\"promising career\" + -0.080*\"promising\" + -0.080*\"politics university\" + -0.080*\"politics\" + -0.080*\"lucky\" + -0.080*\"minute datetimetoken\" + -0.080*\"first dev job small agency\" + -0.080*\"insurmountable\" + -0.080*\"first time\"'),\n",
       " (9,\n",
       "  '-0.141*\"quincy\" + -0.135*\"qa\" + -0.129*\"thats\" + -0.105*\"right\" + -0.101*\"analyst\" + 0.088*\"employer\" + -0.085*\"frontend dev\" + -0.085*\"assignment\" + 0.078*\"happy\" + 0.075*\"portfolio employer\" + 0.075*\"interview call\" + 0.075*\"dev post yesterday\" + 0.075*\"happy part platform\" + 0.075*\"experience real time project\" + 0.074*\"graphic\"'),\n",
       " (10,\n",
       "  '-0.161*\"lawyer\" + -0.160*\"day\" + 0.118*\"internship\" + 0.092*\"fullstack\" + 0.087*\"self-taught\" + 0.087*\"skill\" + -0.087*\"medium article\" + 0.085*\"idea\" + 0.084*\"future\" + 0.083*\"quincy\" + 0.081*\"pay\" + 0.081*\"datetimetoken time\" + 0.081*\"idea type question\" + 0.081*\"interview frontend dev job today\" + 0.081*\"self-taught fcc\"'),\n",
       " (11,\n",
       "  '-0.170*\"lawyer\" + 0.104*\"intro\" + 0.097*\"path\" + 0.095*\"interview process\" + 0.090*\"email\" + -0.090*\"design\" + -0.090*\"article\" + -0.088*\"medium article\" + -0.085*\"ideas.ataccama.com\" + -0.085*\"satisfied\" + -0.085*\"satisfied current job\" + 0.079*\"thats\" + 0.079*\"college\" + 0.075*\"right\" + 0.073*\"frontend dev\"'),\n",
       " (12,\n",
       "  '-0.122*\"lawyer\" + -0.113*\"jquery\" + 0.096*\"student\" + -0.095*\"github\" + 0.093*\"lol\" + -0.089*\"datetimetoken half\" + 0.079*\"venezuela\" + -0.078*\"failure\" + -0.075*\"internship\" + 0.073*\"way\" + 0.072*\"freelance\" + -0.070*\"article\" + -0.069*\"half\" + -0.068*\"angularjs\" + 0.068*\"upwork\"'),\n",
       " (13,\n",
       "  '-0.135*\"larson\" + -0.135*\"thank much fcc\" + -0.135*\"amazing platform community\" + -0.135*\"previous programming knowledge\" + -0.135*\"first job frontend email dev digital marketing agency\" + -0.135*\"journey datetimetoken\" + -0.100*\"previous\" + -0.093*\"agency\" + 0.093*\"article share story resource\" + 0.093*\"belong\" + 0.093*\"community thank\" + 0.093*\"contributor\" + 0.093*\"doesnt\" + 0.093*\"part learning process\" + 0.093*\"school kid\"'),\n",
       " (14,\n",
       "  '-0.225*\"lawyer\" + -0.133*\"xml\" + -0.133*\"xsl\" + -0.133*\"first period\" + -0.133*\"first frontend dev job\" + -0.133*\"backend cert\" + -0.133*\"second week new place\" + -0.126*\"backend\" + -0.126*\"venezuela\" + -0.112*\"satisfied current job\" + -0.112*\"ideas.ataccama.com\" + -0.112*\"satisfied\" + -0.106*\"call\" + -0.105*\"interview call\" + -0.105*\"dev post yesterday\"'),\n",
       " (15,\n",
       "  '0.102*\"freelance\" + 0.097*\"venezuela\" + -0.090*\"half\" + -0.077*\"agency\" + -0.077*\"within\" + 0.074*\"design\" + 0.073*\"hard\" + -0.070*\"student\" + 0.069*\"hard work\" + -0.068*\"bachelor software technology\" + -0.068*\"cv/cv\" + -0.068*\"lot interest technology roadmap\" + -0.068*\"roadmap\" + -0.068*\"single company\" + -0.068*\"student dev within datetimetoken company\"'),\n",
       " (16,\n",
       "  '-0.160*\"day\" + 0.120*\"logic\" + 0.113*\"math\" + -0.113*\"app\" + -0.100*\"adwords\" + -0.091*\"datetimetoken half\" + -0.079*\"group\" + 0.079*\"lawyer\" + -0.077*\"failure\" + -0.076*\"half\" + 0.076*\"advance\" + 0.076*\"trainer\" + 0.076*\"tribute\" + 0.076*\"advance logic\" + 0.071*\"personal\"'),\n",
       " (17,\n",
       "  '-0.165*\"day\" + -0.149*\"lawyer\" + 0.100*\"thank much fcc\" + 0.100*\"previous programming knowledge\" + 0.100*\"larson\" + 0.100*\"journey datetimetoken\" + 0.100*\"first job frontend email dev digital marketing agency\" + 0.100*\"amazing platform community\" + 0.092*\"contributor\" + 0.092*\"topic doesnt belong\" + 0.092*\"belong\" + 0.092*\"article share story resource\" + 0.092*\"community thank\" + 0.092*\"doesnt\" + 0.092*\"school kid\"'),\n",
       " (18,\n",
       "  '-0.119*\"employer\" + 0.117*\"recruiter\" + -0.112*\"experience real time project\" + -0.112*\"happy part platform\" + -0.112*\"interview call\" + -0.112*\"portfolio employer\" + -0.112*\"dev post yesterday\" + -0.102*\"qa\" + 0.097*\"udacity\" + 0.085*\"xml\" + 0.085*\"first period\" + 0.085*\"first frontend dev job\" + 0.085*\"second week new place\" + 0.085*\"xsl\" + 0.085*\"backend cert\"'),\n",
       " (19,\n",
       "  '0.183*\"day\" + -0.121*\"lawyer\" + 0.109*\"interview frontend dev job today\" + 0.109*\"idea type question\" + 0.109*\"datetimetoken time\" + 0.109*\"self-taught fcc\" + 0.103*\"css js\" + 0.097*\"start\" + 0.087*\"technology\" + 0.085*\"call\" + 0.085*\"experience real time project\" + 0.085*\"dev post yesterday\" + 0.085*\"portfolio employer\" + 0.085*\"happy part platform\" + 0.085*\"interview call\"')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"group\" + 0.002*\"prototype\" + 0.002*\"daughter\" + 0.002*\"portal\" + 0.002*\"good thing\" + 0.001*\"weekend\" + 0.001*\"person\" + 0.001*\"app\" + 0.001*\"create\" + 0.001*\"frustration programming\" + 0.001*\"startup company\" + 0.001*\"web stuff\" + 0.001*\"urban\" + 0.001*\"shy person\" + 0.001*\"gardening\" + 0.001*\"responsible\" + 0.001*\"time work\" + 0.001*\"shy\" + 0.001*\"visualization\" + 0.001*\"tech skill\"'),\n",
       " (1,\n",
       "  '0.001*\"similar\" + 0.001*\"share story\" + 0.001*\"small real-life coding exercise\" + 0.001*\"small remote\" + 0.001*\"solution similar problem\" + 0.001*\"something codepen\" + 0.001*\"something similar product card\" + 0.001*\"technology knew\" + 0.001*\"viewpoint\" + 0.001*\"development knowledge moment\" + 0.001*\"dedication\" + 0.001*\"current job time\" + 0.001*\"criterion\" + 0.001*\"computer science-y stuff\" + 0.001*\"community environment chance\" + 0.001*\"card\" + 0.001*\"awesome community\" + 0.001*\"role frontend design\" + 0.001*\"shopify platform\" + 0.001*\"agency canada\"'),\n",
       " (2,\n",
       "  '0.003*\"qa\" + 0.002*\"analyst\" + 0.002*\"assignment\" + 0.001*\"fullstack\" + 0.001*\"machinelearning\" + 0.001*\"datascience\" + 0.001*\"pay\" + 0.001*\"backend\" + 0.001*\"skill\" + 0.001*\"startup\" + 0.001*\"hackathons\" + 0.001*\"dream job future\" + 0.001*\"node.js\" + 0.001*\"scientist\" + 0.001*\"fcc backend project\" + 0.001*\"hunt\" + 0.001*\"fun\" + 0.001*\"quincy\" + 0.001*\"city\" + 0.001*\"curriculum\"'),\n",
       " (3,\n",
       "  '0.002*\"adwords\" + 0.002*\"jquery\" + 0.002*\"app\" + 0.001*\"ad\" + 0.001*\"script\" + 0.001*\"boss\" + 0.001*\"code\" + 0.001*\"recruiter\" + 0.001*\"company\" + 0.001*\"salary\" + 0.001*\"people\" + 0.001*\"project\" + 0.001*\"web-app\" + 0.001*\"code base\" + 0.001*\"someone experience\" + 0.001*\"entry level dev job\" + 0.001*\"end interview\" + 0.001*\"jquery frontend asp.net visual basic backend\" + 0.001*\"relevant\" + 0.001*\"asp.net\"'),\n",
       " (4,\n",
       "  '0.005*\"day\" + 0.002*\"object\" + 0.001*\"comment\" + 0.001*\"leader\" + 0.001*\"right\" + 0.001*\"gulp\" + 0.001*\"path\" + 0.001*\"employee\" + 0.001*\"additional\" + 0.001*\"accomplishment\" + 0.001*\"blog\" + 0.001*\"much\" + 0.001*\"framework\" + 0.001*\"client\" + 0.001*\"github\" + 0.001*\"thought\" + 0.001*\"js\" + 0.001*\"environment\" + 0.001*\"project\" + 0.001*\"countless number applicant\"'),\n",
       " (5,\n",
       "  '0.003*\"venezuela\" + 0.002*\"freelance\" + 0.001*\"first dev job age\" + 0.001*\"food home\" + 0.001*\"freelance job week\" + 0.001*\"freelance summer\" + 0.001*\"frontend backend cert\" + 0.001*\"js technology deep way\" + 0.001*\"local dev job\" + 0.001*\"night hour spent reading coding\" + 0.001*\"post fcc solid path\" + 0.001*\"call guy\" + 0.001*\"obligation\" + 0.001*\"spent\" + 0.001*\"fcc datetimetoken\" + 0.001*\"deep\" + 0.001*\"hard time\" + 0.001*\"interview next datetimetoken\" + 0.001*\"age\" + 0.001*\"food\"'),\n",
       " (6,\n",
       "  '0.001*\"firm\" + 0.001*\"resource\" + 0.001*\"everything\" + 0.001*\"goal\" + 0.001*\"fact\" + 0.001*\"passionate\" + 0.001*\"approach\" + 0.001*\"education\" + 0.001*\"entire life field\" + 0.001*\"belgium\" + 0.001*\"tutorials…\" + 0.001*\"job search\" + 0.001*\"entry point\" + 0.001*\"glad\" + 0.001*\"economics\" + 0.001*\"lucas\" + 0.001*\"learning consult different resource\" + 0.001*\"confidence enough knowledge\" + 0.001*\"hands-on\" + 0.001*\"glad share story\"'),\n",
       " (7,\n",
       "  '0.002*\"success\" + 0.002*\"additional library\" + 0.002*\"post long time\" + 0.002*\"friend local fcc group everyone\" + 0.002*\"job fulltime dev\" + 0.002*\"thank success story\" + 0.002*\"success story fcc forum\" + 0.002*\"frontend cert along udemy course\" + 0.002*\"proud\" + 0.002*\"na\" + 0.002*\"story\" + 0.001*\"next month\" + 0.001*\"mern\" + 0.001*\"library\" + 0.001*\"success story\" + 0.001*\"mern stack\" + 0.001*\"along\" + 0.001*\"holiday\" + 0.001*\"hard work\" + 0.001*\"additional\"'),\n",
       " (8,\n",
       "  '0.001*\"big thank\" + 0.001*\"fcc one\" + 0.001*\"first interview hour\" + 0.001*\"flaw\" + 0.001*\"edit:3.\" + 0.001*\"supportive community\" + 0.001*\"edit:3. interview\" + 0.001*\"much detail\" + 0.001*\"list stuff\" + 0.001*\"supportive\" + 0.001*\"\\'yes\" + 0.001*\"test well.and\" + 0.001*\"true\" + 0.001*\"fcc flaw\" + 0.001*\"well.and\" + 0.001*\"zeppelin\" + 0.001*\"course true question\" + 0.001*\"sake\" + 0.001*\"thank good luck\" + 0.001*\"strength\"'),\n",
       " (9,\n",
       "  '0.002*\"github\" + 0.002*\"test\" + 0.002*\"lol\" + 0.002*\"source\" + 0.001*\"intro\" + 0.001*\"studying\" + 0.001*\"parent\" + 0.001*\"hi guy\" + 0.001*\"open\" + 0.001*\"nice\" + 0.001*\"work github\" + 0.001*\"league\" + 0.001*\"league good open source project\" + 0.001*\"lot freedom\" + 0.001*\"make sure\" + 0.001*\"github link\" + 0.001*\"job startup parent company city\" + 0.001*\"new app ground\" + 0.001*\"make\" + 0.001*\"project github account\"'),\n",
       " (10,\n",
       "  '0.002*\"energy\" + 0.002*\"huge\" + 0.001*\"recruitment\" + 0.001*\"texas\" + 0.001*\"udacity\" + 0.001*\"json\" + 0.001*\"way\" + 0.001*\"phone\" + 0.001*\"lead\" + 0.001*\"portfolio\" + 0.001*\"next datetimetoken\" + 0.001*\"code\" + 0.001*\"forum\" + 0.001*\"factory\" + 0.001*\"“to\" + 0.001*\"time energy\" + 0.001*\"work coding factory huge team\" + 0.001*\"frontend/fullstack dev job phone\" + 0.001*\"“to learn\" + 0.001*\"first job application\"'),\n",
       " (11,\n",
       "  '0.002*\"idea type question\" + 0.002*\"interview frontend dev job today\" + 0.002*\"datetimetoken time\" + 0.002*\"self-taught fcc\" + 0.002*\"thats\" + 0.002*\"css js\" + 0.002*\"math\" + 0.002*\"logic\" + 0.002*\"start\" + 0.002*\"self-taught\" + 0.002*\"skill\" + 0.002*\"frontend dev\" + 0.001*\"type\" + 0.001*\"right\" + 0.001*\"event\" + 0.001*\"weekly\" + 0.001*\"trainer\" + 0.001*\"tribute\" + 0.001*\"advance\" + 0.001*\"advance logic\"'),\n",
       " (12,\n",
       "  '0.003*\"interview call\" + 0.003*\"dev post yesterday\" + 0.003*\"experience real time project\" + 0.003*\"portfolio employer\" + 0.003*\"happy part platform\" + 0.002*\"happy\" + 0.001*\"yesterday\" + 0.001*\"first frontend dev job\" + 0.001*\"first period\" + 0.001*\"backend cert\" + 0.001*\"employer\" + 0.001*\"call\" + 0.001*\"platform\" + 0.001*\"xml\" + 0.001*\"real\" + 0.001*\"xsl\" + 0.001*\"second week new place\" + 0.001*\"period\" + 0.001*\"place\" + 0.001*\"post\"'),\n",
       " (13,\n",
       "  '0.001*\"cover\" + 0.001*\"letter\" + 0.001*\"candidate\" + 0.001*\"structure\" + 0.001*\"cv\" + 0.001*\"manager\" + 0.001*\"dom\" + 0.001*\"company\" + 0.001*\"cover letter\" + 0.001*\"others\" + 0.001*\"boilerplate\" + 0.001*\"framework\" + 0.001*\"algorithms\" + 0.001*\"problem\" + 0.001*\"people\" + 0.001*\"data\" + 0.001*\"hr\" + 0.001*\"failure\" + 0.001*\"react\" + 0.001*\"code\"'),\n",
       " (14,\n",
       "  '0.002*\"fullstack dev job offer\" + 0.002*\"codecs\" + 0.002*\"organisation\" + 0.001*\"quincy\" + 0.001*\"mean\" + 0.001*\"fullstack\" + 0.001*\"stack\" + 0.001*\"people across globe\" + 0.001*\"quora\" + 0.001*\"me.2\" + 0.001*\"master signal processing\" + 0.001*\"mean stack us\" + 0.001*\"home organisation\" + 0.001*\"remote work\" + 0.001*\"good concepts.3\" + 0.001*\"use google\" + 0.001*\"signal\" + 0.001*\"us\" + 0.001*\"great community\" + 0.001*\"india\"'),\n",
       " (15,\n",
       "  '0.000*\"partnership\" + 0.000*\"presentation communication skill\" + 0.000*\"project management\" + 0.000*\"progression\" + 0.000*\"patient\" + 0.000*\"non-technical\" + 0.000*\"outside\" + 0.000*\"order\" + 0.000*\"open new challenge new approach\" + 0.000*\"numerous interview offer\" + 0.000*\"numerous\" + 0.000*\"npm\" + 0.000*\"nothing month\" + 0.000*\"nothing\" + 0.000*\"nonsense\" + 0.000*\"none skills/technologies\" + 0.000*\"none\" + 0.000*\"people tech\" + 0.000*\"progression chance\" + 0.000*\"presentation\"'),\n",
       " (16,\n",
       "  '0.000*\"partnership\" + 0.000*\"presentation communication skill\" + 0.000*\"project management\" + 0.000*\"progression\" + 0.000*\"patient\" + 0.000*\"non-technical\" + 0.000*\"outside\" + 0.000*\"order\" + 0.000*\"open new challenge new approach\" + 0.000*\"numerous interview offer\" + 0.000*\"numerous\" + 0.000*\"npm\" + 0.000*\"nothing month\" + 0.000*\"nothing\" + 0.000*\"nonsense\" + 0.000*\"none skills/technologies\" + 0.000*\"none\" + 0.000*\"people tech\" + 0.000*\"progression chance\" + 0.000*\"presentation\"'),\n",
       " (17,\n",
       "  '0.002*\"upwork\" + 0.002*\"lot interest technology roadmap\" + 0.002*\"single company\" + 0.002*\"bachelor software technology\" + 0.002*\"cv/cv\" + 0.002*\"roadmap\" + 0.002*\"student dev within datetimetoken company\" + 0.002*\"role\" + 0.002*\"technology\" + 0.002*\"science\" + 0.002*\"bright\" + 0.002*\"good kind resource\" + 0.002*\"much time learning\" + 0.002*\"armenia\" + 0.002*\"month graduation\" + 0.002*\"political science\" + 0.002*\"codacademy\" + 0.002*\"bright datetimetoken\" + 0.002*\"local code camp armenia\" + 0.002*\"dev offer\"'),\n",
       " (18,\n",
       "  '0.001*\"junior\" + 0.001*\"london\" + 0.001*\"role\" + 0.001*\"decision\" + 0.001*\"ability\" + 0.001*\"building\" + 0.001*\"informed\" + 0.001*\"take\" + 0.001*\"final interview\" + 0.001*\"progression\" + 0.001*\"access\" + 0.001*\"box\" + 0.001*\"factor\" + 0.001*\"final\" + 0.001*\"progression chance\" + 0.001*\"senior\" + 0.001*\"etc\" + 0.001*\"skill\" + 0.001*\"sale\" + 0.001*\"method\"'),\n",
       " (19,\n",
       "  '0.002*\"datetimetoken half\" + 0.002*\"failure\" + 0.002*\"sleep\" + 0.002*\"dream\" + 0.002*\"response\" + 0.001*\"half\" + 0.001*\"mind\" + 0.001*\"posting\" + 0.001*\"datetimetoken old united state degree\" + 0.001*\"dev fcc”\" + 0.001*\"education background\" + 0.001*\"sacrifice\" + 0.001*\"fcc”\" + 0.001*\"interview offer\" + 0.001*\"job posting\" + 0.001*\"united\" + 0.001*\"last project\" + 0.001*\"datetimetoken life\" + 0.001*\"“how\" + 0.001*\"sacrifice life\"'),\n",
       " (20,\n",
       "  '0.001*\"universe\" + 0.001*\"rail\" + 0.001*\"webmaster\" + 0.001*\"graduate\" + 0.001*\"english\" + 0.001*\"fresh\" + 0.001*\"puzzle\" + 0.001*\"salary\" + 0.001*\"confidence\" + 0.001*\"passion\" + 0.001*\"love\" + 0.001*\"interviewer\" + 0.001*\"dev job\" + 0.001*\"recruiter\" + 0.001*\"level\" + 0.001*\"article\" + 0.001*\"second\" + 0.001*\"line\" + 0.001*\"entry level salary\" + 0.001*\"urge\"'),\n",
       " (21,\n",
       "  '0.002*\"time http\" + 0.002*\"http\" + 0.002*\"many many job application\" + 0.002*\"gaining\" + 0.002*\"development knowledge gaining experience fulltime job\" + 0.002*\"share others\" + 0.002*\"helpful tip\" + 0.002*\"internship couple freelance project\" + 0.002*\"little bit code free time\" + 0.002*\"whoo\" + 0.002*\"fulltime commitment\" + 0.001*\"graphic design datetimetoken\" + 0.001*\"subject\" + 0.001*\"second week new place\" + 0.001*\"share\" + 0.001*\"xsl\" + 0.001*\"helpful\" + 0.001*\"xml\" + 0.001*\"minute datetimetoken\" + 0.001*\"politics university\"'),\n",
       " (22,\n",
       "  '0.004*\"lawyer\" + 0.002*\"design\" + 0.002*\"journey datetimetoken\" + 0.002*\"previous programming knowledge\" + 0.002*\"thank much fcc\" + 0.002*\"first job frontend email dev digital marketing agency\" + 0.002*\"larson\" + 0.002*\"amazing platform community\" + 0.002*\"satisfied\" + 0.002*\"satisfied current job\" + 0.002*\"ideas.ataccama.com\" + 0.002*\"graphic\" + 0.002*\"medium article\" + 0.002*\"closure\" + 0.002*\"previous\" + 0.002*\"marketing\" + 0.002*\"digital\" + 0.001*\"look\" + 0.001*\"medium\" + 0.001*\"meetup\"'),\n",
       " (23,\n",
       "  '0.001*\"degree\" + 0.001*\"thing lot\" + 0.001*\"university\" + 0.001*\"current\" + 0.001*\"self-taught\" + 0.001*\"country\" + 0.001*\"internet\" + 0.001*\"lot\" + 0.001*\"without\" + 0.001*\"resource\" + 0.001*\"science\" + 0.001*\"field\" + 0.001*\"ireland\" + 0.001*\"lot different frontend project\" + 0.001*\"-of-fcc-gave-me-my-first-job-as-a-frontend-dev\" + 0.001*\"i\\'am\" + 0.001*\"ticket\" + 0.001*\"kind mentality internet place\" + 0.001*\"pragmatic\" + 0.001*\"“our policy\"'),\n",
       " (24,\n",
       "  '0.002*\"internship\" + 0.002*\"part learning process\" + 0.002*\"contributor\" + 0.002*\"belong\" + 0.002*\"school kid\" + 0.002*\"topic doesnt belong\" + 0.002*\"doesnt\" + 0.002*\"article share story resource\" + 0.002*\"community thank\" + 0.001*\"college\" + 0.001*\"python\" + 0.001*\"resource\" + 0.001*\"way\" + 0.001*\"dev job\" + 0.001*\"tracker\" + 0.001*\"college degree\" + 0.001*\"topic\" + 0.001*\"kid\" + 0.001*\"student\" + 0.001*\"angel.co\"')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=25, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dictionary.id2token[2298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "adwords 1581 0.00198948\n",
      "designer 1588 0.00131095\n",
      "boss 1584 0.00129881\n",
      "json 2959 0.00122844\n",
      "udacity 3334 0.00122844\n",
      "script 707 0.00120071\n",
      "app 323 0.00116174\n",
      "lead 3734 0.00113772\n",
      "linkedin 2648 0.000888094\n",
      "recruiter 685 0.000848897\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "happy part platform 1924 0.00196795\n",
      "portfolio employer 1926 0.00196795\n",
      "experience real time project 1923 0.00196795\n",
      "interview call 1925 0.00196795\n",
      "dev post yesterday 1922 0.00196795\n",
      "share 721 0.00169308\n",
      "topic doesnt belong 1935 0.00164048\n",
      "contributor 1930 0.00164048\n",
      "belong 1928 0.00164048\n",
      "doesnt 1931 0.00164048\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "path 64 0.00197845\n",
      "venezuela 130 0.0019701\n",
      "freelance 102 0.0014727\n",
      "intro 227 0.00133973\n",
      "object 634 0.00123151\n",
      "obligation 120 0.00109938\n",
      "freelance summer 104 0.00109938\n",
      "local dev job 116 0.00109938\n",
      "js technology deep way 114 0.00109938\n",
      "first dev job age 99 0.00109938\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "first frontend dev job 291 0.00200805\n",
      "backend cert 289 0.00200805\n",
      "first period 292 0.00200805\n",
      "xsl 298 0.00200805\n",
      "xml 297 0.00200805\n",
      "second week new place 296 0.00200805\n",
      "lol 959 0.00160576\n",
      "period 265 0.00152079\n",
      "place 294 0.0013932\n",
      "first 98 0.00132815\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "design 926 0.00223611\n",
      "graphic 941 0.00186303\n",
      "larson 2195 0.0015164\n",
      "previous programming knowledge 2196 0.0015164\n",
      "journey datetimetoken 2194 0.0015164\n",
      "thank much fcc 2197 0.0015164\n",
      "first job frontend email dev digital marketing agency 2193 0.0015164\n",
      "amazing platform community 2192 0.0015164\n",
      "http 2942 0.00145263\n",
      "digital 423 0.00144621\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "jquery 1730 0.0017115\n",
      "ad 310 0.00107257\n",
      "memphis 2106 0.000999778\n",
      "code base 2091 0.000999778\n",
      "hope project 2099 0.000999778\n",
      "person phone 2108 0.000999778\n",
      "link relevant project 2105 0.000999778\n",
      "ad stackoverflow company memphis 2087 0.000999778\n",
      "job future 2102 0.000999778\n",
      "call next datetimetoken 2089 0.000999778\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "lawyer 2262 0.00340731\n",
      "satisfied 2264 0.00182208\n",
      "satisfied current job 2265 0.00182208\n",
      "ideas.ataccama.com 2261 0.00182208\n",
      "medium article 2263 0.00154533\n",
      "student 2023 0.00118331\n",
      "medium 1680 0.00117948\n",
      "look 1049 0.00117948\n",
      "article 331 0.00116455\n",
      "current 395 0.00110668\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "day 2127 0.00346424\n",
      "skill 725 0.00148931\n",
      "group 943 0.00138864\n",
      "backend 89 0.00131238\n",
      "team 282 0.00128138\n",
      "prototype 1907 0.00127369\n",
      "data 399 0.00118936\n",
      "app 323 0.00117979\n",
      "people 65 0.00113764\n",
      "python 1739 0.00113115\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "success 742 0.00186696\n",
      "math 3648 0.00177609\n",
      "logic 1678 0.00172157\n",
      "job fulltime dev 1634 0.00130533\n",
      "thank success story 1642 0.00130533\n",
      "na 1638 0.00130533\n",
      "success story fcc forum 1641 0.00130533\n",
      "frontend cert along udemy course 1632 0.00130533\n",
      "post long time 1639 0.00130533\n",
      "proud 1640 0.00130533\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "junior 151 0.000966064\n",
      "london 2501 0.000869345\n",
      "role 859 0.000797953\n",
      "building 919 0.000791621\n",
      "ability 2400 0.000791621\n",
      "decision 413 0.000791621\n",
      "informed 2481 0.000748526\n",
      "progression 2544 0.000748526\n",
      "take 2586 0.000748526\n",
      "access 2402 0.000748526\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "#https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 2893\n",
      "maxdiv 7.198931240688173\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=25, wordimportance = wordimportance, unigrams_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.277*\"company\" + 0.221*\"interview\" + 0.214*\"project\" + 0.206*\"job\" + 0.186*\"time\" + 0.142*\"people\" + 0.133*\"thing\" + 0.129*\"datetimetoken\" + 0.128*\"experience\" + 0.125*\"way\" + 0.124*\"code\" + 0.122*\"cv\" + 0.115*\"lot\" + 0.109*\"many\" + 0.106*\"month\" + 0.105*\"dev\" + 0.104*\"question\" + 0.097*\"something\" + 0.097*\"fcc\" + 0.095*\"challenge\"'),\n",
       " (1,\n",
       "  '-0.163*\"fcc\" + -0.161*\"datetimetoken\" + -0.149*\"job\" + -0.139*\"recruiter\" + -0.130*\"week\" + -0.116*\"backend\" + -0.110*\"end\" + -0.110*\"project\" + 0.101*\"cv\" + 0.095*\"thing\" + -0.087*\"guy\" + -0.084*\"call\" + -0.084*\"level\" + -0.083*\"story\" + 0.082*\"code\" + 0.082*\"github\" + -0.079*\"agency\" + 0.075*\"problem\" + -0.075*\"salary\" + -0.074*\"interview\"'),\n",
       " (2,\n",
       "  '0.227*\"dev\" + 0.221*\"job\" + -0.192*\"company\" + 0.168*\"fcc\" + 0.158*\"skill\" + 0.143*\"frontend\" + 0.142*\"lot\" + -0.107*\"recruiter\" + 0.104*\"good\" + 0.104*\"chance\" + 0.103*\"course\" + 0.101*\"thank\" + -0.098*\"call\" + 0.095*\"resource\" + -0.086*\"level\" + 0.082*\"couple\" + 0.080*\"month\" + 0.079*\"professional\" + 0.076*\"css\" + -0.075*\"moment\"'),\n",
       " (3,\n",
       "  '0.163*\"dev\" + -0.154*\"junior\" + -0.115*\"role\" + -0.103*\"chance\" + -0.101*\"couple\" + -0.097*\"senior\" + -0.093*\"decision\" + -0.092*\"etc\" + -0.092*\"skill\" + -0.090*\"sale\" + -0.086*\"tech\" + -0.084*\"technology\" + -0.082*\"ability\" + -0.082*\"professional\" + -0.080*\"building\" + -0.079*\"way\" + -0.079*\"message\" + -0.079*\"technical\" + -0.078*\"employer\" + -0.078*\"life\"'),\n",
       " (4,\n",
       "  '-0.157*\"lot\" + -0.150*\"fcc\" + 0.123*\"email\" + 0.102*\"interview\" + -0.095*\"degree\" + -0.094*\"backend\" + 0.089*\"learning\" + 0.088*\"week\" + -0.079*\"dream\" + -0.079*\"field\" + -0.077*\"coding\" + 0.077*\"salary\" + -0.077*\"startup\" + -0.077*\"fullstack\" + 0.076*\"rate\" + -0.075*\"city\" + 0.074*\"parttime\" + 0.072*\"environment\" + -0.071*\"story\" + 0.070*\"gig\"'),\n",
       " (5,\n",
       "  '0.144*\"fcc\" + -0.131*\"something\" + -0.122*\"lead\" + -0.115*\"designer\" + -0.115*\"someone\" + -0.109*\"time\" + -0.108*\"fact\" + -0.102*\"udacity\" + -0.102*\"tip\" + -0.101*\"track\" + -0.100*\"json\" + -0.100*\"try\" + -0.098*\"portfolio\" + -0.098*\"life\" + -0.096*\"js\" + -0.093*\"linkedin\" + -0.092*\"top\" + -0.092*\"area\" + -0.088*\"basic\" + 0.087*\"job\"'),\n",
       " (6,\n",
       "  '0.197*\"degree\" + 0.176*\"course\" + 0.170*\"lot\" + -0.149*\"skill\" + 0.149*\"current\" + -0.143*\"fcc\" + 0.138*\"chance\" + 0.131*\"resource\" + 0.124*\"science\" + -0.120*\"dev\" + 0.107*\"college\" + 0.102*\"computer\" + 0.101*\"university\" + -0.100*\"good\" + 0.097*\"little\" + -0.091*\"city\" + 0.089*\"programming\" + -0.089*\"startup\" + 0.082*\"datetimetoken\" + 0.080*\"life\"'),\n",
       " (7,\n",
       "  '0.176*\"datetimetoken\" + -0.173*\"skill\" + -0.148*\"lot\" + 0.142*\"dev\" + -0.130*\"backend\" + 0.119*\"js\" + -0.110*\"pay\" + -0.109*\"country\" + 0.105*\"community\" + 0.103*\"language\" + -0.103*\"good\" + 0.102*\"hi\" + -0.101*\"everyone\" + -0.099*\"experience\" + 0.099*\"html\" + 0.091*\"programming\" + -0.091*\"fullstack\" + -0.090*\"internet\" + -0.086*\"degree\" + 0.085*\"first\"'),\n",
       " (8,\n",
       "  '0.159*\"language\" + 0.147*\"web\" + 0.138*\"python\" + 0.133*\"skill\" + -0.131*\"project\" + 0.128*\"team\" + -0.124*\"dev\" + -0.122*\"phone\" + 0.106*\"friend\" + -0.102*\"thing\" + 0.101*\"data\" + 0.097*\"website\" + -0.094*\"communication\" + 0.090*\"programming\" + -0.089*\"online\" + 0.086*\"part\" + -0.085*\"world\" + -0.083*\"class\" + -0.082*\"detail\" + 0.079*\"head\"'),\n",
       " (9,\n",
       "  '-0.176*\"design\" + 0.162*\"team\" + 0.151*\"week\" + -0.137*\"basic\" + -0.134*\"position\" + -0.110*\"new\" + -0.107*\"html\" + -0.106*\"month\" + 0.105*\"international\" + 0.104*\"python\" + 0.102*\"interest\" + -0.095*\"css\" + -0.094*\"graphic\" + -0.089*\"challenge\" + 0.084*\"way\" + 0.083*\"large\" + -0.080*\"experience\" + 0.079*\"head\" + 0.078*\"stack\" + 0.078*\"business\"'),\n",
       " (10,\n",
       "  '0.163*\"dev\" + -0.146*\"path\" + 0.144*\"confidence\" + 0.141*\"salary\" + -0.113*\"datetimetoken\" + 0.104*\"graduate\" + 0.102*\"article\" + -0.098*\"test\" + 0.097*\"fresh\" + 0.097*\"line\" + -0.095*\"good\" + -0.095*\"month\" + 0.093*\"html\" + -0.091*\"community\" + 0.087*\"interviewer\" + 0.086*\"english\" + 0.086*\"file\" + 0.086*\"deeply.at\" + 0.086*\"attraction\" + 0.086*\"gamer\"'),\n",
       " (11,\n",
       "  '0.153*\"js\" + 0.150*\"path\" + 0.149*\"project\" + -0.140*\"job\" + 0.124*\"career\" + -0.117*\"design\" + 0.116*\"much\" + -0.106*\"learning\" + 0.097*\"foundation\" + -0.097*\"new\" + 0.096*\"right\" + -0.096*\"fulltime\" + 0.094*\"additional\" + 0.094*\"employee\" + 0.093*\"object\" + -0.092*\"coding\" + 0.088*\"time\" + 0.087*\"wife\" + -0.084*\"challenge\" + 0.079*\"strong\"'),\n",
       " (12,\n",
       "  '0.153*\"js\" + -0.131*\"resource\" + -0.130*\"app\" + -0.127*\"knowledge\" + -0.123*\"dev\" + -0.116*\"college\" + 0.116*\"fcc\" + 0.111*\"something\" + -0.109*\"goal\" + 0.109*\"design\" + 0.109*\"project\" + -0.105*\"study\" + 0.091*\"position\" + -0.090*\"process\" + -0.089*\"job\" + -0.087*\"time\" + -0.083*\"passionate\" + 0.077*\"codepen\" + -0.076*\"thank\" + -0.074*\"learning\"'),\n",
       " (13,\n",
       "  '0.204*\"design\" + 0.145*\"support\" + -0.138*\"community\" + 0.137*\"graphic\" + 0.122*\"role\" + -0.115*\"something\" + -0.114*\"book\" + 0.109*\"basic\" + 0.108*\"language\" + -0.102*\"git\" + 0.097*\"tutorial\" + -0.095*\"beta\" + -0.093*\"function\" + 0.093*\"engineering\" + -0.091*\"coding\" + -0.089*\"question\" + 0.084*\"bachelor\" + -0.084*\"definition\" + -0.084*\"coffeescript\" + -0.084*\"applet\"'),\n",
       " (14,\n",
       "  '0.162*\"fcc\" + -0.161*\"client\" + 0.155*\"datetimetoken\" + -0.145*\"fulltime\" + -0.134*\"profile\" + -0.116*\"idea\" + 0.111*\"project\" + -0.104*\"experience\" + -0.103*\"upwork\" + -0.099*\"dev\" + -0.099*\"communication\" + -0.098*\"entry\" + -0.097*\"living\" + -0.096*\"everything\" + -0.095*\"view\" + -0.094*\"summer\" + 0.093*\"people\" + 0.090*\"networking\" + -0.090*\"gig\" + 0.085*\"course\"'),\n",
       " (15,\n",
       "  '-0.147*\"datetimetoken\" + -0.146*\"interview\" + -0.141*\"couple\" + -0.127*\"career\" + 0.126*\"app\" + 0.120*\"study\" + 0.109*\"dev\" + -0.105*\"employer\" + 0.101*\"website\" + -0.098*\"passion\" + -0.095*\"learning\" + -0.095*\"php\" + 0.094*\"thought\" + -0.092*\"in-person\" + 0.091*\"thank\" + -0.089*\"learner\" + -0.088*\"path\" + -0.087*\"local\" + -0.087*\"intro\" + -0.086*\"california\"'),\n",
       " (16,\n",
       "  '0.133*\"test\" + -0.121*\"science\" + -0.117*\"role\" + 0.115*\"css\" + 0.112*\"position\" + 0.108*\"course\" + -0.100*\"current\" + 0.100*\"datetimetoken\" + -0.098*\"networking\" + -0.098*\"goal\" + -0.093*\"people\" + -0.093*\"project\" + 0.092*\"html\" + -0.091*\"fcc\" + -0.085*\"environment\" + 0.084*\"thought\" + -0.083*\"way\" + 0.082*\"design\" + 0.080*\"study\" + -0.080*\"support\"'),\n",
       " (17,\n",
       "  '-0.147*\"frontend\" + -0.127*\"way\" + 0.117*\"passionate\" + 0.113*\"goal\" + -0.111*\"datetimetoken\" + 0.107*\"process\" + -0.104*\"course\" + -0.101*\"profile\" + 0.100*\"entire\" + -0.097*\"gig\" + 0.088*\"approach\" + 0.088*\"class\" + -0.086*\"client\" + -0.083*\"cert\" + -0.083*\"curriculum\" + 0.083*\"stuff\" + -0.083*\"regular\" + -0.081*\"science\" + 0.080*\"resource\" + 0.079*\"path\"'),\n",
       " (18,\n",
       "  '0.160*\"people\" + 0.153*\"app\" + -0.125*\"course\" + 0.124*\"skill\" + 0.118*\"luck\" + -0.112*\"frontend\" + 0.103*\"mobile\" + 0.101*\"website\" + 0.097*\"weekly\" + 0.093*\"lot\" + -0.086*\"datetimetoken\" + 0.086*\"switzerland\" + 0.086*\"upcoming\" + 0.086*\"peter\" + 0.086*\"pity\" + 0.086*\"night“\" + 0.086*\"quantity\" + 0.086*\"lecture\" + 0.086*\"enormous\" + 0.086*\"hack-a-thons\"'),\n",
       " (19,\n",
       "  '0.151*\"college\" + 0.148*\"programming\" + -0.134*\"course\" + 0.127*\"role\" + 0.127*\"class\" + -0.113*\"frontend\" + -0.112*\"position\" + 0.112*\"cert\" + 0.102*\"question\" + 0.094*\"support\" + 0.089*\"challenge\" + 0.088*\"employer\" + -0.086*\"project\" + 0.085*\"current\" + -0.081*\"everything\" + 0.079*\"backend\" + 0.078*\"aspect\" + 0.077*\"process\" + 0.074*\"career\" + 0.073*\"path\"')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '0.007*\"course\" + 0.006*\"dev\" + 0.006*\"thank\" + 0.006*\"fcc\" + 0.006*\"frontend\" + 0.005*\"cert\" + 0.005*\"portfolio\" + 0.005*\"datetimetoken\" + 0.005*\"https\" + 0.004*\"month\" + 0.004*\"summer\" + 0.004*\"lot\" + 0.004*\"week\" + 0.004*\"video\" + 0.004*\"js\" + 0.004*\"path\" + 0.004*\"something\" + 0.004*\"job\" + 0.004*\"hi\" + 0.004*\"css\"'),\n",
       " (3,\n",
       "  '0.009*\"start-up\" + 0.006*\"relax\" + 0.006*\"assurance\" + 0.006*\"paycheck\" + 0.006*\"comeback\" + 0.006*\"toronto\" + 0.006*\"tac\" + 0.006*\"toe\" + 0.006*\"tic\" + 0.006*\"contribution\" + 0.006*\"web-based\" + 0.006*\"desperation\" + 0.006*\"straight\" + 0.006*\"fun\" + 0.005*\"heart\" + 0.005*\"profession\" + 0.005*\"game\" + 0.005*\"opening\" + 0.005*\"family\" + 0.005*\"quality\"'),\n",
       " (13,\n",
       "  '0.004*\"mentor\" + 0.004*\"html5\" + 0.004*\"hope\" + 0.003*\"financial\" + 0.003*\"diy-ing\" + 0.003*\"motivational\" + 0.003*\"learning\" + 0.003*\"execution\" + 0.003*\"template\" + 0.003*\"digital\" + 0.003*\"textile\" + 0.003*\"apprenticeship\" + 0.003*\"commercial\" + 0.003*\"part\" + 0.003*\"arises\" + 0.003*\"wordpress\" + 0.003*\"retail\" + 0.003*\"various\" + 0.003*\"seo\" + 0.003*\"mix\"'),\n",
       " (6,\n",
       "  '0.007*\"medium\" + 0.006*\"christmas\" + 0.005*\"article\" + 0.005*\"hi\" + 0.004*\"little\" + 0.004*\"satisfied\" + 0.004*\"ideas.ataccama.com\" + 0.004*\"commitment\" + 0.004*\"studytime\" + 0.004*\"birthday\" + 0.004*\"merry\" + 0.004*\"clue\" + 0.004*\"spreadsheet\" + 0.004*\"teller\" + 0.004*\"wikipedia\" + 0.004*\"untill\" + 0.004*\"podcast\" + 0.004*\"publicity\" + 0.004*\"region.and\" + 0.004*\"psd\"'),\n",
       " (11,\n",
       "  '0.005*\"apps\" + 0.005*\"politics\" + 0.005*\"graduation\" + 0.005*\"promising\" + 0.005*\"insurmountable\" + 0.005*\"optimistic\" + 0.004*\"hybrid\" + 0.004*\"stuck\" + 0.004*\"site\" + 0.004*\"idea\" + 0.003*\"future\" + 0.003*\"reading\" + 0.003*\"mobile\" + 0.003*\"computer\" + 0.003*\"agency\" + 0.003*\"web-apps\" + 0.003*\"career\" + 0.003*\"process\" + 0.003*\"minute\" + 0.003*\"someone\"'),\n",
       " (4,\n",
       "  '0.003*\"procedure\" + 0.002*\"cv/cover\" + 0.002*\"setup\" + 0.002*\"woman\" + 0.002*\"buddy\" + 0.002*\"of…meh\" + 0.002*\"big-picture\" + 0.002*\"non-confrontational\" + 0.002*\"experiment\" + 0.002*\"gigantic\" + 0.002*\"language/process/framework/whatever\" + 0.002*\"town\" + 0.002*\"ahh\" + 0.002*\"sang\" + 0.002*\"got\" + 0.002*\"advancement\" + 0.002*\"lupecamach0\" + 0.002*\"twitter\" + 0.002*\"tech-related\" + 0.002*\"chrome\"'),\n",
       " (21,\n",
       "  '0.004*\"architecture\" + 0.004*\"aspect\" + 0.004*\"horse\" + 0.003*\"dead\" + 0.003*\"nyc\" + 0.003*\"wonderful\" + 0.003*\"analyst/it\" + 0.003*\"puppet\" + 0.003*\"windows/linux\" + 0.003*\"threw\" + 0.003*\"juice\" + 0.003*\"fundamental\" + 0.003*\"restful\" + 0.003*\"situation\" + 0.003*\"content\" + 0.003*\"determination\" + 0.003*\"attempt\" + 0.003*\"specialist\" + 0.003*\"fortune\" + 0.003*\"mixed\"'),\n",
       " (16,\n",
       "  '0.005*\"dark\" + 0.005*\"beaten-up\" + 0.005*\"regressive\" + 0.005*\"strategy\" + 0.005*\"hey\" + 0.005*\"active\" + 0.005*\"perseverence\" + 0.005*\"utility\" + 0.005*\"bullet\" + 0.005*\"reign\" + 0.005*\"manual\" + 0.005*\"elegant\" + 0.005*\"deliberation\" + 0.005*\"off-\" + 0.005*\"begging\" + 0.005*\"scraper\" + 0.005*\"ethos\" + 0.005*\"archaic\" + 0.005*\"dud\" + 0.005*\"middle\"'),\n",
       " (18,\n",
       "  '0.005*\"community\" + 0.005*\"hi\" + 0.005*\"stuff\" + 0.005*\"dev\" + 0.004*\"frontend\" + 0.004*\"class\" + 0.004*\"image\" + 0.004*\"bit\" + 0.004*\"computer\" + 0.004*\"digital\" + 0.004*\"first\" + 0.004*\"datetimetoken\" + 0.004*\"week\" + 0.004*\"emergency\" + 0.004*\"programming\" + 0.004*\"app\" + 0.003*\"job\" + 0.003*\"tech\" + 0.003*\"much\" + 0.003*\"local\"'),\n",
       " (9,\n",
       "  '0.005*\"programming\" + 0.004*\"last\" + 0.004*\"js\" + 0.004*\"php\" + 0.004*\"week\" + 0.004*\"many\" + 0.004*\"part\" + 0.004*\"career\" + 0.004*\"html\" + 0.003*\"tutorial\" + 0.003*\"passion\" + 0.003*\"css\" + 0.003*\"class\" + 0.003*\"personal\" + 0.003*\"test\" + 0.003*\"learning\" + 0.003*\"major\" + 0.003*\"issue\" + 0.003*\"evidence\" + 0.003*\"outline\"'),\n",
       " (8,\n",
       "  '0.010*\"proud\" + 0.010*\"holiday\" + 0.010*\"mern\" + 0.008*\"forum\" + 0.007*\"udemy\" + 0.007*\"library\" + 0.007*\"journey\" + 0.007*\"na\" + 0.007*\"additional\" + 0.007*\"along\" + 0.006*\"local\" + 0.006*\"everyone\" + 0.006*\"hi\" + 0.006*\"success\" + 0.006*\"cert\" + 0.006*\"fulltime\" + 0.006*\"friend\" + 0.006*\"story\" + 0.006*\"stack\" + 0.005*\"long\"'),\n",
       " (10,\n",
       "  '0.003*\"vision\" + 0.003*\"regular\" + 0.003*\"history\" + 0.003*\"freelancing\" + 0.003*\"timely\" + 0.003*\"copywriter\" + 0.003*\"contractor\" + 0.003*\"date\" + 0.003*\"talent\" + 0.003*\"introduction\" + 0.003*\"dependency\" + 0.003*\"proposal\" + 0.003*\"substantial\" + 0.003*\"long-term\" + 0.003*\"option\" + 0.003*\"endpoint\" + 0.003*\"world\" + 0.003*\"transcription\" + 0.003*\"california\" + 0.003*\"wage\"'),\n",
       " (20,\n",
       "  '0.001*\"stable\" + 0.001*\"state\" + 0.001*\"failure\" + 0.001*\"posting\" + 0.001*\"shantanu\" + 0.001*\"dark\" + 0.001*\"steele\" + 0.001*\"tomorrow\" + 0.001*\"4th\" + 0.001*\"accurate\" + 0.001*\"active\" + 0.001*\"archaic\" + 0.001*\"basis\" + 0.001*\"beaten-up\" + 0.001*\"begging\" + 0.001*\"bullet\" + 0.001*\"clojure\" + 0.001*\"interviewed\" + 0.001*\"fcc”\" + 0.001*\"united\"'),\n",
       " (22,\n",
       "  '0.009*\"type\" + 0.007*\"today\" + 0.007*\"css\" + 0.007*\"idea\" + 0.006*\"self-taught\" + 0.006*\"js\" + 0.005*\"question\" + 0.005*\"skill\" + 0.005*\"frontend\" + 0.004*\"dev\" + 0.004*\"start\" + 0.004*\"time\" + 0.004*\"fcc\" + 0.003*\"interview\" + 0.003*\"datetimetoken\" + 0.003*\"job\" + 0.001*\"nature\" + 0.001*\"affair\" + 0.001*\"jam\" + 0.001*\"linux\"'),\n",
       " (7,\n",
       "  '0.004*\"stuff\" + 0.004*\"advice\" + 0.004*\"site\" + 0.004*\"screening\" + 0.003*\"tech\" + 0.003*\"dev\" + 0.003*\"community\" + 0.003*\"fulltime\" + 0.003*\"end\" + 0.003*\"hi\" + 0.003*\"post\" + 0.003*\"expectation\" + 0.003*\"yesterday\" + 0.003*\"monster\" + 0.003*\"array\" + 0.003*\"thank\" + 0.003*\"month\" + 0.003*\"process\" + 0.003*\"way\" + 0.003*\"cert\"'),\n",
       " (12,\n",
       "  '0.001*\"“summary”\" + 0.001*\"commits\" + 0.001*\"hundred\" + 0.001*\"browser\" + 0.001*\"oftentimes\" + 0.001*\"component\" + 0.001*\"favor\" + 0.001*\"challenging\" + 0.001*\"10-20\" + 0.001*\"steep\" + 0.001*\"setting\" + 0.001*\"weight\" + 0.001*\"redux\" + 0.001*\"contending\" + 0.001*\"spectacular\" + 0.001*\"cycle\" + 0.001*\"recruiting\" + 0.001*\"starting\" + 0.001*\"trial\" + 0.001*\"gpa\"'),\n",
       " (24,\n",
       "  '0.005*\"platform\" + 0.005*\"hi\" + 0.004*\"experience\" + 0.004*\"month\" + 0.004*\"dev\" + 0.004*\"thank\" + 0.004*\"lot\" + 0.004*\"chance\" + 0.004*\"learning\" + 0.003*\"week\" + 0.003*\"job\" + 0.003*\"coding\" + 0.003*\"new\" + 0.003*\"community\" + 0.003*\"frontend\" + 0.003*\"good\" + 0.003*\"website\" + 0.003*\"datetimetoken\" + 0.003*\"time\" + 0.003*\"fcc\"'),\n",
       " (1,\n",
       "  '0.006*\"community\" + 0.006*\"beginning\" + 0.006*\"thank\" + 0.005*\"course\" + 0.005*\"position\" + 0.005*\"curriculum\" + 0.005*\"advice\" + 0.004*\"knowledge\" + 0.004*\"month\" + 0.004*\"test\" + 0.004*\"good\" + 0.004*\"share\" + 0.004*\"thread\" + 0.004*\"stuff\" + 0.004*\"title\" + 0.004*\"zeppelin\" + 0.004*\"well.and\" + 0.004*\"requested\" + 0.004*\"webdev\" + 0.004*\"9-10\"'),\n",
       " (19,\n",
       "  '0.005*\"star\" + 0.004*\"person\" + 0.004*\"memphis\" + 0.004*\"everything\" + 0.004*\"twitch\" + 0.004*\"impressed\" + 0.004*\"circumstance\" + 0.004*\"entry\" + 0.004*\"someone\" + 0.003*\"coding\" + 0.003*\"basic\" + 0.003*\"frontend\" + 0.003*\"visual\" + 0.003*\"asp.net\" + 0.003*\"page\" + 0.003*\"affinitysometimes\" + 0.003*\"htmlcssjs\" + 0.003*\"stackoverflow\" + 0.003*\"interviewee\" + 0.003*\"several\"'),\n",
       " (14,\n",
       "  '0.001*\"direction\" + 0.001*\"sme\" + 0.001*\"belgium\" + 0.001*\"knowledge\" + 0.001*\"type\" + 0.001*\"education\" + 0.001*\"search\" + 0.001*\"chain\" + 0.001*\"today\" + 0.001*\"session\" + 0.001*\"economics\" + 0.001*\"story\" + 0.001*\"hands-on\" + 0.001*\"process\" + 0.001*\"hobby\" + 0.001*\"everything\" + 0.001*\"road\" + 0.001*\"purchaser\" + 0.001*\"confidence\" + 0.001*\"on-the-job\"')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "league 1758 0.00610117\n",
      "programming 187 0.00597851\n",
      "freedom 1525 0.00586227\n",
      "hi 35 0.00583979\n",
      "spent 91 0.00553632\n",
      "ground 667 0.00553632\n",
      "mern 977 0.00553632\n",
      "single 1155 0.0052059\n",
      "studying 1030 0.0052059\n",
      "parent 180 0.0052059\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "community 15 0.00591316\n",
      "beginning 942 0.00581373\n",
      "thank 59 0.00559104\n",
      "course 18 0.00546591\n",
      "position 432 0.00520033\n",
      "curriculum 20 0.0051643\n",
      "advice 3 0.00455189\n",
      "knowledge 375 0.00438868\n",
      "month 44 0.00432057\n",
      "test 198 0.00421267\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "course 18 0.00652311\n",
      "dev 23 0.00627335\n",
      "thank 59 0.00613259\n",
      "fcc 27 0.00587162\n",
      "frontend 78 0.00552757\n",
      "cert 70 0.00532902\n",
      "portfolio 431 0.0050261\n",
      "datetimetoken 21 0.00495025\n",
      "https 1344 0.00451634\n",
      "month 44 0.0044746\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "start-up 632 0.00942403\n",
      "toronto 1173 0.00641176\n",
      "comeback 1162 0.00641176\n",
      "paycheck 1164 0.00641176\n",
      "tac 1170 0.00641176\n",
      "tic 1171 0.00641176\n",
      "toe 1172 0.00641176\n",
      "relax 1168 0.00641176\n",
      "assurance 1161 0.00641176\n",
      "web-based 1102 0.00581815\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "procedure 788 0.00256291\n",
      "cv/cover 721 0.00248325\n",
      "setup 801 0.00231737\n",
      "woman 820 0.00226946\n",
      "buddy 704 0.00224029\n",
      "of…meh 774 0.00223576\n",
      "big-picture 699 0.00221759\n",
      "non-confrontational 769 0.0022079\n",
      "experiment 728 0.00219216\n",
      "gigantic 737 0.00218406\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "php/lamp 1296 0.00140607\n",
      "mean/mern 1289 0.00133656\n",
      "monkey-work 1292 0.00132913\n",
      "full 334 0.00130972\n",
      "basis 1273 0.00129854\n",
      "monitor 1291 0.00125866\n",
      "issue 371 0.00124696\n",
      "department 139 0.00123536\n",
      "laptop 751 0.00122424\n",
      "accurate 1270 0.00119542\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "medium 994 0.00671999\n",
      "christmas 988 0.00608481\n",
      "article 233 0.00535278\n",
      "hi 35 0.00485465\n",
      "little 547 0.00432511\n",
      "satisfied 1252 0.00432464\n",
      "ideas.ataccama.com 1250 0.00432464\n",
      "commitment 658 0.00432348\n",
      "studytime 1004 0.00426953\n",
      "birthday 986 0.00425939\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "stuff 194 0.00438108\n",
      "advice 3 0.00359363\n",
      "site 118 0.00352048\n",
      "screening 585 0.00350472\n",
      "tech 587 0.0034704\n",
      "dev 23 0.00340308\n",
      "community 15 0.0033678\n",
      "fulltime 151 0.00334496\n",
      "end 305 0.0033129\n",
      "hi 35 0.00326492\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "holiday 975 0.00979331\n",
      "mern 977 0.00979331\n",
      "proud 979 0.00979331\n",
      "forum 542 0.00798743\n",
      "udemy 888 0.00691055\n",
      "library 976 0.00686098\n",
      "journey 748 0.00670672\n",
      "along 941 0.00669937\n",
      "na 978 0.00669937\n",
      "additional 646 0.00669937\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "programming 187 0.00499842\n",
      "last 162 0.0044698\n",
      "js 83 0.00393551\n",
      "php 185 0.00389921\n",
      "week 65 0.00383893\n",
      "many 170 0.00376245\n",
      "part 48 0.00375719\n",
      "career 101 0.00370991\n",
      "html 992 0.00355319\n",
      "tutorial 887 0.00341297\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-76-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-76-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 4781\n",
      "maxdiv 7.198931240688173\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=15, wordimportance = wordimportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.276*\"company\" + 0.220*\"interview\" + 0.212*\"project\" + 0.204*\"job\" + 0.185*\"time\" + 0.141*\"people\" + 0.132*\"thing\" + 0.128*\"datetimetoken\" + 0.127*\"experience\" + 0.124*\"way\" + 0.123*\"code\" + 0.121*\"cv\" + 0.114*\"lot\" + 0.108*\"many\" + 0.105*\"month\" + 0.104*\"question\" + 0.103*\"dev\" + 0.097*\"something\" + 0.096*\"fcc\" + 0.094*\"challenge\"'),\n",
       " (1,\n",
       "  '-0.155*\"datetimetoken\" + -0.154*\"fcc\" + -0.140*\"job\" + -0.137*\"recruiter\" + -0.126*\"week\" + -0.113*\"backend\" + -0.108*\"end\" + -0.107*\"project\" + 0.097*\"cv\" + 0.093*\"thing\" + -0.085*\"guy\" + -0.084*\"call\" + -0.083*\"level\" + -0.080*\"story\" + 0.079*\"github\" + 0.079*\"code\" + -0.077*\"agency\" + -0.074*\"interview\" + -0.073*\"salary\" + 0.073*\"problem\"'),\n",
       " (2,\n",
       "  '0.219*\"dev\" + 0.219*\"job\" + -0.183*\"company\" + 0.162*\"fcc\" + 0.152*\"skill\" + 0.140*\"frontend\" + 0.134*\"lot\" + 0.100*\"chance\" + 0.099*\"good\" + 0.098*\"thank\" + -0.098*\"recruiter\" + 0.097*\"course\" + 0.093*\"resource\" + -0.091*\"call\" + 0.081*\"couple\" + 0.081*\"month\" + -0.079*\"level\" + 0.076*\"professional\" + 0.073*\"css\" + -0.069*\"moment\"'),\n",
       " (3,\n",
       "  '0.135*\"junior\" + -0.131*\"dev\" + 0.101*\"chance\" + 0.099*\"role\" + 0.096*\"skill\" + 0.084*\"couple\" + 0.083*\"senior\" + 0.081*\"decision\" + 0.079*\"etc\" + 0.079*\"sale\" + 0.079*\"professional\" + -0.078*\"salary\" + 0.078*\"tech\" + -0.078*\"thing\" + 0.075*\"life\" + -0.074*\"month\" + 0.073*\"project\" + 0.073*\"technology\" + 0.071*\"ability\" + 0.070*\"employer\"'),\n",
       " (4,\n",
       "  '0.149*\"fcc\" + 0.149*\"lot\" + -0.109*\"interview\" + -0.106*\"email\" + 0.096*\"dev\" + 0.093*\"degree\" + 0.079*\"backend\" + -0.074*\"rate\" + 0.073*\"field\" + -0.073*\"junior\" + 0.072*\"dream\" + -0.071*\"learning\" + -0.071*\"couple\" + 0.070*\"coding\" + 0.069*\"startup\" + -0.069*\"command\" + 0.067*\"city\" + 0.066*\"fullstack\" + -0.065*\"responsive\" + -0.065*\"template\"'),\n",
       " (5,\n",
       "  '-0.138*\"fcc\" + 0.116*\"lead\" + 0.115*\"something\" + 0.109*\"designer\" + 0.108*\"someone\" + 0.102*\"time\" + 0.101*\"tip\" + 0.099*\"fact\" + 0.096*\"track\" + 0.096*\"try\" + 0.095*\"json\" + 0.095*\"udacity\" + 0.092*\"portfolio\" + 0.087*\"linkedin\" + -0.087*\"lot\" + 0.086*\"area\" + -0.084*\"job\" + 0.084*\"life\" + 0.083*\"top\" + 0.080*\"encouragement\"'),\n",
       " (6,\n",
       "  '-0.201*\"degree\" + -0.166*\"lot\" + -0.153*\"course\" + 0.151*\"fcc\" + -0.146*\"current\" + 0.134*\"dev\" + -0.125*\"chance\" + -0.125*\"resource\" + 0.121*\"skill\" + -0.117*\"science\" + -0.105*\"university\" + -0.101*\"college\" + -0.094*\"computer\" + -0.093*\"little\" + 0.090*\"good\" + -0.084*\"life\" + 0.083*\"city\" + 0.080*\"startup\" + -0.075*\"post\" + -0.075*\"worker\"'),\n",
       " (7,\n",
       "  '0.180*\"skill\" + -0.173*\"datetimetoken\" + 0.138*\"backend\" + 0.133*\"lot\" + -0.120*\"js\" + 0.115*\"pay\" + 0.114*\"good\" + -0.112*\"dev\" + -0.105*\"language\" + -0.105*\"community\" + 0.101*\"everyone\" + 0.101*\"country\" + -0.101*\"hi\" + 0.100*\"fullstack\" + -0.099*\"programming\" + 0.095*\"experience\" + 0.092*\"startup\" + -0.090*\"html\" + 0.086*\"dream\" + -0.082*\"course\"'),\n",
       " (8,\n",
       "  '-0.144*\"language\" + -0.138*\"python\" + -0.137*\"team\" + -0.137*\"web\" + 0.134*\"dev\" + 0.126*\"project\" + -0.125*\"skill\" + 0.114*\"phone\" + -0.109*\"friend\" + -0.101*\"data\" + 0.094*\"thing\" + -0.090*\"website\" + 0.086*\"communication\" + -0.083*\"part\" + -0.082*\"programming\" + 0.082*\"online\" + -0.079*\"head\" + 0.077*\"world\" + 0.077*\"class\" + -0.076*\"business\"'),\n",
       " (9,\n",
       "  '0.159*\"design\" + -0.142*\"team\" + -0.134*\"week\" + 0.129*\"basic\" + 0.117*\"position\" + 0.108*\"new\" + 0.098*\"html\" + -0.097*\"international\" + -0.092*\"python\" + 0.092*\"challenge\" + -0.090*\"interest\" + 0.090*\"css\" + 0.089*\"month\" + -0.086*\"way\" + 0.086*\"experience\" + 0.081*\"graphic\" + -0.075*\"large\" + -0.071*\"head\" + 0.070*\"living\" + -0.070*\"phone\"'),\n",
       " (10,\n",
       "  '-0.143*\"dev\" + 0.137*\"path\" + -0.136*\"confidence\" + -0.133*\"salary\" + 0.107*\"datetimetoken\" + -0.099*\"graduate\" + -0.097*\"article\" + 0.095*\"month\" + -0.094*\"line\" + -0.092*\"fresh\" + -0.090*\"html\" + -0.090*\"web\" + 0.088*\"test\" + 0.086*\"project\" + 0.085*\"good\" + -0.084*\"interviewer\" + -0.082*\"english\" + 0.082*\"community\" + -0.082*\"codecademy.i\" + -0.082*\"beautiful\"'),\n",
       " (11,\n",
       "  '0.127*\"js\" + 0.127*\"path\" + -0.123*\"job\" + 0.117*\"project\" + 0.112*\"career\" + 0.104*\"much\" + 0.103*\"dev\" + -0.100*\"coding\" + 0.091*\"foundation\" + 0.085*\"time\" + -0.082*\"life\" + 0.081*\"right\" + 0.081*\"additional\" + 0.080*\"object\" + 0.080*\"employee\" + 0.079*\"wife\" + -0.078*\"platform\" + -0.075*\"hour\" + -0.074*\"learning\" + 0.072*\"strong\"'),\n",
       " (12,\n",
       "  '-0.161*\"js\" + -0.139*\"something\" + -0.109*\"project\" + 0.106*\"resource\" + 0.103*\"knowledge\" + -0.096*\"git\" + 0.094*\"app\" + -0.092*\"codepen\" + 0.092*\"job\" + 0.091*\"bachelor\" + 0.091*\"dev\" + 0.088*\"learning\" + 0.086*\"goal\" + -0.082*\"fcc\" + 0.079*\"background\" + 0.078*\"college\" + -0.077*\"path\" + -0.077*\"section\" + 0.075*\"role\" + 0.074*\"different\"'),\n",
       " (13,\n",
       "  '-0.241*\"design\" + -0.149*\"support\" + -0.148*\"graphic\" + -0.113*\"tutorial\" + -0.112*\"role\" + -0.104*\"basic\" + -0.102*\"language\" + 0.096*\"community\" + -0.092*\"engineering\" + -0.087*\"photoshop\" + -0.086*\"position\" + 0.085*\"question\" + -0.084*\"new\" + 0.083*\"resource\" + -0.081*\"mozilla\" + -0.081*\"paid\" + -0.081*\"optimisation…\" + -0.081*\"mix\" + -0.081*\"apprenticeship\" + -0.081*\"execution\"'),\n",
       " (14,\n",
       "  '-0.166*\"fcc\" + -0.142*\"datetimetoken\" + 0.138*\"fulltime\" + 0.127*\"client\" + -0.124*\"project\" + 0.113*\"dev\" + 0.106*\"profile\" + 0.101*\"everything\" + -0.100*\"networking\" + -0.099*\"people\" + 0.094*\"experience\" + 0.090*\"idea\" + -0.090*\"skype\" + 0.086*\"coding\" + 0.085*\"entry\" + 0.085*\"communication\" + -0.083*\"person\" + 0.082*\"upwork\" + 0.080*\"living\" + 0.076*\"summer\"')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"computer\" + 0.003*\"bit\" + 0.002*\"little\" + 0.002*\"anyone\" + 0.002*\"image\" + 0.002*\"big\" + 0.002*\"stuff\" + 0.002*\"tech\" + 0.002*\"relation\" + 0.002*\"profit\" + 0.002*\"html/css/php\" + 0.002*\"cad\" + 0.002*\"flexible\" + 0.002*\"diploma\" + 0.002*\"dps\" + 0.002*\"2-3\" + 0.002*\"documentation\" + 0.002*\"dev\" + 0.002*\"emergency\" + 0.002*\"month\"'),\n",
       " (1,\n",
       "  '0.004*\"thank\" + 0.004*\"month\" + 0.003*\"dev\" + 0.003*\"week\" + 0.003*\"frontend\" + 0.003*\"time\" + 0.003*\"local\" + 0.003*\"datetimetoken\" + 0.003*\"job\" + 0.003*\"course\" + 0.003*\"single\" + 0.003*\"project\" + 0.003*\"post\" + 0.003*\"lot\" + 0.002*\"fcc\" + 0.002*\"cert\" + 0.002*\"spent\" + 0.002*\"mern\" + 0.002*\"first\" + 0.002*\"last\"'),\n",
       " (2,\n",
       "  '0.002*\"beginning\" + 0.002*\"knowledge\" + 0.002*\"limit\" + 0.002*\"\\'yes\" + 0.002*\"community\" + 0.002*\"zeppelin\" + 0.002*\"example\" + 0.002*\"well.and\" + 0.002*\"share\" + 0.002*\"true\" + 0.002*\"previous\" + 0.002*\"flaw\" + 0.002*\"big\" + 0.001*\"unit\" + 0.001*\"sake\" + 0.001*\"btter\" + 0.001*\"thread\" + 0.001*\"test\" + 0.001*\"course\" + 0.001*\"solid\"'),\n",
       " (3,\n",
       "  '0.003*\"commitment\" + 0.003*\"whoo\" + 0.003*\"gaining\" + 0.002*\"free\" + 0.002*\"thank\" + 0.002*\"tip\" + 0.002*\"many\" + 0.002*\"little\" + 0.002*\"wordpress\" + 0.002*\"issue\" + 0.002*\"uncomfortable\" + 0.002*\"advice\" + 0.002*\"fulltime commitment\" + 0.002*\"mid-career\" + 0.002*\"helpful tip\" + 0.002*\"rails.i\" + 0.002*\"experience\" + 0.002*\"fulltime\" + 0.002*\"partovi\" + 0.002*\"trigger\"'),\n",
       " (4,\n",
       "  '0.004*\"hi\" + 0.004*\"course\" + 0.004*\"stuff\" + 0.003*\"beginning\" + 0.003*\"coding\" + 0.003*\"experience\" + 0.003*\"dev\" + 0.003*\"html\" + 0.003*\"month\" + 0.003*\"position\" + 0.003*\"knowledge\" + 0.003*\"css\" + 0.003*\"community\" + 0.003*\"something\" + 0.003*\"today\" + 0.003*\"lot\" + 0.003*\"story\" + 0.003*\"first\" + 0.003*\"chance\" + 0.003*\"thank\"'),\n",
       " (5,\n",
       "  '0.002*\"couple\" + 0.002*\"california\" + 0.002*\"fulltime\" + 0.002*\"month\" + 0.002*\"learning\" + 0.002*\"school\" + 0.002*\"last\" + 0.002*\"chance\" + 0.002*\"lot\" + 0.002*\"field\" + 0.002*\"takeaway\" + 0.002*\"option\" + 0.002*\"thing\" + 0.002*\"interest\" + 0.002*\"entire\" + 0.002*\"message\" + 0.002*\"dev\" + 0.002*\"meetups\" + 0.002*\"coding\" + 0.002*\"“who\"'),\n",
       " (6,\n",
       "  '0.004*\"awsome\" + 0.004*\"increadible\" + 0.004*\"//manuelbasanta.github.io/\" + 0.004*\"chellanges\" + 0.004*\"cs50\" + 0.004*\"cs50 edx\" + 0.004*\"edx\" + 0.004*\"aproach\" + 0.003*\"increadible website\" + 0.003*\"hope\" + 0.003*\"someone\" + 0.002*\"https\" + 0.002*\"helpful\" + 0.002*\"market\" + 0.002*\"cert\" + 0.002*\"react angularjs\" + 0.002*\"course\" + 0.002*\"website\" + 0.002*\"programming\" + 0.002*\"portfolio\"'),\n",
       " (7,\n",
       "  '0.003*\"curriculum\" + 0.003*\"game\" + 0.002*\"heart\" + 0.002*\"position\" + 0.002*\"tac\" + 0.002*\"assurance\" + 0.002*\"toe\" + 0.002*\"tic\" + 0.002*\"comeback\" + 0.002*\"month\" + 0.002*\"kid toronto\" + 0.002*\"paycheck\" + 0.002*\"start-up\" + 0.002*\"thank\" + 0.002*\"relax\" + 0.002*\"beginning\" + 0.002*\"straight\" + 0.002*\"course\" + 0.002*\"toronto\" + 0.002*\"desperation\"'),\n",
       " (8,\n",
       "  '0.003*\"frontend\" + 0.003*\"old\" + 0.003*\"programming\" + 0.003*\"hi\" + 0.003*\"“how\" + 0.003*\"posting\" + 0.003*\"fcc”\" + 0.003*\"united\" + 0.003*\"larson\" + 0.003*\"background\" + 0.003*\"life\" + 0.003*\"word\" + 0.003*\"motivation\" + 0.003*\"sacrifice\" + 0.003*\"word motivation\" + 0.002*\"puppet\" + 0.002*\"windows/linux\" + 0.002*\"stress\" + 0.002*\"clone\" + 0.002*\"digital\"'),\n",
       " (9,\n",
       "  '0.001*\"command\" + 0.001*\"example\" + 0.000*\"db\" + 0.000*\"location\" + 0.000*\"enthousiasm\" + 0.000*\"dozen\" + 0.000*\"rate\" + 0.000*\"trainee\\\\junior\" + 0.000*\"unnecessary\" + 0.000*\"euphoria\" + 0.000*\"js\\\\react\\\\angularjs\" + 0.000*\"ad\" + 0.000*\"wordpress\" + 0.000*\"consulting\" + 0.000*\"irl\" + 0.000*\"corner\" + 0.000*\"anything\" + 0.000*\"onwards\" + 0.000*\"position\" + 0.000*\"workplace\"'),\n",
       " (10,\n",
       "  '0.003*\"js\" + 0.003*\"frontend\" + 0.002*\"css\" + 0.002*\"node\" + 0.002*\"anything\" + 0.002*\"part\" + 0.002*\"interest\" + 0.002*\"skill\" + 0.002*\"dev\" + 0.002*\"large\" + 0.002*\"journalism\" + 0.002*\"craigs\" + 0.002*\"logical\" + 0.002*\"updated\" + 0.002*\"american\" + 0.002*\"emotionally\" + 0.002*\"overflow\" + 0.002*\"locaton\" + 0.002*\"trip\" + 0.002*\"-your\"'),\n",
       " (11,\n",
       "  '0.004*\"hi\" + 0.003*\"week\" + 0.003*\"cert\" + 0.003*\"thank\" + 0.003*\"dev\" + 0.003*\"new\" + 0.003*\"community\" + 0.002*\"learning\" + 0.002*\"life\" + 0.002*\"js\" + 0.002*\"someone\" + 0.002*\"basic\" + 0.002*\"programming\" + 0.002*\"time\" + 0.002*\"fcc\" + 0.002*\"website\" + 0.002*\"huge\" + 0.002*\"frontend\" + 0.002*\"way\" + 0.002*\"month\"'),\n",
       " (12,\n",
       "  '0.003*\"community\" + 0.003*\"curriculum\" + 0.002*\"share\" + 0.002*\"article\" + 0.002*\"thank\" + 0.002*\"professional\" + 0.002*\"belong\" + 0.002*\"contributor\" + 0.002*\"someone\" + 0.002*\"coffee\" + 0.002*\"doesnt\" + 0.002*\"connection\" + 0.002*\"topic\" + 0.002*\"topic doesnt belong\" + 0.002*\"part\" + 0.002*\"dev\" + 0.002*\"learning\" + 0.002*\"month\" + 0.002*\"kid\" + 0.002*\"story\"'),\n",
       " (13,\n",
       "  '0.003*\"css\" + 0.003*\"html\" + 0.003*\"tutorial\" + 0.002*\"algebra\" + 0.002*\"template\" + 0.002*\"thought\" + 0.002*\"month\" + 0.002*\"hour\" + 0.002*\"page\" + 0.002*\"paper\" + 0.002*\"initial\" + 0.002*\"codepen\" + 0.002*\"php\" + 0.002*\"mistake\" + 0.002*\"fcc\" + 0.002*\"skill\" + 0.002*\"test\" + 0.002*\"basic\" + 0.002*\"edx.org\" + 0.002*\"learning\"'),\n",
       " (14,\n",
       "  '0.003*\"medium\" + 0.003*\"article\" + 0.002*\"medium article\" + 0.002*\"current\" + 0.002*\"dev\" + 0.002*\"stuff\" + 0.002*\"last\" + 0.002*\"satisfied\" + 0.002*\"ideas.ataccama.com\" + 0.002*\"awesome\" + 0.002*\"computer\" + 0.002*\"platform\" + 0.002*\"pro\" + 0.002*\"tutorial\" + 0.002*\"fulltime\" + 0.002*\"many\" + 0.002*\"great\" + 0.002*\"lot\" + 0.002*\"good\" + 0.002*\"offer\"')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=15, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09731554182749776"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordimportance[\"new language framework\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "computer 137 0.00282643\n",
      "bit 352 0.00279403\n",
      "little 841 0.00239746\n",
      "anyone 134 0.00237079\n",
      "image 1601 0.00235484\n",
      "big 1011 0.00233213\n",
      "stuff 278 0.00230105\n",
      "tech 909 0.00228523\n",
      "html/css/php 1600 0.00224107\n",
      "relation 1612 0.00224107\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "thank 79 0.00373844\n",
      "month 57 0.003648\n",
      "dev 28 0.00329376\n",
      "week 86 0.0030424\n",
      "frontend 105 0.00302401\n",
      "time 129 0.00292836\n",
      "local 115 0.0029147\n",
      "datetimetoken 25 0.00291374\n",
      "job 50 0.00284896\n",
      "course 21 0.0027844\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "beginning 1554 0.0019701\n",
      "knowledge 558 0.00195711\n",
      "limit 2177 0.00183461\n",
      "'yes 2165 0.0018012\n",
      "community 18 0.0017999\n",
      "zeppelin 2191 0.0017755\n",
      "example 451 0.0017739\n",
      "well.and 2190 0.00177105\n",
      "share 721 0.00166482\n",
      "true 2189 0.00160505\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "commitment 1022 0.00263954\n",
      "whoo 1629 0.00250515\n",
      "gaining 1622 0.00250515\n",
      "free 1159 0.00235387\n",
      "thank 79 0.00232036\n",
      "tip 759 0.00223372\n",
      "many 248 0.0020768\n",
      "little 841 0.00205642\n",
      "wordpress 796 0.00202981\n",
      "issue 547 0.00198533\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "hi 45 0.00434035\n",
      "course 21 0.00409966\n",
      "stuff 278 0.0037344\n",
      "beginning 1554 0.00299524\n",
      "coding 93 0.00295806\n",
      "experience 204 0.00292043\n",
      "dev 28 0.00290916\n",
      "html 1674 0.00286531\n",
      "month 57 0.00285356\n",
      "position 659 0.00283761\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "couple 191 0.00237353\n",
      "california 182 0.00232274\n",
      "fulltime 213 0.00232015\n",
      "month 57 0.00222071\n",
      "learning 234 0.00215452\n",
      "school 277 0.00205301\n",
      "last 231 0.00198243\n",
      "chance 185 0.00193498\n",
      "lot 51 0.00192497\n",
      "field 1338 0.00192481\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "awsome 3490 0.00365086\n",
      "increadible 3500 0.00365086\n",
      "//manuelbasanta.github.io/ 3488 0.00365086\n",
      "chellanges 3492 0.00365086\n",
      "cs50 3493 0.00365086\n",
      "cs50 edx 3494 0.00365086\n",
      "edx 3497 0.00365086\n",
      "aproach 3489 0.00365086\n",
      "increadible website 3501 0.00293871\n",
      "hope 515 0.00263537\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "curriculum 24 0.00291731\n",
      "game 497 0.00261713\n",
      "heart 44 0.00245742\n",
      "position 659 0.00244582\n",
      "tac 2082 0.00243057\n",
      "assurance 2047 0.00236087\n",
      "toe 2085 0.00232078\n",
      "tic 2083 0.00231927\n",
      "comeback 2048 0.00218506\n",
      "month 57 0.00218024\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "frontend 105 0.00322553\n",
      "old 848 0.00308603\n",
      "programming 270 0.00296692\n",
      "hi 45 0.00284362\n",
      "posting 2280 0.00283525\n",
      "fcc” 2276 0.00283525\n",
      "“how 2286 0.00283525\n",
      "united 2284 0.00283525\n",
      "larson 2195 0.00283525\n",
      "background 343 0.00283295\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "command 373 0.000510936\n",
      "example 451 0.000503987\n",
      "db 408 0.000497436\n",
      "location 576 0.000472543\n",
      "enthousiasm 439 0.000472272\n",
      "dozen 427 0.000468699\n",
      "rate 682 0.000467619\n",
      "trainee\\junior 766 0.000465583\n",
      "unnecessary 775 0.000457467\n",
      "euphoria 449 0.000448948\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "k-means to approximate the number of topics before trying a more elaborate form\n",
    "Check and improve previous work:\n",
    "* https://github.com/evaristoc/fccgitterDataScience/blob/master/Identifying%20Relevant%20Topics%20in%20a%20Chatroom.ipynb\n",
    "* https://stackoverflow.com/questions/24816912/number-of-latent-semantic-indexing-topics\n",
    "* https://stackoverflow.com/questions/9582291/how-do-we-decide-the-number-of-dimensions-for-latent-semantic-analysis/9759218#9759218\n",
    "\n",
    "Also check:\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/ (good example but conceptually a bit wrong)\n",
    "* https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "* http://hojunhao.github.io/sgparliament/LDA.html\n",
    "* https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process\n",
    "* https://nlpforhackers.io/recipe-text-clustering/\n",
    "* https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling\n",
    "* http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=3&lambda=1&term=\n",
    "* https://stackoverflow.com/questions/50106516/k-means-for-topic-modelling-elbow-method\n",
    "* https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html\n",
    "* http://wdsinet.org/Annual_Meetings/2016_Proceedings/papers/Paper45.pdf\n",
    "* http://ramet.elte.hu/~podani/Methods.htm\n",
    "* https://hk.saowen.com/a/edc29232eae094158f66e8ff3f08d6f35b8a2a45d628fce8917d2dce6f94282e\n",
    "* https://rare-technologies.com/validating-gensims-topic-coherence-pipeline/\n",
    "* http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html\n",
    "* https://www.searchenginejournal.com/latent-semantic-indexing-wont-help-seo/240705/\n",
    "* https://www.quora.com/What-is-topic-coherence + http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "* https://stackoverflow.com/questions/50340657/pyldavis-with-mallet-lda-implementation-ldamallet-object-has-no-attribute-inf\n",
    "* https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 8\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "K = list(range(1, n_components+1))\n",
    "KM = [KMeans(n_clusters=k).fit(X) for k in K]\n",
    "centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [numpy.argmin(D,axis=1) for D in D_k]\n",
    "dist = [numpy.min(D,axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "\n",
    "kIdx = 8-1\n",
    "\n",
    "# elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(K, avgWithinSS, 'b*-')\n",
    "ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for KMeans clustering')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "true_k = 8\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "newX = numpy.array(pandas.concat([pandas.DataFrame(X),datadf_foran['timestamp_norm'].reset_index()['timestamp_norm']],axis=1))\n",
    "km.fit(newX)\n",
    "\n",
    "print()\n",
    "\n",
    "labels = [x for x in range(datadf_foran.shape[0])]\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(newX, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "fig_clusters = plt.figure()\n",
    "fig_clusters.suptitle('Clusters over first 2 Components')\n",
    "ax = fig_clusters.add_subplot(111)\n",
    "ax.set_xlabel('Component I')\n",
    "ax.set_ylabel('Component II')\n",
    "plt.scatter(newX[:,0],newX[:,1], c=km.fit_predict(newX))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727"
    }
   },
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "038497e7-3cf4-4c64-867c-1bc637cad5e5"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## raw_lda_frankjupyter2: modified to fit allrecordsPreparation2\n",
    "############################\n",
    "def raw_lda_frankjupyter2(lemmposrecs, wordimportance, metmodel=2, STOPWORDS=nltk.corpus.stopwords.words('english')):\n",
    "    '''\n",
    "    description: modified model based on https://www.frankcleary.com/svd/ for a more raw construction of a lda\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    def metriccalc(st, normalizer, wordimportance):\n",
    "        '''\n",
    "        description:\n",
    "        text normalization based on ALL characters in the sentence; why? Example: if two writers wrote 20 words, 2 of them very important, but one of them wrote half of characters stopwords, those 2 words wouldnt be penalized accordingly for this writer: the other wrote more important content\n",
    "        '''       \n",
    "        metfuncs = [\n",
    "            lambda w: math.pow(0.1+float(wordimportance[w]),textbow[w]/normalizer) if w in list(wordimportance.keys()) else 0.0, #a sort of idf-normalization based on number of words in the text: the more the words in a text, the more important\n",
    "            lambda w: float(wordimportance[w])*textbow[w] if w in list(wordimportance.keys()) else 0.0, #good but ignore those words with worimportance too low or 0 but that are frequent in text\n",
    "            lambda w: 1.0+2.0**float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- probably the best one; because it is not normilized this indicator would simply say that if it has the word at least once is already on topic\n",
    "            lambda w: float(wordimportance[w]) + textbow[w]/normalizer if w in list(wordimportance.keys()) else 0.0,\n",
    "            lambda w: (1.0+textbow[w]/normalizer)*float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- apparently the second best with the adhoc word ranking\n",
    "            lambda w: normalizer/textbow[w]*wordimportance[w]/sum(list(wordimportance.values())), #tfidf-ish\n",
    "            lambda w: float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0\n",
    "        ]\n",
    "        likedict = collections.defaultdict(float)\n",
    "        textbow = collections.Counter(st)\n",
    "        for w in st:\n",
    "            likedict[w] = metfuncs[metmodel](w)\n",
    "        return likedict\n",
    "\n",
    "    #redo_corpus_by_sts = []\n",
    "    words_df = pandas.DataFrame()\n",
    "    textreference = {}\n",
    "    \n",
    "    \n",
    "    for textindex, lemmpos_r in enumerate(lemmposrecs):\n",
    "        lemmpos_t = lemmpos_r[3]\n",
    "        #print('lemmpos_t', len(lemmpos_t))\n",
    "        lemm_sts = ''\n",
    "        for lemmpos_TUPLE in lemmpos_t:\n",
    "            w = lemmpos_TUPLE[2]\n",
    "            lemm_sts = lemm_sts + w + ' '\n",
    "        sts = lemm_sts.split('.')\n",
    "        for stindex, lemmpos_st in enumerate(sts):\n",
    "            treated_st, lensts = cleaningtext(lemmpos_st.split(), STOPWORDS=STOPWORDS)        \n",
    "            #print('treated_st', lensts)\n",
    "            if len(treated_st) > 3:\n",
    "                likedict = metriccalc(treated_st, lensts, wordimportance)\n",
    "                st_df = pandas.DataFrame.from_dict(likedict, orient='index')\n",
    "                textindexing = str(textindex)+'_'+str(stindex)\n",
    "                st_df.columns = [textindexing]\n",
    "                textreference[textindexing] = {}\n",
    "                textreference[textindexing]['treated_st'] = treated_st\n",
    "                #st_df.columns = [str(count)]\n",
    "                words_df = words_df.join(st_df, how='outer', )\n",
    "    \n",
    "    words_df = words_df.fillna(0)\n",
    "    print(\"Number of unique words: %s\" % len(words_df))\n",
    "    print(words_df.head(10))\n",
    "    #print(words_df.sort(columns=words_df.columns[0], ascending=False).head(10))\n",
    "    \n",
    "    return words_df, textreference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e3fa11e-feb2-487d-9d53-43dc3deb650c": {
     "id": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "prev": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "regions": {
      "07b1e699-5143-43f7-a938-d61bc27e8e62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e1c251e9-1608-43d5-ba5a-e27bb09739f5",
        "part": "whole"
       },
       "id": "07b1e699-5143-43f7-a938-d61bc27e8e62"
      }
     }
    },
    "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5": {
     "id": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "prev": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "regions": {
      "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727",
        "part": "whole"
       },
       "id": "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0"
      }
     }
    },
    "1cf808ac-39f2-4009-8112-3ef3a17b4af6": {
     "id": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "prev": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "regions": {
      "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5992afc-b214-46ae-8f12-1026835184cd",
        "part": "whole"
       },
       "id": "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8"
      }
     }
    },
    "1f7a3d0c-f066-482e-8b40-cf107299d110": {
     "id": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "prev": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "regions": {
      "1badc1e9-6ff7-4358-9493-665026267eab": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d236a59f-ff21-406d-b1df-9436aedbdb11",
        "part": "whole"
       },
       "id": "1badc1e9-6ff7-4358-9493-665026267eab"
      }
     }
    },
    "3659283d-893a-48fa-9115-7dd64d2aed00": {
     "id": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "prev": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "regions": {
      "835576ab-c1f9-4bfa-af97-ed77dbde5925": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e294b910-1a25-4e48-abb5-37239f441f2e",
        "part": "whole"
       },
       "id": "835576ab-c1f9-4bfa-af97-ed77dbde5925"
      }
     }
    },
    "395c02de-c982-4ca8-a48e-10a79828812d": {
     "id": "395c02de-c982-4ca8-a48e-10a79828812d",
     "prev": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "regions": {
      "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3cda7d21-4a2d-4d01-ac74-10e33c83629e",
        "part": "whole"
       },
       "id": "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7"
      }
     }
    },
    "3f4f1ce3-4872-48cc-8c96-31c19dba5a74": {
     "id": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "prev": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "regions": {
      "7fb22b2c-4bff-4c35-86aa-40952a6b9b24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5bb4cd0-4cb1-493f-99e0-e75214f084a7",
        "part": "whole"
       },
       "id": "7fb22b2c-4bff-4c35-86aa-40952a6b9b24"
      }
     }
    },
    "45ee2b8f-c113-44cf-a993-6a20d43e99f5": {
     "id": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "prev": "532186bd-d619-49da-be76-e28eb7db691a",
     "regions": {
      "e4a63835-d541-4073-bee5-0017da9a065b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8e459ff-ae34-48aa-8146-50365df9ea53",
        "part": "whole"
       },
       "id": "e4a63835-d541-4073-bee5-0017da9a065b"
      }
     }
    },
    "532186bd-d619-49da-be76-e28eb7db691a": {
     "id": "532186bd-d619-49da-be76-e28eb7db691a",
     "prev": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "regions": {
      "2b7cff32-e764-4b30-8301-0cb4bb2268dc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384",
        "part": "whole"
       },
       "id": "2b7cff32-e764-4b30-8301-0cb4bb2268dc"
      }
     }
    },
    "99fa0d29-85d3-4e57-a540-74b76519f4ac": {
     "id": "99fa0d29-85d3-4e57-a540-74b76519f4ac",
     "prev": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "regions": {
      "afebcb71-18ec-40b4-9e65-8f01e4c70d57": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "038497e7-3cf4-4c64-867c-1bc637cad5e5",
        "part": "whole"
       },
       "id": "afebcb71-18ec-40b4-9e65-8f01e4c70d57"
      }
     }
    },
    "a59429ee-539c-4c02-bfc8-8582111bf455": {
     "id": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "prev": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "regions": {
      "34164544-7832-445e-a820-4ef4dbc522ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a",
        "part": "whole"
       },
       "id": "34164544-7832-445e-a820-4ef4dbc522ac"
      }
     }
    },
    "aa51d16f-a5b4-460f-91fa-569faac9a6fe": {
     "id": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "prev": null,
     "regions": {
      "e46d430e-2c0b-416c-bac3-86a2d27711c1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "560a94b7-6c8e-465f-96fa-fcf2c3859245",
        "part": "whole"
       },
       "id": "e46d430e-2c0b-416c-bac3-86a2d27711c1"
      }
     }
    },
    "ac6a71ca-a1d6-4449-9ee7-87ff28115363": {
     "id": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "prev": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "regions": {
      "ce9295e6-aa18-4ceb-a277-fb19127d89ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67bf390c-9e34-4764-a6de-ab9d3082090b",
        "part": "whole"
       },
       "id": "ce9295e6-aa18-4ceb-a277-fb19127d89ed"
      }
     }
    },
    "baa3a1da-cdb2-4638-8bec-2986349ac603": {
     "id": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "prev": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "regions": {
      "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59c7646c-3944-4e2c-81a1-1728a02396ae",
        "part": "whole"
       },
       "id": "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce"
      }
     }
    },
    "ca7a6c34-27e7-4921-9da2-43a92faf8e8e": {
     "id": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "prev": "395c02de-c982-4ca8-a48e-10a79828812d",
     "regions": {
      "6414ffa6-ea4d-40fe-9d90-5b0816ff0789": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c1fd569-b2c4-423d-a598-1466fc992a49",
        "part": "whole"
       },
       "id": "6414ffa6-ea4d-40fe-9d90-5b0816ff0789"
      }
     }
    },
    "dae9886f-f215-4b89-aefb-8831c7f2ddfc": {
     "id": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "prev": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "regions": {
      "5eeb753c-493e-4bba-b5ab-025339c264f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d80b2206-d953-46b3-b807-2bf5a8557d28",
        "part": "whole"
       },
       "id": "5eeb753c-493e-4bba-b5ab-025339c264f2"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
