{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/derekgreene/topic-model-tutorial \n",
    "* https://github.com/derekgreene/topic-model-tutorial/blob/master/topic-modelling-with-scikitlearn.pdf\n",
    "* https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n",
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d80b2206-d953-46b3-b807-2bf5a8557d28"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    def createcands(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        #print(list(group)[::-1])\n",
    "        reversing = [None]\n",
    "        #for el in group:\n",
    "        #    print(el)\n",
    "        for el in group:\n",
    "            if reversing[0] == None:\n",
    "                reversing[0] = el[0]\n",
    "            else:\n",
    "                reversing.insert(0, el[0])\n",
    "        #print(reversing)\n",
    "        for word in reversing:\n",
    "            if current == '':\n",
    "                current = word\n",
    "            else:\n",
    "                current = word + ' ' + current\n",
    "            total.append(current)\n",
    "        #print(list(set(reversing+total)))\n",
    "        return list(set(reversing+total))\n",
    "    \n",
    "    def createcands2(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        for word, pos, chunk in list(group):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            #print(word)\n",
    "            if current == '':\n",
    "                current = word\n",
    "                total.append(word)\n",
    "            else:\n",
    "                current = current+' '+word\n",
    "                total.append(word)\n",
    "        total.append(current)\n",
    "\n",
    "        return list(set(total))\n",
    "            \n",
    "    #candidates = [' '.join(word for word, pos, chunk in group)\n",
    "    candidates = [createcands2(group)\n",
    "                    for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "    #print(candidates)\n",
    "\n",
    "    #return [cand for cand in candidates]\n",
    "    #print(list(itertools.chain.from_iterable(candidates)))\n",
    "    return list(itertools.chain.from_iterable(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        pattern_A01 = re.compile(r'^hey( |$)')\n",
    "        pattern_B01 = re.compile(r'(january|february| march(,|\\.)? |april|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday|morning|afternoon|evening|mont(s|ly|\\.|,)|year(s|ly|\\.|,)?| day(s|\\.|,)?)')\n",
    "        pattern_C01 = re.compile(r'(chance(s)?|opportunit(y|ies))')\n",
    "        pattern_D01 = re.compile(r'(http?s?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?',re.I)\n",
    "        \n",
    "        ## Text extraction, REGEXs and Replacements\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text() \\\n",
    "                                                        .lower() \\\n",
    "                                                        .replace('’',\"'\") \\\n",
    "                                                        .replace('‘', \"'\") \\\n",
    "                                                        .replace('´', \"'\") \\\n",
    "                                                        .replace('free code camp', 'fcc') \\\n",
    "                                                        .replace('freecodecamp', 'fcc') \\\n",
    "                                                        .replace('javascript', 'js') \\\n",
    "                                                        .replace('part time', 'parttime') \\\n",
    "                                                        .replace('part-time', 'parttime') \\\n",
    "                                                        .replace('full time', 'fulltime') \\\n",
    "                                                        .replace('full-time', 'fulltime') \\\n",
    "                                                        .replace('web application', 'web-app') \\\n",
    "                                                        .replace('web app', 'web-app') \\\n",
    "                                                        .replace('web development', 'dev') \\\n",
    "                                                        .replace('web-development', 'dev') \\\n",
    "                                                        .replace('developer', 'dev') \\\n",
    "                                                        .replace('web dev', 'dev') \\\n",
    "                                                        .replace('dev position', 'dev job') \\\n",
    "                                                        .replace('dev role', 'dev job') \\\n",
    "                                                        .replace('front end', 'frontend') \\\n",
    "                                                        .replace('front-end','frontend') \\\n",
    "                                                        .replace('back end', 'backend') \\\n",
    "                                                        .replace('back-end','backend') \\\n",
    "                                                        .replace('full stack', 'fullstack') \\\n",
    "                                                        .replace('full-stack','fullstack') \\\n",
    "                                                        .replace('frontend job', 'dev job') \\\n",
    "                                                        .replace('frontend position', 'dev job') \\\n",
    "                                                        .replace('frontend role', 'dev job') \\\n",
    "                                                        .replace('frontend web job', 'dev job') \\\n",
    "                                                        .replace('frontend web position', 'dev job') \\\n",
    "                                                        .replace('frontend web role', 'dev job') \\\n",
    "                                                        .replace('frontend web dev job', 'dev job') \\\n",
    "                                                        .replace('frontend web dev position', 'dev job') \\\n",
    "                                                        .replace('frontend web dev role', 'dev job') \\\n",
    "                                                        .replace('resume', 'cv') \\\n",
    "                                                        .replace('angularjs', 'angular') \\\n",
    "                                                        .replace('angular', 'angularjs') \\\n",
    "                                                        .replace('certification', 'cert') \\\n",
    "                                                        .replace('certificate', 'cert') \\\n",
    "                                                        .replace('machine learning', 'machinelearning') \\\n",
    "                                                        .replace('data science', 'datascience') \\\n",
    "                                                        .replace('self learning', 'self-taught') \\\n",
    "                                                        .replace('self learned', 'self-taught') \\\n",
    "                                                        .replace('self-learning', 'self-taught') \\\n",
    "                                                        .replace('self-learned', 'self-taught') \\\n",
    "                                                        .replace('self taught', 'self-taught') \\\n",
    "                                                        .replace('thanks', 'thank') \\\n",
    "                                                        .replace('thankful', 'thank') \\\n",
    "                                                        .replace('gratitude', 'thank') \\\n",
    "                                                        .replace('many thank', 'thank') \\\n",
    "                                                        .replace('much thank', 'thank') \\\n",
    "                                                        .replace('special thank', 'thank') \\\n",
    "                                                        .replace('big thank', 'thank')\n",
    "                                                        #.replace('app', 'web-app') \\\n",
    "                                                        #.replace('web-dev job', 'dev-job') \\\n",
    "                                                        #.replace('web-dev position', 'web-dev-job') \\\n",
    "                                                        #.replace('web-dev role', 'web-dev-job') \\\n",
    "                                                        #.replace('backend job', 'dev job') \\\n",
    "                                                        #.replace('backend position', 'dev job') \\\n",
    "                                                        #.replace('backend role', 'dev job') \\\n",
    "                                                        #.replace('backend web job', 'dev job') \\\n",
    "                                                        #.replace('backend web position', 'dev job') \\\n",
    "                                                        #.replace('backend web role', 'dev job') \\\n",
    "                                                        #.replace('backend web dev job', 'dev job') \\\n",
    "                                                        #.replace('backend web dev position', 'dev job') \\\n",
    "                                                        #.replace('backend web dev role', 'dev job') \\ \n",
    "                                                        #.replace('dev job', 'dev job') \\\n",
    "                                                        \n",
    "        soup_forumpostTEXT = re.sub(pattern_A01, ' hi ', soup_forumpostTEXT)\n",
    "        soup_forumpostTEXT = soup_forumpostTEXT.replace('fellow camper', '') \\\n",
    "                                    .replace('camper', '') \\\n",
    "                                    .replace('fccers', '') \\\n",
    "                                    .replace('fccer', '') \\\n",
    "                                    .replace('everybody', 'everyone') \\\n",
    "                                    .replace('hello', 'hi') \\\n",
    "                                    .replace('hi everyone', 'hi') \\\n",
    "                                    .replace('hi, everyone', 'hi') \\\n",
    "                                    .replace('hi everybody', 'hi') \\\n",
    "                                    .replace('hi, everybody', 'hi')\n",
    "                                    \n",
    "        soup_forumpostTEXT = re.sub(pattern_B01, ' datetimetoken ', soup_forumpostTEXT)\n",
    "        \n",
    "        soup_forumpostTEXT = re.sub(pattern_C01, ' chance ', soup_forumpostTEXT)\n",
    "                \n",
    "        soup_forumpostTEXT = re.sub(pattern_D01, ' thiswasalink ', soup_forumpostTEXT)\n",
    "        \n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':','\\\\'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    #print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    #print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                #print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f8e459ff-ae34-48aa-8146-50365df9ea53"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var4: wordimportance_var3 modified to fit keyphrases; changes in the \"opacity\" formula\n",
    "############################\n",
    "def wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=True):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    selectedgrams = None\n",
    "    if unigrams_test:\n",
    "        selectedgrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                         if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                        ])\n",
    "    else:\n",
    "        selectedgrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                        ])        \n",
    "    print('unigrams',len(selectedgrams))\n",
    "    maxdiv = math.log(sorted(selectedgrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "    print('maxdiv', maxdiv)\n",
    "    opacity = collections.defaultdict(float)\n",
    "    #for grams, counts in lemmws_fd.items(): #grams assumes a phrase is possible\n",
    "    for grams, counts in selectedgrams.items():\n",
    "        if grams == '':\n",
    "            opacity[grams] = 0.0\n",
    "            continue\n",
    "        opval = []\n",
    "        #assert len(grams.split()) == 1, print(grams)\n",
    "        for gram in grams.split():\n",
    "            if gram == '':\n",
    "                continue\n",
    "            if gram in list(selectedgrams.keys()):\n",
    "                #if grams == \"new language framework\":\n",
    "                #    print(gram, math.log(selectedgrams[gram]))\n",
    "                opval.append(math.log(selectedgrams[gram]))\n",
    "            else:\n",
    "                opval.append(0.0)\n",
    "        #assert len(opval) != 0, print('grams',grams)\n",
    "        averopval = sum(opval)/len(opval)\n",
    "        if 1 - averopval/maxdiv != 0.0:\n",
    "            opacity[grams] = 1 - averopval/maxdiv\n",
    "        else:\n",
    "            opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "        #if grams == \"new language framework\":\n",
    "        #    print(grams, opacity[grams])\n",
    "    #print('opval',opval[:10])\n",
    "\n",
    "    #sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "    sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(selectedgrams.keys())])\n",
    "    #assert \"new language framework\" not in list(sizing_matrix.keys())\n",
    "    \n",
    "    ## Count lemmatized words/characters per text  \n",
    "    for i,lemmpos_t in enumerate(lemmposrecs):\n",
    "        for k, lemmpos_sts in enumerate(lemmpos_t[3]):\n",
    "            ## Use lemmatized word\n",
    "            #print(lemmpos_sts)\n",
    "            for tk_TUPLE in lemmpos_sts: \n",
    "                lemmw = tk_TUPLE[2]\n",
    "                if lemmw in list(selectedgrams.keys()):\n",
    "                    sizing_matrix[lemmw][i] = sizing_matrix[lemmw][i] + 1\n",
    "\n",
    "        if not unigrams_test:\n",
    "            #print('not unigrams only')\n",
    "            for cand in lemmpos_t[4]: #<--------------------------------- problem!! it counted candidates only once!!!\n",
    "                if len(cand.split()) > 1:\n",
    "                    #assert cand != \"new language framework\", print(cand)\n",
    "                    for w in cand.split():\n",
    "                        sizing_matrix[cand][i] = sizing_matrix[cand][i] + sizing_matrix[w][i]\n",
    "                    sizing_matrix[cand][i] = sizing_matrix[cand][i]/len(cand.split())\n",
    "                        #if cand == \"new language framework\":\n",
    "                        #    print(sizing_matrix[cand][i], sizing_matrix[w][i])\n",
    "\n",
    "\n",
    "    ## Normalization\n",
    "    normalization = dict([(k, (sum(vector)-max(vector)+1)/sum(vector)) if sum(vector) != 0 else (k,0.0) for k, vector in sizing_matrix.items()])\n",
    "    #assert \"new language framework\" not in list(normalization.keys()), normalization[\"new language framework\"]\n",
    "    #normalization = collections.defaultdict(float)\n",
    "    #for k, vector in sizing_matrix.items():\n",
    "    #    if sum(vector) != 0:\n",
    "    #        normalization[k] = (sum(vector)-max(vector))/sum(vector)\n",
    "    #    else:\n",
    "    #        \n",
    "        \n",
    "    \n",
    "    wordimportance = dict([(k, valnorm*opacity[k]) for k, valnorm in normalization.items()])\n",
    "\n",
    "    return wordimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleaningtext(st, unigrams_test = False):\n",
    "    treated_st = []\n",
    "    if not unigrams_test:\n",
    "        for w in st:\n",
    "            treated_st.append(w)\n",
    "    else:\n",
    "        for w in st:\n",
    "            if len(w.split()) == 1:\n",
    "                treated_st.append(w)\n",
    "    countwds = len(treated_st)\n",
    "    return treated_st, countwds\n",
    "\n",
    "def cleaningtext2(st): #discharge phrases\n",
    "    treated_st = []\n",
    "    countwds = len(st)\n",
    "\n",
    "    return treated_st, countwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "59c7646c-3944-4e2c-81a1-1728a02396ae"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## gensim_models2 re-evaluated: modification of gensim_model2's cleanedsts_from_lemmpostxts to fit keyphrases\n",
    "############################\n",
    "\n",
    "def gensim_models2(lemmposrecs, NUM_TOPICS = 15, lemmws_fd = {}, wordimportance = {}, unigrams_test = False, nltk = nltk, gensim = gensim):\n",
    "    \n",
    "    def cleanedsts_from_lemmpostxts2(lemmposrecs, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "        redo_corpus_by_sts = []\n",
    "        for lemmpos_r in lemmposrecs:\n",
    "            candidates = lemmpos_r[4]\n",
    "            treated_lemmcands = cleaningtext(candidates, unigrams_test=unigrams_test)[0] #passing candidates only\n",
    "            redo_corpus_by_sts.append(treated_lemmcands)\n",
    "            #redo_corpus_by_sts.append(candidates)\n",
    "        \n",
    "        #print(len(redo_corpus_by_sts), redo_corpus_by_sts[-1])\n",
    "        return redo_corpus_by_sts\n",
    "        \n",
    "    def basedonBOW(redo_corpus_by_sts):\n",
    "        dictionary = gensim.corpora.Dictionary(redo_corpus_by_sts) #[token for st in redo_corpus_by_sts for token in st]\n",
    "        corpus = [dictionary.doc2bow(text) for text in redo_corpus_by_sts]\n",
    "        return corpus, dictionary\n",
    "    \n",
    "    def basedonTFIDF(corpus):\n",
    "        return gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    def basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance):\n",
    "        \n",
    "        def metriccalc(w):\n",
    "            if w in wordimportance:\n",
    "                return 1.0+2.0**float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        def metriccalc2(w):\n",
    "            if w in wordimportance:\n",
    "                return float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        corpus = []\n",
    "        for sts in redo_corpus_by_sts:\n",
    "            st = []\n",
    "            for w in sts:\n",
    "                st.append((dictionary.token2id[w], metriccalc2(w)))\n",
    "            corpus.append(st)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    redo_corpus_by_sts  = cleanedsts_from_lemmpostxts2(lemmposrecs)\n",
    "    \n",
    "    corpus, dictionary = basedonBOW(redo_corpus_by_sts)\n",
    "    if wordimportance == {'tfidf':True}:\n",
    "        tfidf = basedonTFIDF(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "    if wordimportance != {} and wordimportance != {'tfidf':True}:\n",
    "        corpus = basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance)\n",
    "    \n",
    "    \n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, minimum_probability=0.005, iterations=100, per_word_topics = True, minimum_phi_value = 0.001, id2word=dictionary)\n",
    "    lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "   \n",
    "    return lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e294b910-1a25-4e48-abb5-37239f441f2e"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d236a59f-ff21-406d-b1df-9436aedbdb11"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of treated posts (len(count)) :: 53\n",
      "lemm errs: 19265\n"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, \n",
    "#                                                                                NUM_TOPICS=20,\n",
    "#                                                                                wordimportance = {'tfidf':True})\n",
    "\n",
    "#lda_model.print_topics(num_words=15)\n",
    "\n",
    "#[' '.join([l for wr in rec[3] for w,_,l,pos in wr]) for rec in lemmposrecs]\n",
    "#[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs]\n",
    "#[' '.join([cand for cand in rec[4]]) for rec in lemmposrecs]\n",
    "##https://stackoverflow.com/questions/46282473/error-while-identify-the-coherence-value-from-lda-model\n",
    "#texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "#texts\n",
    "\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                                                  texts=texts,\n",
    "#                                                                  #corpus=corpus,\n",
    "#                                                                  window_size=20,\n",
    "#                                                                  dictionary=dictionary, \n",
    "#                                                                  coherence='c_uci')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, \n",
    "#                                                   texts=texts, \n",
    "#                                                   dictionary=dictionary,\n",
    "#                                                   coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "## Compute Coherence Score using UMass\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "#                                     corpus = corpus,\n",
    "#                                     dictionary=dictionary, \n",
    "#                                     coherence=\"u_mass\")\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#dictionary\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TOPICS = range(2,61,2)\n",
    "cv_coherence_values = []\n",
    "for numtopics in TOPICS:\n",
    "    print('\\n\\nFor NUM_TOPICS:', numtopics)\n",
    "    lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=numtopics, wordimportance = {'tfidf':True})\n",
    "    umass_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "                                     corpus = corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"u_mass\")\n",
    "    umass_coherence_lda = umass_coherence_model_lda.get_coherence()\n",
    "    print('U-Mass Coherence Score: ', umass_coherence_lda)\n",
    "    texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "    cv_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     texts=texts,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"c_v\")\n",
    "    cv_coherence_lda = cv_coherence_model_lda.get_coherence()\n",
    "    cv_coherence_values.append(cv_coherence_lda)\n",
    "    print('C_V Coherence Score: ', cv_coherence_lda)\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.quora.com/Can-I-combine-LSI-and-K-means-for-text-document-clustering-Are-there-any-sources-to-learn-about-it\n",
    "#http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##https://stackoverflow.com/questions/14261903/how-can-i-open-the-interactive-matplotlib-window-in-ipython-notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "limit=61; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, cv_coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=40, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.print_topics(num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.128*\"project\" + -0.102*\"company\" + -0.100*\"people\" + -0.099*\"interview\" + -0.097*\"lot\" + -0.096*\"code\" + -0.093*\"way\" + -0.090*\"cv\" + -0.087*\"skill\" + -0.086*\"time\" + -0.084*\"app\" + -0.081*\"thiswasalink\" + -0.081*\"experience\" + -0.080*\"good\" + -0.079*\"recruiter\"'),\n",
       " (1,\n",
       "  '-0.210*\"design\" + -0.151*\"graphic\" + 0.122*\"success\" + 0.112*\"backend\" + -0.100*\"time thiswasalink\" + 0.098*\"mern stack\" + 0.098*\"mern\" + 0.094*\"group\" + 0.093*\"github\" + 0.086*\"hard work\" + -0.086*\"lawyer\" + 0.082*\"long\" + -0.081*\"html\" + 0.078*\"friend\" + 0.076*\"source\"'),\n",
       " (2,\n",
       "  '0.141*\"success\" + 0.128*\"story\" + 0.119*\"resource\" + 0.106*\"lawyer\" + 0.101*\"science\" + 0.099*\"group\" + -0.091*\"cv\" + 0.082*\"bright\" + 0.082*\"local code camp armenia\" + 0.082*\"political science\" + 0.082*\"bright datetimetoken\" + 0.082*\"good kind resource\" + 0.082*\"much time learning\" + 0.082*\"armenia\" + 0.082*\"month graduation\"'),\n",
       " (3,\n",
       "  '0.166*\"role\" + -0.126*\"logic\" + -0.114*\"study\" + 0.108*\"current\" + 0.107*\"technology\" + -0.106*\"app\" + -0.092*\"adwords\" + -0.090*\"salary\" + 0.086*\"lawyer\" + 0.083*\"employer\" + -0.082*\"math\" + 0.079*\"science\" + -0.078*\"html\" + 0.077*\"student\" + -0.076*\"css js\"'),\n",
       " (4,\n",
       "  '-0.124*\"internship\" + 0.117*\"day\" + 0.105*\"current\" + 0.101*\"role\" + 0.100*\"app\" + -0.095*\"jquery\" + -0.094*\"fulltime\" + 0.093*\"skill\" + 0.089*\"degree\" + -0.085*\"email\" + -0.084*\"development knowledge gaining experience fulltime job\" + -0.084*\"helpful tip\" + -0.084*\"many many job application\" + -0.084*\"internship couple freelance project\" + -0.084*\"gaining\"'),\n",
       " (5,\n",
       "  '-0.130*\"design\" + -0.124*\"group\" + 0.115*\"dev job\" + -0.110*\"graphic\" + 0.088*\"dream\" + 0.085*\"article\" + -0.084*\"jquery\" + 0.081*\"upwork\" + -0.079*\"day\" + 0.078*\"sleep\" + 0.074*\"freelance\" + -0.071*\"thats\" + 0.070*\"mind\" + 0.069*\"intro\" + 0.069*\"css js\"'),\n",
       " (6,\n",
       "  '0.191*\"day\" + 0.123*\"bar\" + 0.121*\"technology\" + -0.120*\"yesterday\" + -0.111*\"interview call\" + -0.111*\"experience real time project\" + -0.111*\"portfolio employer\" + -0.111*\"dev post yesterday\" + -0.111*\"happy part platform\" + 0.110*\"reply\" + 0.107*\"single\" + 0.106*\"lot interest technology roadmap\" + 0.106*\"cv/cv\" + 0.106*\"roadmap\" + 0.106*\"bachelor software technology\"'),\n",
       " (7,\n",
       "  '-0.129*\"lawyer\" + 0.126*\"qa\" + 0.102*\"quincy\" + 0.094*\"analyst\" + 0.092*\"half\" + 0.086*\"course\" + 0.083*\"time thiswasalink\" + -0.082*\"adwords\" + 0.080*\"fullstack\" + 0.079*\"assignment\" + 0.077*\"fcc project\" + 0.077*\"datetimetoken half\" + -0.076*\"people\" + -0.075*\"medium article\" + -0.072*\"mobile\"'),\n",
       " (8,\n",
       "  '-0.171*\"internship\" + 0.114*\"design\" + -0.103*\"helpful\" + -0.101*\"quincy\" + -0.100*\"thats\" + -0.095*\"lawyer\" + 0.094*\"thank much fcc\" + 0.094*\"previous programming knowledge\" + 0.094*\"larson\" + 0.094*\"journey datetimetoken\" + 0.094*\"first job frontend email dev digital marketing agency\" + 0.094*\"amazing platform community\" + 0.093*\"graphic\" + 0.091*\"digital\" + -0.091*\"many many job application\"'),\n",
       " (9,\n",
       "  '0.167*\"internship\" + 0.113*\"jquery\" + -0.098*\"qa\" + -0.097*\"lawyer\" + 0.093*\"college\" + 0.090*\"remote\" + 0.088*\"intro\" + 0.087*\"udemy\" + -0.083*\"cv\" + 0.083*\"lot\" + 0.082*\"happy\" + 0.076*\"employer\" + 0.074*\"interview call\" + 0.074*\"experience real time project\" + 0.074*\"happy part platform\"'),\n",
       " (10,\n",
       "  '0.111*\"day\" + 0.108*\"knowledge\" + -0.101*\"future\" + 0.095*\"freelance\" + -0.094*\"graduation\" + 0.091*\"study\" + 0.089*\"app\" + 0.082*\"technology\" + -0.076*\"idea\" + 0.076*\"logic\" + -0.073*\"university\" + -0.071*\"udemy\" + -0.071*\"interview process\" + 0.070*\"venezuela\" + 0.069*\"thought\"'),\n",
       " (11,\n",
       "  '0.111*\"design\" + -0.105*\"datetimetoken half\" + -0.105*\"github\" + -0.101*\"process\" + 0.097*\"backend\" + -0.096*\"failure\" + 0.092*\"graphic\" + 0.092*\"upwork\" + -0.081*\"jquery\" + -0.080*\"day\" + -0.074*\"goal\" + 0.070*\"css js\" + -0.070*\"article share story resource\" + -0.070*\"doesnt\" + -0.070*\"community thank\"'),\n",
       " (12,\n",
       "  '0.218*\"lawyer\" + 0.124*\"article\" + 0.111*\"medium article\" + 0.109*\"satisfied current job\" + 0.109*\"satisfied\" + -0.102*\"thats\" + -0.098*\"email\" + 0.089*\"jquery\" + 0.085*\"design\" + -0.084*\"frontend dev\" + -0.080*\"quincy\" + 0.079*\"medium\" + -0.073*\"challenge\" + 0.072*\"school kid\" + 0.072*\"contributor\"'),\n",
       " (13,\n",
       "  '-0.128*\"lawyer\" + -0.102*\"previous programming knowledge\" + -0.102*\"larson\" + -0.102*\"first job frontend email dev digital marketing agency\" + -0.102*\"amazing platform community\" + -0.102*\"journey datetimetoken\" + -0.102*\"thank much fcc\" + 0.100*\"logic\" + 0.092*\"math\" + -0.081*\"journey\" + -0.078*\"pay\" + -0.078*\"medium article\" + -0.074*\"everyone\" + -0.073*\"knowledge\" + -0.073*\"agency\"'),\n",
       " (14,\n",
       "  '-0.223*\"lawyer\" + -0.156*\"venezuela\" + -0.121*\"backend cert\" + -0.121*\"xsl\" + -0.121*\"xml\" + -0.121*\"first period\" + -0.121*\"first frontend dev job\" + -0.121*\"second week new place\" + -0.119*\"call\" + -0.117*\"backend\" + -0.112*\"satisfied current job\" + -0.112*\"satisfied\" + -0.105*\"happy part platform\" + -0.105*\"interview call\" + -0.105*\"dev post yesterday\"'),\n",
       " (15,\n",
       "  '0.134*\"lawyer\" + 0.117*\"logic\" + 0.102*\"math\" + -0.092*\"freelance\" + -0.083*\"day\" + -0.080*\"fulltime\" + 0.076*\"energy\" + 0.075*\"student\" + -0.074*\"venezuela\" + 0.074*\"online\" + 0.072*\"roadmap\" + 0.072*\"bachelor software technology\" + 0.072*\"lot interest technology roadmap\" + 0.072*\"single company\" + 0.072*\"cv/cv\"'),\n",
       " (16,\n",
       "  '0.121*\"half\" + 0.107*\"day\" + 0.104*\"datetimetoken half\" + 0.086*\"app\" + 0.084*\"failure\" + -0.082*\"self-taught\" + -0.082*\"interview frontend dev job today\" + -0.082*\"idea type question\" + -0.082*\"datetimetoken time\" + -0.082*\"self-taught fcc\" + 0.071*\"agency\" + -0.070*\"type\" + 0.069*\"group\" + -0.067*\"skill\" + -0.064*\"share\"'),\n",
       " (17,\n",
       "  '-0.183*\"lawyer\" + 0.139*\"dev post yesterday\" + 0.139*\"experience real time project\" + 0.139*\"happy part platform\" + 0.139*\"interview call\" + 0.139*\"portfolio employer\" + 0.114*\"qa\" + 0.106*\"employer\" + -0.097*\"recruiter\" + -0.092*\"satisfied current job\" + -0.092*\"satisfied\" + 0.086*\"platform\" + 0.085*\"analyst\" + -0.085*\"medium article\" + 0.083*\"assignment\"'),\n",
       " (18,\n",
       "  '-0.236*\"day\" + -0.092*\"role\" + -0.088*\"github\" + -0.083*\"lawyer\" + -0.076*\"employer\" + -0.074*\"portfolio employer\" + -0.074*\"interview call\" + -0.074*\"happy part platform\" + -0.074*\"dev post yesterday\" + -0.074*\"experience real time project\" + 0.073*\"reply\" + 0.070*\"single company\" + 0.070*\"roadmap\" + 0.070*\"lot interest technology roadmap\" + 0.070*\"cv/cv\"'),\n",
       " (19,\n",
       "  '-0.144*\"datetimetoken time\" + -0.144*\"idea type question\" + -0.144*\"interview frontend dev job today\" + -0.144*\"self-taught fcc\" + -0.143*\"start\" + -0.137*\"css js\" + -0.089*\"thats\" + -0.088*\"xml\" + -0.088*\"first frontend dev job\" + -0.088*\"first period\" + -0.088*\"second week new place\" + -0.088*\"backend cert\" + -0.088*\"xsl\" + -0.086*\"self-taught\" + 0.086*\"app\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"lawyer\" + 0.002*\"satisfied current job\" + 0.002*\"satisfied\" + 0.002*\"medium article\" + 0.002*\"github\" + 0.002*\"little bit code free time\" + 0.002*\"internship couple freelance project\" + 0.002*\"helpful tip\" + 0.002*\"gaining\" + 0.002*\"many many job application\" + 0.002*\"whoo\" + 0.002*\"share others\" + 0.002*\"development knowledge gaining experience fulltime job\" + 0.001*\"fulltime commitment\" + 0.001*\"look\" + 0.001*\"medium\" + 0.001*\"share\" + 0.001*\"helpful\" + 0.001*\"current\" + 0.001*\"source\"'),\n",
       " (1,\n",
       "  '0.001*\"python\" + 0.001*\"coder\" + 0.001*\"peer\" + 0.001*\"scraping\" + 0.001*\"clojure\" + 0.001*\"head\" + 0.001*\"team\" + 0.001*\"data\" + 0.001*\"web\" + 0.001*\"language\" + 0.001*\"half\" + 0.001*\"friend\" + 0.001*\"many friend\" + 0.001*\"head team\" + 0.001*\"middle 4th week\" + 0.001*\"dark time\" + 0.001*\"basis\" + 0.001*\"head department\" + 0.001*\"middle\" + 0.001*\"monkey-work\"'),\n",
       " (2,\n",
       "  '0.002*\"time thiswasalink\" + 0.001*\"drunk\" + 0.001*\"intermediate\" + 0.001*\"graphic design datetimetoken\" + 0.001*\"subject\" + 0.001*\"bar\" + 0.001*\"firm\" + 0.001*\"design\" + 0.001*\"entire\" + 0.001*\"graphic\" + 0.001*\"sass\" + 0.001*\"bootstrap\" + 0.001*\"resource\" + 0.001*\"everything\" + 0.001*\"sale\" + 0.001*\"jquery\" + 0.001*\"lot\" + 0.001*\"today\" + 0.001*\"goal\" + 0.001*\"point\"'),\n",
       " (3,\n",
       "  '0.001*\"recruiter\" + 0.001*\"medior\" + 0.001*\"essential\" + 0.001*\"hell\" + 0.001*\"call\" + 0.001*\"level\" + 0.001*\"lesson\" + 0.001*\"car\" + 0.001*\"scope\" + 0.001*\"super\" + 0.001*\"pressure\" + 0.001*\"async\" + 0.001*\"company\" + 0.001*\"second\" + 0.001*\"backend\" + 0.001*\"functionality\" + 0.001*\"database\" + 0.001*\"es6\" + 0.001*\"moment\" + 0.001*\"lack\"'),\n",
       " (4,\n",
       "  '0.002*\"venezuela\" + 0.002*\"freelance\" + 0.001*\"post fcc solid path\" + 0.001*\"call guy\" + 0.001*\"first dev job age\" + 0.001*\"food home\" + 0.001*\"freelance job week\" + 0.001*\"frontend backend cert\" + 0.001*\"freelance summer\" + 0.001*\"local dev job\" + 0.001*\"night hour spent reading coding\" + 0.001*\"js technology deep way\" + 0.001*\"obligation\" + 0.001*\"deep\" + 0.001*\"food\" + 0.001*\"hell\" + 0.001*\"spent\" + 0.001*\"interview next datetimetoken\" + 0.001*\"fcc datetimetoken\" + 0.001*\"age\"'),\n",
       " (5,\n",
       "  '0.001*\"junior\" + 0.001*\"london\" + 0.001*\"role\" + 0.001*\"building\" + 0.001*\"ability\" + 0.001*\"decision\" + 0.001*\"progression\" + 0.001*\"final interview\" + 0.001*\"informed\" + 0.001*\"box\" + 0.001*\"take\" + 0.001*\"final\" + 0.001*\"progression chance\" + 0.001*\"factor\" + 0.001*\"access\" + 0.001*\"senior\" + 0.001*\"etc\" + 0.001*\"skill\" + 0.001*\"sale\" + 0.001*\"method\"'),\n",
       " (6,\n",
       "  '0.002*\"success\" + 0.002*\"upwork\" + 0.001*\"test\" + 0.001*\"communication\" + 0.001*\"cv\" + 0.001*\"story\" + 0.001*\"way\" + 0.001*\"candidate\" + 0.001*\"client\" + 0.001*\"good\" + 0.001*\"cover\" + 0.001*\"frontend cert along udemy course\" + 0.001*\"job fulltime dev\" + 0.001*\"na\" + 0.001*\"proud\" + 0.001*\"friend local fcc group everyone\" + 0.001*\"success story fcc forum\" + 0.001*\"thank success story\" + 0.001*\"post long time\" + 0.001*\"additional library\"'),\n",
       " (7,\n",
       "  '0.002*\"design\" + 0.002*\"thank much fcc\" + 0.002*\"amazing platform community\" + 0.002*\"first job frontend email dev digital marketing agency\" + 0.002*\"previous programming knowledge\" + 0.002*\"journey datetimetoken\" + 0.002*\"larson\" + 0.002*\"interview process\" + 0.002*\"marketing\" + 0.002*\"email\" + 0.001*\"graphic\" + 0.001*\"digital\" + 0.001*\"previous\" + 0.001*\"agency\" + 0.001*\"intro\" + 0.001*\"programming\" + 0.001*\"web-apps hybrid mobile apps\" + 0.001*\"insurmountable task\" + 0.001*\"politics university\" + 0.001*\"lucky optimistic future\"'),\n",
       " (8,\n",
       "  '0.004*\"day\" + 0.002*\"experience real time project\" + 0.002*\"happy part platform\" + 0.002*\"dev post yesterday\" + 0.002*\"portfolio employer\" + 0.002*\"interview call\" + 0.002*\"datetimetoken time\" + 0.002*\"interview frontend dev job today\" + 0.002*\"idea type question\" + 0.002*\"self-taught fcc\" + 0.002*\"technology\" + 0.002*\"roadmap\" + 0.002*\"single company\" + 0.002*\"student dev\" + 0.002*\"bachelor software technology\" + 0.002*\"lot interest technology roadmap\" + 0.002*\"cv/cv\" + 0.002*\"css js\" + 0.002*\"start\" + 0.002*\"self-taught\"'),\n",
       " (9,\n",
       "  '0.000*\"driver\" + 0.000*\"email address\" + 0.000*\"essential\" + 0.000*\"email address manager\" + 0.000*\"dynamic\" + 0.000*\"desk hr rep\" + 0.000*\"dreaded white board question\" + 0.000*\"dreaded\" + 0.000*\"dozen post company\" + 0.000*\"downright lie\" + 0.000*\"downright\" + 0.000*\"dom\" + 0.000*\"direction\" + 0.000*\"difficult cs question senior devs get\" + 0.000*\"devs high demand\" + 0.000*\"development community full people\" + 0.000*\"dev rate” “everyone\" + 0.000*\"dynamic web-app project\" + 0.000*\"error\" + 0.000*\"elusive first dev job\"'),\n",
       " (10,\n",
       "  '0.001*\"sleep\" + 0.001*\"response\" + 0.001*\"dream\" + 0.001*\"mind\" + 0.001*\"fullstack\" + 0.001*\"single response company\" + 0.001*\"“a-ha” moment\" + 0.001*\"goal mind\" + 0.001*\"fullstack project\" + 0.001*\"frontend portion\" + 0.001*\"first hurdle beginning stage\" + 0.001*\"dungeon crawler game life\" + 0.001*\"anthropology\" + 0.001*\"crawler\" + 0.001*\"“a-ha”\" + 0.001*\"amazing position frontend dev\" + 0.001*\"hurdle\" + 0.001*\"huge shout-out\" + 0.001*\"degree anthropology\" + 0.001*\"speed-bump\"'),\n",
       " (11,\n",
       "  '0.002*\"thats\" + 0.002*\"group\" + 0.002*\"energy\" + 0.002*\"role\" + 0.002*\"prototype\" + 0.001*\"people\" + 0.001*\"frontend dev\" + 0.001*\"challenge\" + 0.001*\"room\" + 0.001*\"data\" + 0.001*\"cert\" + 0.001*\"right\" + 0.001*\"json\" + 0.001*\"udacity\" + 0.001*\"infrastructure\" + 0.001*\"daughter\" + 0.001*\"good thing\" + 0.001*\"portal\" + 0.001*\"code\" + 0.001*\"recruiter\"'),\n",
       " (12,\n",
       "  '0.002*\"adwords\" + 0.001*\"object\" + 0.001*\"schedule\" + 0.001*\"much\" + 0.001*\"script\" + 0.001*\"app\" + 0.001*\"salary\" + 0.001*\"idea\" + 0.001*\"lol\" + 0.001*\"boss\" + 0.001*\"comment\" + 0.001*\"leader\" + 0.001*\"home\" + 0.001*\"backend\" + 0.001*\"area\" + 0.001*\"right\" + 0.001*\"study\" + 0.001*\"agency\" + 0.001*\"first\" + 0.001*\"first job\"'),\n",
       " (13,\n",
       "  '0.002*\"datetimetoken half\" + 0.002*\"math\" + 0.002*\"part learning process\" + 0.002*\"contributor\" + 0.002*\"topic doesnt belong\" + 0.002*\"doesnt\" + 0.002*\"community thank\" + 0.002*\"belong\" + 0.002*\"article share story resource\" + 0.002*\"school kid\" + 0.002*\"logic\" + 0.002*\"half\" + 0.002*\"topic\" + 0.001*\"failure\" + 0.001*\"recruitment\" + 0.001*\"article\" + 0.001*\"trainer\" + 0.001*\"advance logic\" + 0.001*\"advance\" + 0.001*\"tribute\"'),\n",
       " (14,\n",
       "  '0.001*\"degree\" + 0.001*\"thing lot\" + 0.001*\"university\" + 0.001*\"current\" + 0.001*\"self-taught\" + 0.001*\"country\" + 0.001*\"lot\" + 0.001*\"without\" + 0.001*\"science\" + 0.001*\"field\" + 0.001*\"nope\" + 0.001*\"lot application\" + 0.001*\"munch \\'real world\" + 0.001*\"munch\" + 0.001*\"mentality\" + 0.001*\"mechanical engineer course history datetimetoken computer science\" + 0.001*\"lot personal project github codepen\" + 0.001*\"payment \\'plus\" + 0.001*\"nowdays\" + 0.001*\"kind mentality internet place\"'),\n",
       " (15,\n",
       "  '0.002*\"internship\" + 0.001*\"udemy\" + 0.001*\"remote\" + 0.001*\"exercise fcc\" + 0.001*\"thiswasalink note\" + 0.001*\"stable job future\" + 0.001*\"shantanu\" + 0.001*\"js jquery udemy course\" + 0.001*\"stable\" + 0.001*\"steele\" + 0.001*\"remote internship position thiswasalink\" + 0.001*\"c++ python job\" + 0.001*\"internship offer\" + 0.001*\"interviewed\" + 0.001*\"software dev\" + 0.001*\"dev job interviewed\" + 0.001*\"colt steele udemy\" + 0.001*\"colt\" + 0.001*\"remote dev\" + 0.001*\"note\"'),\n",
       " (16,\n",
       "  '0.002*\"jquery\" + 0.001*\"exercise\" + 0.001*\"project\" + 0.001*\"texas\" + 0.001*\"problem\" + 0.001*\"tracker\" + 0.001*\"college degree\" + 0.001*\"code\" + 0.001*\"chance\" + 0.001*\"college\" + 0.001*\"skill\" + 0.001*\"salary\" + 0.001*\"knowledge\" + 0.001*\"similar\" + 0.001*\"graduate\" + 0.001*\"webmaster\" + 0.001*\"fresh\" + 0.001*\"english\" + 0.001*\"universe\" + 0.001*\"moment\"'),\n",
       " (17,\n",
       "  '0.002*\"bright\" + 0.002*\"local code camp armenia\" + 0.002*\"much time learning\" + 0.002*\"dev offer\" + 0.002*\"codacademy\" + 0.002*\"armenia\" + 0.002*\"good kind resource\" + 0.002*\"month graduation\" + 0.002*\"political science\" + 0.002*\"bright datetimetoken\" + 0.001*\"graduation\" + 0.001*\"political\" + 0.001*\"camp\" + 0.001*\"profession\" + 0.001*\"new job\" + 0.001*\"science\" + 0.001*\"success\" + 0.001*\"month\" + 0.001*\"kind\" + 0.001*\"local\"'),\n",
       " (18,\n",
       "  '0.002*\"qa\" + 0.002*\"backend cert\" + 0.002*\"first frontend dev job\" + 0.002*\"second week new place\" + 0.002*\"xml\" + 0.002*\"first period\" + 0.002*\"xsl\" + 0.002*\"analyst\" + 0.002*\"quincy\" + 0.001*\"assignment\" + 0.001*\"period\" + 0.001*\"fullstack dev job offer\" + 0.001*\"organisation\" + 0.001*\"place\" + 0.001*\"fullstack\" + 0.001*\"second\" + 0.001*\"heart\" + 0.001*\"offer\" + 0.001*\"backend\" + 0.001*\"mean\"'),\n",
       " (19,\n",
       "  '0.002*\"closure\" + 0.001*\"js”\" + 0.001*\"beta\" + 0.001*\"function\" + 0.001*\"section\" + 0.001*\"git\" + 0.001*\"book\" + 0.001*\"something\" + 0.001*\"important point\" + 0.001*\"5-digit\" + 0.001*\"beta challenge\" + 0.001*\"wrong code\" + 0.001*\"js” series\" + 0.001*\"linkedin github\" + 0.001*\"8-hours\" + 0.001*\"“eloquent js”\" + 0.001*\"“eloquent\" + 0.001*\"asynchronous\" + 0.001*\"junior dev asap\" + 0.001*\"interview life\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dictionary.id2token[2298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "udacity 1927 0.00156796\n",
      "json 1863 0.00156796\n",
      "designer 454 0.00111441\n",
      "linkedin 1362 0.00111441\n",
      "recruiter 256 0.00106218\n",
      "lead 1867 0.00104634\n",
      "cosmopolitan 1828 0.00104634\n",
      "tree 1922 0.00104634\n",
      "church 1823 0.00104634\n",
      "json tree 1865 0.00104634\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "dom 2486 0.000268025\n",
      "direction 2485 0.000268025\n",
      "dreaded 2490 0.000268025\n",
      "dozen post company 2489 0.000268025\n",
      "downright lie 2488 0.000268025\n",
      "downright 2487 0.000268025\n",
      "driver 2492 0.000268025\n",
      "dev rate” “everyone 2481 0.000268025\n",
      "devs high demand 2483 0.000268025\n",
      "dynamic web-app project 2494 0.000268025\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "dom 2486 0.000268025\n",
      "direction 2485 0.000268025\n",
      "dreaded 2490 0.000268025\n",
      "dozen post company 2489 0.000268025\n",
      "downright lie 2488 0.000268025\n",
      "downright 2487 0.000268025\n",
      "driver 2492 0.000268025\n",
      "dev rate” “everyone 2481 0.000268025\n",
      "devs high demand 2483 0.000268025\n",
      "dynamic web-app project 2494 0.000268025\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "css js 534 0.00236145\n",
      "self-taught fcc 538 0.002157\n",
      "datetimetoken time 535 0.002157\n",
      "idea type question 536 0.002157\n",
      "interview frontend dev job today 537 0.002157\n",
      "thats 2291 0.00208225\n",
      "start 539 0.0016168\n",
      "self-taught 510 0.0016168\n",
      "agency 1 0.00151275\n",
      "idea 152 0.001512\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "article share story resource 1790 0.00193401\n",
      "school kid 1796 0.00193401\n",
      "part learning process 1795 0.00193401\n",
      "doesnt 1794 0.00193401\n",
      "contributor 1793 0.00193401\n",
      "community thank 1792 0.00193401\n",
      "topic doesnt belong 1797 0.00193401\n",
      "belong 1791 0.00193401\n",
      "jquery 666 0.00192926\n",
      "closure 3135 0.00168955\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "dom 2486 0.000268025\n",
      "direction 2485 0.000268025\n",
      "dreaded 2490 0.000268025\n",
      "dozen post company 2489 0.000268025\n",
      "downright lie 2488 0.000268025\n",
      "downright 2487 0.000268025\n",
      "driver 2492 0.000268025\n",
      "dev rate” “everyone 2481 0.000268025\n",
      "devs high demand 2483 0.000268025\n",
      "dynamic web-app project 2494 0.000268025\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "amazing platform community 1484 0.00248419\n",
      "larson 1487 0.00248419\n",
      "thank much fcc 1489 0.00248419\n",
      "previous programming knowledge 1488 0.00248419\n",
      "journey datetimetoken 1486 0.00248419\n",
      "first job frontend email dev digital marketing agency 1485 0.00248419\n",
      "marketing 314 0.00170788\n",
      "previous 499 0.00158292\n",
      "digital 282 0.00158292\n",
      "amazing 346 0.0013945\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "day 93 0.00419668\n",
      "github 102 0.00266286\n",
      "portfolio employer 1702 0.00216728\n",
      "experience real time project 1699 0.00216728\n",
      "interview call 1701 0.00216728\n",
      "happy part platform 1700 0.00216728\n",
      "dev post yesterday 1698 0.00216728\n",
      "venezuela 2054 0.00215719\n",
      "employer 849 0.00185942\n",
      "spent 2053 0.00173628\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "internship 1020 0.00207036\n",
      "energy 543 0.00195547\n",
      "people 248 0.0017965\n",
      "remote 1004 0.00155672\n",
      "passionate 1197 0.00151835\n",
      "udemy 339 0.00134288\n",
      "luck 231 0.00131078\n",
      "weekly 2178 0.00130786\n",
      "event 2128 0.00130786\n",
      "knowledge 221 0.00128437\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "datetimetoken half 2015 0.0021777\n",
      "half 26 0.00197026\n",
      "failure 2020 0.00183724\n",
      "fullstack dev job offer 982 0.00166642\n",
      "organisation 994 0.00166642\n",
      "python 1025 0.00130369\n",
      "stack 1007 0.00121122\n",
      "clojure 1716 0.00120817\n",
      "coder 1717 0.00120817\n",
      "peer 1768 0.00120817\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "#https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 2839\n",
      "maxdiv 7.181591944611865\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = wordimportance, unigrams_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.273*\"company\" + 0.221*\"interview\" + 0.213*\"project\" + 0.205*\"job\" + 0.186*\"time\" + 0.143*\"people\" + 0.141*\"datetimetoken\" + 0.133*\"thing\" + 0.128*\"experience\" + 0.125*\"code\" + 0.122*\"cv\" + 0.122*\"way\" + 0.115*\"lot\" + 0.109*\"many\" + 0.106*\"month\" + 0.105*\"dev\" + 0.101*\"question\" + 0.098*\"something\" + 0.095*\"application\" + 0.095*\"challenge\"'),\n",
       " (1,\n",
       "  '0.154*\"datetimetoken\" + 0.153*\"fcc\" + 0.149*\"job\" + 0.145*\"thiswasalink\" + 0.135*\"recruiter\" + 0.130*\"week\" + 0.114*\"backend\" + 0.111*\"project\" + 0.110*\"end\" + -0.099*\"cv\" + -0.095*\"thing\" + 0.091*\"call\" + 0.087*\"guy\" + 0.082*\"story\" + -0.082*\"github\" + -0.081*\"code\" + 0.075*\"level\" + 0.075*\"salary\" + 0.075*\"interview\" + -0.075*\"problem\"'),\n",
       " (2,\n",
       "  '-0.225*\"dev\" + -0.215*\"job\" + 0.180*\"company\" + -0.171*\"fcc\" + -0.156*\"skill\" + -0.140*\"frontend\" + -0.139*\"lot\" + 0.109*\"call\" + 0.107*\"recruiter\" + -0.105*\"chance\" + -0.105*\"good\" + -0.102*\"course\" + -0.101*\"thank\" + -0.088*\"resource\" + -0.085*\"couple\" + 0.083*\"level\" + -0.081*\"professional\" + 0.078*\"moment\" + 0.075*\"next\" + -0.075*\"css\"'),\n",
       " (3,\n",
       "  '0.173*\"dev\" + -0.155*\"junior\" + -0.117*\"role\" + -0.101*\"couple\" + -0.101*\"chance\" + -0.098*\"senior\" + -0.095*\"skill\" + -0.095*\"decision\" + -0.094*\"etc\" + -0.091*\"sale\" + -0.086*\"tech\" + -0.085*\"technology\" + -0.083*\"ability\" + -0.081*\"message\" + -0.081*\"building\" + -0.080*\"professional\" + -0.080*\"technical\" + -0.080*\"way\" + -0.078*\"life\" + -0.077*\"employer\"'),\n",
       " (4,\n",
       "  '-0.155*\"lot\" + -0.151*\"fcc\" + 0.123*\"email\" + 0.101*\"interview\" + -0.095*\"backend\" + -0.094*\"degree\" + 0.092*\"week\" + 0.088*\"learning\" + 0.081*\"salary\" + -0.081*\"coding\" + -0.080*\"dream\" + -0.079*\"fullstack\" + -0.079*\"startup\" + -0.078*\"field\" + -0.076*\"city\" + 0.076*\"parttime\" + 0.075*\"rate\" + 0.073*\"environment\" + 0.072*\"gig\" + -0.072*\"pay\"'),\n",
       " (5,\n",
       "  '-0.149*\"fcc\" + 0.136*\"something\" + 0.115*\"designer\" + 0.113*\"someone\" + 0.112*\"time\" + 0.112*\"thiswasalink\" + 0.107*\"fact\" + 0.107*\"js\" + 0.102*\"tip\" + 0.101*\"udacity\" + 0.100*\"lead\" + 0.100*\"portfolio\" + 0.099*\"json\" + 0.099*\"track\" + 0.098*\"try\" + 0.096*\"life\" + 0.093*\"linkedin\" + 0.093*\"top\" + -0.091*\"job\" + 0.091*\"basic\"'),\n",
       " (6,\n",
       "  '0.198*\"course\" + -0.179*\"skill\" + 0.170*\"degree\" + 0.136*\"chance\" + 0.135*\"datetimetoken\" + 0.135*\"current\" + 0.132*\"lot\" + -0.128*\"good\" + 0.124*\"science\" + 0.118*\"programming\" + -0.117*\"fcc\" + -0.111*\"backend\" + 0.109*\"college\" + -0.107*\"startup\" + -0.105*\"city\" + -0.100*\"pay\" + 0.096*\"computer\" + 0.083*\"resource\" + -0.083*\"fullstack\" + -0.082*\"coding\"'),\n",
       " (7,\n",
       "  '-0.191*\"lot\" + 0.176*\"dev\" + 0.176*\"datetimetoken\" + -0.151*\"degree\" + -0.124*\"country\" + -0.113*\"skill\" + -0.107*\"university\" + 0.103*\"js\" + -0.098*\"backend\" + 0.097*\"community\" + 0.096*\"html\" + -0.094*\"current\" + -0.093*\"people\" + -0.093*\"field\" + 0.089*\"hi\" + -0.087*\"statistic\" + -0.085*\"experience\" + 0.085*\"stuff\" + -0.085*\"pay\" + 0.084*\"first\"'),\n",
       " (8,\n",
       "  '0.155*\"language\" + -0.135*\"project\" + 0.134*\"skill\" + 0.134*\"web\" + -0.130*\"phone\" + -0.118*\"dev\" + 0.113*\"python\" + 0.111*\"design\" + -0.108*\"thing\" + -0.107*\"communication\" + -0.098*\"way\" + -0.096*\"online\" + 0.096*\"programming\" + 0.095*\"website\" + -0.094*\"world\" + -0.093*\"class\" + -0.092*\"detail\" + 0.092*\"js\" + 0.089*\"basic\" + 0.087*\"part\"'),\n",
       " (9,\n",
       "  '0.180*\"team\" + 0.171*\"week\" + -0.154*\"design\" + 0.131*\"python\" + 0.130*\"web\" + -0.126*\"position\" + -0.116*\"basic\" + -0.111*\"month\" + 0.108*\"international\" + 0.107*\"computer\" + 0.107*\"interest\" + -0.101*\"new\" + 0.097*\"head\" + 0.088*\"language\" + -0.087*\"graphic\" + 0.082*\"active\" + 0.082*\"monkey-work\" + 0.082*\"basis\" + 0.082*\"beaten-up\" + 0.082*\"begging\"'),\n",
       " (10,\n",
       "  '0.168*\"dev\" + -0.162*\"path\" + 0.132*\"confidence\" + -0.127*\"datetimetoken\" + 0.127*\"salary\" + -0.106*\"time\" + -0.105*\"project\" + -0.097*\"career\" + 0.096*\"skill\" + 0.096*\"html\" + 0.095*\"job\" + -0.093*\"much\" + -0.092*\"good\" + 0.091*\"graduate\" + 0.087*\"line\" + 0.087*\"article\" + -0.085*\"test\" + 0.085*\"fresh\" + -0.084*\"local\" + -0.082*\"process\"'),\n",
       " (11,\n",
       "  '-0.190*\"js\" + -0.144*\"project\" + 0.134*\"job\" + -0.131*\"path\" + 0.131*\"learning\" + 0.111*\"fulltime\" + -0.103*\"career\" + -0.099*\"foundation\" + -0.095*\"much\" + -0.094*\"right\" + -0.092*\"employee\" + -0.090*\"object\" + -0.090*\"additional\" + 0.088*\"role\" + 0.086*\"bachelor\" + -0.084*\"strong\" + -0.083*\"salary\" + -0.083*\"place\" + -0.083*\"html\" + -0.082*\"codepen\"'),\n",
       " (12,\n",
       "  '-0.162*\"design\" + 0.147*\"dev\" + 0.145*\"resource\" + 0.128*\"app\" + 0.128*\"knowledge\" + 0.119*\"college\" + 0.118*\"study\" + -0.111*\"fcc\" + 0.106*\"goal\" + -0.101*\"js\" + -0.099*\"position\" + 0.097*\"thank\" + -0.095*\"part\" + 0.094*\"time\" + 0.092*\"process\" + -0.087*\"tutorial\" + -0.087*\"graphic\" + 0.082*\"thought\" + 0.076*\"passionate\" + -0.074*\"phone\"'),\n",
       " (13,\n",
       "  '0.163*\"design\" + 0.139*\"support\" + -0.132*\"community\" + -0.127*\"datetimetoken\" + 0.119*\"basic\" + 0.113*\"dev\" + 0.109*\"graphic\" + 0.100*\"language\" + -0.096*\"book\" + -0.096*\"fcc\" + 0.095*\"environment\" + -0.094*\"heart\" + -0.093*\"interview\" + 0.092*\"role\" + -0.091*\"beta\" + -0.089*\"function\" + 0.089*\"month\" + -0.088*\"something\" + 0.085*\"engineering\" + -0.085*\"request\"'),\n",
       " (14,\n",
       "  '-0.141*\"client\" + 0.136*\"fcc\" + 0.129*\"project\" + -0.129*\"profile\" + -0.122*\"fulltime\" + 0.121*\"datetimetoken\" + -0.111*\"experience\" + -0.107*\"idea\" + 0.106*\"skill\" + -0.101*\"summer\" + -0.100*\"takeaway\" + -0.098*\"coding\" + -0.098*\"entry\" + -0.096*\"upwork\" + -0.095*\"living\" + -0.094*\"couple\" + 0.093*\"networking\" + -0.092*\"everything\" + -0.092*\"view\" + 0.092*\"people\"'),\n",
       " (15,\n",
       "  '0.142*\"interview\" + -0.131*\"app\" + 0.128*\"career\" + 0.126*\"datetimetoken\" + -0.123*\"study\" + 0.114*\"employer\" + 0.112*\"couple\" + 0.104*\"passion\" + -0.100*\"thought\" + 0.099*\"fcc\" + -0.094*\"website\" + 0.093*\"path\" + -0.093*\"week\" + -0.092*\"journey\" + 0.091*\"learning\" + 0.090*\"php\" + 0.085*\"science\" + 0.084*\"confidence\" + 0.083*\"in-person\" + -0.082*\"knowledge\"'),\n",
       " (16,\n",
       "  '0.159*\"datetimetoken\" + 0.121*\"course\" + 0.118*\"test\" + -0.115*\"project\" + 0.114*\"css\" + -0.114*\"people\" + 0.111*\"thiswasalink\" + 0.095*\"position\" + -0.093*\"goal\" + 0.090*\"html\" + -0.089*\"something\" + -0.089*\"strong\" + -0.088*\"night\" + -0.087*\"session\" + -0.084*\"request\" + -0.084*\"platform\" + -0.084*\"fcc\" + -0.082*\"networking\" + -0.082*\"mobile\" + -0.082*\"skype\"'),\n",
       " (17,\n",
       "  '0.161*\"frontend\" + 0.143*\"way\" + -0.120*\"class\" + -0.110*\"process\" + 0.110*\"science\" + -0.105*\"lot\" + -0.100*\"entire\" + -0.099*\"passionate\" + 0.097*\"role\" + 0.092*\"course\" + -0.091*\"test\" + -0.091*\"luck\" + 0.087*\"client\" + 0.085*\"datetimetoken\" + 0.084*\"gig\" + -0.084*\"people\" + 0.083*\"profile\" + -0.082*\"career\" + -0.081*\"stuff\" + 0.081*\"forum\"'),\n",
       " (18,\n",
       "  '0.151*\"app\" + 0.134*\"people\" + 0.122*\"skill\" + -0.102*\"goal\" + 0.099*\"website\" + 0.099*\"profile\" + 0.097*\"luck\" + 0.095*\"mobile\" + -0.094*\"coding\" + -0.093*\"dev\" + -0.093*\"story\" + -0.092*\"advice\" + -0.091*\"resource\" + 0.090*\"weekly\" + 0.087*\"lot\" + 0.085*\"summer\" + -0.085*\"education\" + 0.083*\"regular\" + 0.082*\"couple\" + -0.080*\"fact\"'),\n",
       " (19,\n",
       "  '-0.163*\"college\" + -0.161*\"role\" + -0.156*\"programming\" + 0.152*\"course\" + -0.131*\"class\" + 0.115*\"frontend\" + -0.105*\"question\" + -0.104*\"current\" + -0.103*\"employer\" + -0.101*\"support\" + 0.099*\"position\" + -0.096*\"aspect\" + -0.087*\"cert\" + 0.085*\"everything\" + -0.084*\"challenge\" + -0.084*\"situation\" + -0.082*\"engineering\" + -0.078*\"process\" + -0.077*\"dead\" + -0.077*\"specialist\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"exact\" + 0.001*\"interactivity\" + 0.001*\"devs/mentors\" + 0.001*\"emphasie\" + 0.001*\"bootcamps\" + 0.001*\"chapter\" + 0.001*\"most/all\" + 0.001*\"photography\" + 0.001*\"hunger\" + 0.001*\"individual\" + 0.001*\"grunt\" + 0.001*\"block\" + 0.001*\"contention\" + 0.001*\"ajax\" + 0.001*\"rough\" + 0.001*\"spurt\" + 0.001*\"codewars\" + 0.001*\"none\" + 0.001*\"skills/technologies\" + 0.001*\"door\"'),\n",
       " (1,\n",
       "  '0.007*\"article\" + 0.006*\"learning\" + 0.006*\"community\" + 0.006*\"process\" + 0.005*\"real-life\" + 0.005*\"science-y\" + 0.005*\"viewpoint\" + 0.005*\"contributor\" + 0.005*\"practising\" + 0.005*\"dedication\" + 0.005*\"criterion\" + 0.005*\"card\" + 0.005*\"alternative\" + 0.005*\"belong\" + 0.005*\"share\" + 0.005*\"medium\" + 0.005*\"story\" + 0.005*\"platform\" + 0.005*\"canada\" + 0.005*\"knew\"'),\n",
       " (2,\n",
       "  '0.006*\"platform\" + 0.006*\"yesterday\" + 0.005*\"part\" + 0.005*\"post\" + 0.005*\"happy\" + 0.005*\"portfolio\" + 0.004*\"employer\" + 0.004*\"real\" + 0.004*\"thank\" + 0.004*\"call\" + 0.003*\"experience\" + 0.003*\"dev\" + 0.003*\"time\" + 0.003*\"project\" + 0.003*\"interview\" + 0.003*\"datetimetoken\" + 0.002*\"job\" + 0.001*\"onwards\" + 0.001*\"programming\" + 0.001*\"link\"'),\n",
       " (3,\n",
       "  '0.006*\"previous\" + 0.006*\"larson\" + 0.005*\"platform\" + 0.005*\"community\" + 0.005*\"knowledge\" + 0.005*\"marketing\" + 0.005*\"digital\" + 0.004*\"journey\" + 0.004*\"hi\" + 0.004*\"thank\" + 0.004*\"programming\" + 0.004*\"amazing\" + 0.003*\"agency\" + 0.003*\"advice\" + 0.003*\"frontend\" + 0.003*\"dev\" + 0.003*\"email\" + 0.003*\"luck\" + 0.003*\"much\" + 0.003*\"first\"'),\n",
       " (4,\n",
       "  '0.001*\"token\" + 0.001*\"tweet\" + 0.001*\"service\" + 0.001*\"driver\" + 0.001*\"rock\" + 0.001*\"hacker\" + 0.001*\"me”\" + 0.001*\"flashy\" + 0.001*\"starting\" + 0.001*\"unicorn\" + 0.001*\"browser\" + 0.001*\"addition\" + 0.001*\"imposter\" + 0.001*\"flag\" + 0.001*\"gpa\" + 0.001*\"10-20\" + 0.001*\"evidence\" + 0.001*\"speech\" + 0.001*\"skillset\" + 0.001*\"cost\"'),\n",
       " (5,\n",
       "  '0.007*\"forum\" + 0.007*\"anything\" + 0.007*\"everyone\" + 0.007*\"life\" + 0.006*\"advice\" + 0.005*\"hi\" + 0.005*\"science\" + 0.005*\"old\" + 0.005*\"huge\" + 0.005*\"cert\" + 0.005*\"computer\" + 0.005*\"curriculum\" + 0.005*\"juice\" + 0.005*\"nyc\" + 0.005*\"rise\" + 0.005*\"dead\" + 0.005*\"threw\" + 0.005*\"determination\" + 0.005*\"devops\" + 0.005*\"notice\"'),\n",
       " (6,\n",
       "  '0.004*\"amount\" + 0.003*\"start-up\" + 0.003*\"fair\" + 0.003*\"dove\" + 0.003*\"benefit\" + 0.003*\"htmlcssjs\" + 0.003*\"mosque\" + 0.003*\"mcginnis\" + 0.003*\"sister\" + 0.003*\"map\" + 0.003*\"mentor\" + 0.003*\"website\" + 0.003*\"debt\" + 0.003*\"locaton\" + 0.003*\"whole\" + 0.003*\"forum\" + 0.003*\"hole\" + 0.003*\"craigs\" + 0.003*\"humilty\" + 0.003*\"anybody\"'),\n",
       " (7,\n",
       "  '0.004*\"js\" + 0.004*\"hi\" + 0.004*\"type\" + 0.004*\"thank\" + 0.004*\"learning\" + 0.004*\"basic\" + 0.004*\"position\" + 0.004*\"css\" + 0.003*\"php\" + 0.003*\"course\" + 0.003*\"quincylarson\" + 0.003*\"community\" + 0.003*\"next\" + 0.003*\"tutorial\" + 0.003*\"bachelor\" + 0.003*\"new\" + 0.003*\"fcc\" + 0.003*\"thing\" + 0.003*\"datetimetoken\" + 0.003*\"dev\"'),\n",
       " (8,\n",
       "  '0.003*\"legendary\" + 0.003*\"working\" + 0.003*\"public\" + 0.003*\"alphago\" + 0.002*\"lee\" + 0.002*\"extensive\" + 0.002*\"mine\" + 0.002*\"office\" + 0.002*\"designing\" + 0.002*\"relief\" + 0.002*\"rwd\" + 0.002*\"anywho\" + 0.002*\"typical\" + 0.002*\"journey\" + 0.002*\"holiday\" + 0.002*\"employment\" + 0.002*\"self-teaching\" + 0.002*\"progressive\" + 0.002*\"all.6\" + 0.002*\"production\"'),\n",
       " (9,\n",
       "  '0.006*\"markdown\" + 0.006*\"certicate\" + 0.005*\"netflix\" + 0.005*\"html\" + 0.005*\"conversion\" + 0.005*\"optimization\" + 0.005*\"react+redux\" + 0.005*\"log\" + 0.005*\"clone\" + 0.005*\"recipe+box\" + 0.005*\"momentum\" + 0.005*\"scss\" + 0.005*\"60-75\" + 0.005*\"everyday\" + 0.005*\"es5\" + 0.005*\"timeline\" + 0.005*\"html/css\" + 0.005*\"css\" + 0.005*\"initial\" + 0.005*\"book\"'),\n",
       " (10,\n",
       "  '0.002*\"euphoria\" + 0.002*\"adequate\" + 0.002*\"extent\" + 0.002*\"field\\\\attribute\" + 0.002*\"allright\" + 0.002*\"testing\" + 0.002*\"finished-\" + 0.002*\"negotations\" + 0.002*\"spaghetti\" + 0.002*\"irl\" + 0.002*\"consultancy\" + 0.002*\"nature\" + 0.002*\"graph\" + 0.002*\"-to\" + 0.002*\"-let\" + 0.002*\"newbie\" + 0.002*\"d*ck\" + 0.002*\"cup\" + 0.002*\"chatroom\" + 0.002*\"activity\"'),\n",
       " (11,\n",
       "  '0.006*\"career\" + 0.005*\"hi\" + 0.005*\"computer\" + 0.005*\"week\" + 0.005*\"someone\" + 0.005*\"website\" + 0.005*\"dev\" + 0.004*\"stuck\" + 0.004*\"future\" + 0.004*\"site\" + 0.004*\"thiswasalink\" + 0.004*\"month\" + 0.004*\"js\" + 0.004*\"weekend\" + 0.003*\"stuff\" + 0.003*\"note\" + 0.003*\"frontend\" + 0.003*\"thing\" + 0.003*\"commitment\" + 0.003*\"idea\"'),\n",
       " (12,\n",
       "  '0.007*\"month\" + 0.007*\"thank\" + 0.006*\"frontend\" + 0.006*\"dev\" + 0.006*\"course\" + 0.005*\"hi\" + 0.005*\"fcc\" + 0.005*\"home\" + 0.005*\"story\" + 0.005*\"hour\" + 0.005*\"curriculum\" + 0.005*\"solid\" + 0.004*\"guy\" + 0.004*\"js\" + 0.004*\"coding\" + 0.004*\"cert\" + 0.004*\"video\" + 0.004*\"journey\" + 0.004*\"test\" + 0.004*\"good\"'),\n",
       " (13,\n",
       "  '0.006*\"whoo\" + 0.006*\"gaining\" + 0.005*\"commitment\" + 0.005*\"bit\" + 0.004*\"helpful\" + 0.004*\"advice\" + 0.004*\"knowledge\" + 0.004*\"tip\" + 0.004*\"freelance\" + 0.004*\"free\" + 0.004*\"hi\" + 0.004*\"development\" + 0.004*\"share\" + 0.003*\"little\" + 0.003*\"couple\" + 0.003*\"fulltime\" + 0.003*\"internship\" + 0.003*\"many\" + 0.003*\"thank\" + 0.003*\"application\"'),\n",
       " (14,\n",
       "  '0.005*\"website\" + 0.005*\"someone\" + 0.004*\"awsome\" + 0.004*\"chellanges\" + 0.004*\"aproach\" + 0.004*\"edx\" + 0.004*\"increadible\" + 0.004*\"cs50\" + 0.004*\"week\" + 0.004*\"important\" + 0.004*\"setting\" + 0.004*\"first\" + 0.004*\"cert\" + 0.003*\"app\" + 0.003*\"writing\" + 0.003*\"professional\" + 0.003*\"helpful\" + 0.003*\"shop\" + 0.003*\"session\" + 0.003*\"documentation\"'),\n",
       " (15,\n",
       "  '0.005*\"entry\" + 0.004*\"website\" + 0.004*\"js\" + 0.004*\"issue\" + 0.003*\"deliberation\" + 0.003*\"hi\" + 0.003*\"programming\" + 0.003*\"great\" + 0.003*\"standard\" + 0.003*\"frontend\" + 0.003*\"beaten-up\" + 0.003*\"idea\" + 0.003*\"thiswasalink\" + 0.003*\"community\" + 0.003*\"book\" + 0.003*\"utility\" + 0.003*\"example\" + 0.003*\"hey\" + 0.003*\"begging\" + 0.003*\"many\"'),\n",
       " (16,\n",
       "  '0.005*\"new\" + 0.005*\"week\" + 0.005*\"stuff\" + 0.004*\"source\" + 0.004*\"xsl\" + 0.004*\"xml\" + 0.004*\"open\" + 0.004*\"long\" + 0.004*\"project\" + 0.004*\"league\" + 0.004*\"last\" + 0.004*\"frontend\" + 0.004*\"month\" + 0.004*\"freedom\" + 0.004*\"hours/week\" + 0.003*\"builder\" + 0.003*\"training\" + 0.003*\"lot\" + 0.003*\"instant\" + 0.003*\"site\"'),\n",
       " (17,\n",
       "  '0.006*\"last\" + 0.006*\"california\" + 0.005*\"part\" + 0.005*\"state\" + 0.005*\"takeaway\" + 0.005*\"option\" + 0.005*\"world\" + 0.005*\"history\" + 0.005*\"couple\" + 0.004*\"school\" + 0.004*\"frustration\" + 0.004*\"degree\" + 0.004*\"frontend\" + 0.004*\"summer\" + 0.004*\"message\" + 0.004*\"fcc”\" + 0.004*\"“how\" + 0.004*\"posting\" + 0.004*\"united\" + 0.004*\"dev\"'),\n",
       " (18,\n",
       "  '0.005*\"political\" + 0.005*\"codacademy\" + 0.005*\"bright\" + 0.005*\"armenia\" + 0.005*\"hi\" + 0.005*\"learning\" + 0.004*\"graduation\" + 0.004*\"camp\" + 0.004*\"profession\" + 0.004*\"local\" + 0.004*\"thank\" + 0.004*\"week\" + 0.003*\"new\" + 0.003*\"5-digit\" + 0.003*\"month\" + 0.003*\"kind\" + 0.003*\"till\" + 0.003*\"someone\" + 0.003*\"resource\" + 0.003*\"success\"'),\n",
       " (19,\n",
       "  '0.005*\"course\" + 0.005*\"dev\" + 0.005*\"knowledge\" + 0.004*\"application\" + 0.004*\"thank\" + 0.004*\"beginning\" + 0.004*\"lot\" + 0.004*\"something\" + 0.003*\"frontend\" + 0.003*\"chance\" + 0.003*\"time\" + 0.003*\"good\" + 0.003*\"first\" + 0.003*\"last\" + 0.003*\"project\" + 0.003*\"community\" + 0.003*\"month\" + 0.003*\"phone\" + 0.003*\"job\" + 0.003*\"position\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "exact 678 0.0012689\n",
      "interactivity 699 0.00124173\n",
      "devs/mentors 673 0.00122161\n",
      "emphasie 677 0.0012064\n",
      "bootcamps 650 0.00120453\n",
      "chapter 656 0.00119339\n",
      "most/all 709 0.00119164\n",
      "photography 725 0.00113323\n",
      "hunger 694 0.00113074\n",
      "individual 697 0.00110055\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "article 567 0.00717332\n",
      "learning 161 0.00602833\n",
      "community 8 0.00585102\n",
      "process 121 0.00553421\n",
      "real-life 808 0.00521003\n",
      "practising 805 0.00521003\n",
      "dedication 800 0.00521003\n",
      "science-y 811 0.00521003\n",
      "alternative 795 0.00521003\n",
      "criterion 799 0.00521003\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "platform 503 0.00629675\n",
      "yesterday 57 0.005684\n",
      "part 343 0.00543\n",
      "post 42 0.00470469\n",
      "happy 913 0.00464304\n",
      "portfolio 41 0.00457745\n",
      "employer 537 0.00407528\n",
      "real 347 0.00407528\n",
      "thank 50 0.00402742\n",
      "call 253 0.00400378\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "previous 346 0.00587751\n",
      "larson 861 0.00575803\n",
      "platform 503 0.00524366\n",
      "community 8 0.00515565\n",
      "knowledge 159 0.00510033\n",
      "marketing 222 0.00488712\n",
      "digital 204 0.00453709\n",
      "journey 274 0.00421998\n",
      "hi 24 0.0041598\n",
      "thank 50 0.00405376\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "token 1460 0.00105222\n",
      "tweet 1467 0.00103662\n",
      "service 1435 0.00102296\n",
      "driver 1312 0.00100875\n",
      "rock 1425 0.00100123\n",
      "hacker 1344 0.000977958\n",
      "me” 1374 0.000952998\n",
      "flashy 1331 0.000950838\n",
      "starting 1444 0.000943053\n",
      "unicorn 1468 0.00093864\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "forum 19 0.00708002\n",
      "anything 3 0.00708002\n",
      "everyone 15 0.00685707\n",
      "life 33 0.00669158\n",
      "advice 0 0.00563892\n",
      "hi 24 0.00541571\n",
      "science 455 0.00520516\n",
      "old 406 0.00511814\n",
      "huge 26 0.00508872\n",
      "cert 6 0.00506473\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "amount 999 0.00355092\n",
      "start-up 417 0.00334912\n",
      "fair 482 0.0032835\n",
      "dove 1016 0.00325706\n",
      "benefit 1002 0.00325108\n",
      "htmlcssjs 1022 0.00306571\n",
      "mosque 1038 0.00306559\n",
      "mcginnis 1037 0.00302319\n",
      "sister 1050 0.0030206\n",
      "map 1036 0.00301179\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "js 31 0.00419534\n",
      "hi 24 0.00418997\n",
      "type 359 0.00416143\n",
      "thank 50 0.00398489\n",
      "learning 161 0.00393439\n",
      "basic 252 0.00392163\n",
      "position 181 0.00390807\n",
      "css 68 0.00352173\n",
      "php 345 0.00343734\n",
      "course 142 0.00338002\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "legendary 884 0.00270561\n",
      "working 908 0.00269533\n",
      "public 897 0.00266855\n",
      "alphago 866 0.00257633\n",
      "lee 883 0.00246304\n",
      "extensive 877 0.00232915\n",
      "mine 890 0.0022589\n",
      "office 787 0.00222767\n",
      "designing 875 0.00222286\n",
      "relief 899 0.0021783\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "markdown 83 0.00582954\n",
      "certicate 63 0.0055097\n",
      "netflix 85 0.00531704\n",
      "html 77 0.0053169\n",
      "conversion 67 0.0052562\n",
      "optimization 87 0.00515698\n",
      "react+redux 90 0.00514374\n",
      "log 82 0.00512571\n",
      "clone 65 0.00509673\n",
      "recipe+box 91 0.00509114\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 4712\n",
      "maxdiv 7.181591944611865\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = wordimportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.271*\"company\" + 0.219*\"interview\" + 0.212*\"project\" + 0.203*\"job\" + 0.185*\"time\" + 0.141*\"people\" + 0.139*\"datetimetoken\" + 0.132*\"thing\" + 0.127*\"experience\" + 0.124*\"code\" + 0.121*\"cv\" + 0.121*\"way\" + 0.114*\"lot\" + 0.108*\"many\" + 0.105*\"month\" + 0.103*\"dev\" + 0.100*\"question\" + 0.097*\"something\" + 0.094*\"application\" + 0.094*\"challenge\"'),\n",
       " (1,\n",
       "  '-0.149*\"datetimetoken\" + -0.144*\"fcc\" + -0.141*\"job\" + -0.140*\"thiswasalink\" + -0.133*\"recruiter\" + -0.126*\"week\" + -0.111*\"backend\" + -0.108*\"end\" + -0.108*\"project\" + 0.095*\"cv\" + 0.092*\"thing\" + -0.091*\"call\" + -0.084*\"guy\" + -0.080*\"story\" + 0.079*\"github\" + 0.078*\"code\" + -0.075*\"level\" + -0.075*\"interview\" + -0.073*\"salary\" + 0.072*\"problem\"'),\n",
       " (2,\n",
       "  '0.216*\"dev\" + 0.212*\"job\" + -0.172*\"company\" + 0.164*\"fcc\" + 0.151*\"skill\" + 0.137*\"frontend\" + 0.131*\"lot\" + -0.102*\"call\" + 0.101*\"chance\" + 0.099*\"good\" + -0.099*\"recruiter\" + 0.098*\"thank\" + 0.097*\"course\" + 0.086*\"resource\" + 0.084*\"couple\" + 0.078*\"professional\" + -0.077*\"level\" + 0.073*\"month\" + -0.072*\"moment\" + 0.072*\"css\"'),\n",
       " (3,\n",
       "  '0.140*\"dev\" + -0.137*\"junior\" + -0.101*\"role\" + -0.100*\"chance\" + -0.098*\"skill\" + -0.085*\"couple\" + -0.084*\"senior\" + -0.083*\"decision\" + -0.081*\"etc\" + -0.080*\"sale\" + -0.078*\"tech\" + -0.078*\"professional\" + 0.078*\"thing\" + 0.078*\"salary\" + 0.075*\"month\" + -0.075*\"life\" + -0.074*\"technology\" + -0.074*\"project\" + -0.073*\"ability\" + -0.071*\"way\"'),\n",
       " (4,\n",
       "  '-0.152*\"fcc\" + -0.149*\"lot\" + 0.109*\"interview\" + 0.107*\"email\" + -0.097*\"dev\" + -0.092*\"degree\" + -0.079*\"backend\" + 0.074*\"rate\" + -0.074*\"field\" + -0.073*\"coding\" + -0.073*\"dream\" + -0.071*\"startup\" + 0.070*\"learning\" + 0.070*\"command\" + -0.069*\"fullstack\" + 0.069*\"week\" + -0.069*\"city\" + 0.067*\"junior\" + 0.067*\"couple\" + -0.065*\"curriculum\"'),\n",
       " (5,\n",
       "  '-0.145*\"fcc\" + 0.119*\"something\" + 0.109*\"designer\" + 0.108*\"someone\" + 0.105*\"time\" + 0.104*\"thiswasalink\" + 0.102*\"tip\" + 0.099*\"fact\" + 0.096*\"lead\" + 0.096*\"json\" + -0.095*\"lot\" + 0.095*\"track\" + 0.095*\"try\" + 0.095*\"udacity\" + 0.094*\"portfolio\" + -0.090*\"job\" + 0.088*\"linkedin\" + 0.087*\"js\" + 0.085*\"top\" + 0.084*\"area\"'),\n",
       " (6,\n",
       "  '0.197*\"degree\" + 0.172*\"course\" + 0.156*\"lot\" + 0.148*\"current\" + -0.137*\"fcc\" + -0.136*\"skill\" + 0.132*\"chance\" + 0.125*\"science\" + -0.109*\"good\" + 0.108*\"college\" + -0.107*\"dev\" + 0.097*\"university\" + 0.097*\"computer\" + -0.093*\"startup\" + -0.093*\"city\" + 0.089*\"programming\" + 0.088*\"resource\" + -0.086*\"backend\" + 0.086*\"life\" + -0.079*\"coding\"'),\n",
       " (7,\n",
       "  '0.194*\"datetimetoken\" + -0.152*\"lot\" + -0.149*\"skill\" + 0.137*\"dev\" + -0.126*\"backend\" + -0.112*\"country\" + 0.111*\"js\" + -0.106*\"pay\" + 0.106*\"community\" + 0.100*\"hi\" + -0.099*\"good\" + -0.094*\"everyone\" + -0.093*\"degree\" + 0.093*\"html\" + -0.089*\"experience\" + 0.088*\"language\" + -0.087*\"fullstack\" + -0.085*\"field\" + -0.082*\"dream\" + -0.081*\"university\"'),\n",
       " (8,\n",
       "  '0.140*\"language\" + -0.134*\"dev\" + -0.132*\"project\" + -0.127*\"phone\" + 0.125*\"web\" + 0.124*\"skill\" + 0.111*\"python\" + -0.103*\"communication\" + -0.102*\"thing\" + 0.096*\"design\" + -0.094*\"way\" + 0.092*\"team\" + -0.091*\"online\" + 0.091*\"programming\" + 0.090*\"website\" + -0.088*\"class\" + -0.088*\"detail\" + -0.087*\"world\" + 0.087*\"friend\" + 0.086*\"part\"'),\n",
       " (9,\n",
       "  '0.172*\"team\" + 0.159*\"week\" + -0.135*\"design\" + 0.131*\"python\" + -0.110*\"basic\" + 0.110*\"web\" + -0.108*\"position\" + 0.104*\"interest\" + -0.101*\"new\" + 0.099*\"international\" + 0.099*\"computer\" + 0.095*\"head\" + -0.089*\"month\" + -0.086*\"experience\" + 0.082*\"language\" + 0.081*\"bit\" + 0.080*\"data\" + 0.080*\"first\" + -0.080*\"challenge\" + 0.079*\"dud\"'),\n",
       " (10,\n",
       "  '-0.160*\"path\" + 0.127*\"dev\" + -0.118*\"project\" + 0.113*\"salary\" + 0.113*\"confidence\" + -0.113*\"datetimetoken\" + -0.100*\"much\" + -0.100*\"time\" + -0.095*\"career\" + 0.092*\"job\" + -0.089*\"process\" + 0.089*\"html\" + 0.086*\"web\" + -0.082*\"test\" + 0.082*\"useful\" + 0.081*\"line\" + -0.081*\"good\" + 0.081*\"graduate\" + -0.081*\"month\" + 0.078*\"book\"'),\n",
       " (11,\n",
       "  '-0.187*\"js\" + 0.129*\"resource\" + -0.123*\"something\" + -0.123*\"project\" + 0.119*\"job\" + -0.103*\"path\" + 0.100*\"knowledge\" + 0.097*\"learning\" + -0.096*\"codepen\" + 0.093*\"goal\" + 0.091*\"bachelor\" + -0.086*\"git\" + 0.084*\"app\" + 0.081*\"education\" + 0.079*\"dev\" + 0.076*\"background\" + -0.072*\"thiswasalink\" + -0.072*\"fcc\" + 0.071*\"passionate\" + 0.071*\"role\"'),\n",
       " (12,\n",
       "  '-0.179*\"dev\" + -0.103*\"confidence\" + -0.102*\"salary\" + -0.093*\"foundation\" + -0.089*\"js\" + 0.084*\"fcc\" + -0.084*\"thank\" + -0.083*\"article\" + -0.080*\"much\" + 0.078*\"community\" + 0.077*\"coding\" + -0.077*\"wife\" + 0.076*\"part\" + -0.076*\"graduate\" + -0.074*\"study\" + -0.074*\"career\" + 0.071*\"hour\" + 0.071*\"datetimetoken\" + -0.071*\"fresh\" + -0.071*\"strong\"'),\n",
       " (13,\n",
       "  '0.232*\"design\" + 0.155*\"support\" + 0.145*\"graphic\" + 0.118*\"basic\" + 0.113*\"tutorial\" + 0.108*\"language\" + 0.106*\"role\" + -0.104*\"community\" + 0.096*\"engineering\" + 0.090*\"environment\" + 0.089*\"fulltime\" + 0.083*\"europe\" + 0.083*\"mozilla\" + 0.083*\"mix\" + 0.083*\"diy-ing\" + 0.083*\"offer/trainee\" + 0.083*\"seo\" + 0.083*\"commercial\" + 0.083*\"duration\" + 0.083*\"arises\"'),\n",
       " (14,\n",
       "  '-0.160*\"fcc\" + -0.131*\"project\" + -0.130*\"datetimetoken\" + 0.129*\"fulltime\" + 0.121*\"client\" + 0.109*\"profile\" + 0.104*\"everything\" + 0.099*\"coding\" + 0.098*\"experience\" + -0.097*\"networking\" + -0.095*\"people\" + -0.094*\"skill\" + 0.090*\"idea\" + 0.089*\"entry\" + 0.088*\"learning\" + -0.087*\"skype\" + 0.084*\"summer\" + 0.082*\"upwork\" + 0.081*\"dev\" + 0.079*\"living\"'),\n",
       " (15,\n",
       "  '-0.134*\"app\" + -0.127*\"study\" + 0.117*\"interview\" + -0.106*\"thought\" + 0.102*\"datetimetoken\" + 0.100*\"fcc\" + 0.097*\"couple\" + 0.095*\"learning\" + 0.094*\"science\" + 0.094*\"career\" + 0.092*\"employer\" + -0.090*\"website\" + 0.086*\"passion\" + -0.084*\"basic\" + -0.080*\"knowledge\" + -0.079*\"week\" + 0.079*\"confidence\" + -0.078*\"journey\" + -0.078*\"coursera\" + 0.072*\"path\"'),\n",
       " (16,\n",
       "  '0.190*\"datetimetoken\" + 0.119*\"course\" + -0.113*\"people\" + 0.111*\"test\" + 0.103*\"css\" + -0.100*\"project\" + 0.097*\"thiswasalink\" + -0.093*\"night\" + -0.093*\"session\" + -0.090*\"goal\" + -0.087*\"platform\" + 0.085*\"html\" + -0.084*\"fact\" + -0.082*\"something\" + 0.082*\"position\" + -0.080*\"strong\" + 0.080*\"interview\" + -0.079*\"mobile\" + 0.078*\"last\" + -0.077*\"everything\"'),\n",
       " (17,\n",
       "  '-0.146*\"way\" + -0.116*\"role\" + -0.112*\"frontend\" + 0.105*\"lot\" + 0.104*\"test\" + -0.103*\"science\" + 0.100*\"luck\" + 0.099*\"class\" + 0.099*\"people\" + 0.092*\"entire\" + 0.091*\"process\" + 0.087*\"career\" + -0.082*\"current\" + 0.080*\"learner\" + 0.077*\"passionate\" + 0.076*\"interview\" + 0.074*\"intro\" + 0.074*\"php\" + 0.072*\"local\" + -0.072*\"forum\"'),\n",
       " (18,\n",
       "  '0.138*\"app\" + 0.130*\"profile\" + 0.106*\"client\" + 0.101*\"regular\" + 0.098*\"summer\" + -0.095*\"goal\" + 0.095*\"upwork\" + 0.092*\"view\" + 0.092*\"guy\" + 0.091*\"gig\" + -0.088*\"story\" + 0.088*\"people\" + 0.086*\"professional\" + 0.085*\"website\" + 0.082*\"endpoint\" + 0.082*\"copywriter\" + 0.082*\"enthusiasm\" + 0.082*\"contractor\" + 0.082*\"eye-opening\" + 0.082*\"introduction\"'),\n",
       " (19,\n",
       "  '-0.170*\"role\" + -0.148*\"programming\" + -0.132*\"college\" + -0.119*\"current\" + -0.105*\"class\" + 0.098*\"course\" + 0.096*\"person\" + -0.096*\"people\" + -0.094*\"employer\" + -0.089*\"background\" + -0.086*\"aspect\" + -0.083*\"engineering\" + -0.083*\"support\" + -0.082*\"windows/linux\" + -0.082*\"specialist\" + -0.082*\"scripting\" + -0.082*\"dead horse\" + -0.082*\"mixed\" + -0.082*\"determination\" + -0.082*\"distributed\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"obligation\" + 0.003*\"satisfied\" + 0.003*\"dev\" + 0.003*\"home\" + 0.003*\"age\" + 0.003*\"food\" + 0.003*\"deep\" + 0.003*\"spent\" + 0.003*\"fcc\" + 0.003*\"reading\" + 0.003*\"coding\" + 0.003*\"night\" + 0.003*\"frontend\" + 0.003*\"solid\" + 0.003*\"js\" + 0.002*\"hi\" + 0.002*\"food home\" + 0.002*\"medium\" + 0.002*\"medium article\" + 0.002*\"college\"'),\n",
       " (1,\n",
       "  '0.002*\"stable\" + 0.002*\"steele\" + 0.002*\"interviewed\" + 0.002*\"note\" + 0.002*\"js\" + 0.001*\"css\" + 0.001*\"career\" + 0.001*\"colt\" + 0.001*\"shantanu\" + 0.001*\"position\" + 0.001*\"simple portfolio website\" + 0.001*\"future\" + 0.001*\"website\" + 0.001*\"course\" + 0.001*\"frontend\" + 0.001*\"udemy\" + 0.001*\"hi\" + 0.001*\"thiswasalink\" + 0.001*\"exercise\" + 0.001*\"development\"'),\n",
       " (2,\n",
       "  '0.004*\"mern\" + 0.004*\"holiday\" + 0.004*\"proud\" + 0.003*\"additional library\" + 0.003*\"mern stack\" + 0.003*\"udemy\" + 0.003*\"forum\" + 0.003*\"cert\" + 0.003*\"journey\" + 0.003*\"library\" + 0.003*\"post\" + 0.003*\"local\" + 0.003*\"along\" + 0.003*\"additional\" + 0.003*\"na\" + 0.003*\"frontend cert along udemy course\" + 0.003*\"long\" + 0.003*\"everyone\" + 0.002*\"hi\" + 0.002*\"success\"'),\n",
       " (3,\n",
       "  '0.003*\"takeaway\" + 0.003*\"curriculum\" + 0.003*\"invitation\" + 0.003*\"last\" + 0.003*\"kid\" + 0.002*\"entire\" + 0.002*\"teacher\" + 0.002*\"position\" + 0.002*\"js\" + 0.002*\"bunch\" + 0.002*\"local\" + 0.002*\"part\" + 0.002*\"fast\" + 0.002*\"stuff\" + 0.002*\"passion\" + 0.002*\"shot\" + 0.002*\"couple\" + 0.002*\"mail\" + 0.002*\"side\" + 0.002*\"hook\"'),\n",
       " (4,\n",
       "  '0.002*\"course\" + 0.002*\"js\" + 0.002*\"codepen\" + 0.002*\"life\" + 0.002*\"takeaway\" + 0.002*\"position\" + 0.002*\"udemy\" + 0.002*\"\\'real\" + 0.002*\"cert\" + 0.002*\"calculus\" + 0.002*\"university\" + 0.002*\"pragmatic\" + 0.002*\"thank\" + 0.002*\"princeton\" + 0.002*\"something\" + 0.002*\"uneducated\" + 0.002*\"workaround\" + 0.002*\"german\" + 0.002*\"web\" + 0.002*\"programming\"'),\n",
       " (5,\n",
       "  '0.002*\"template\" + 0.002*\"optimisation…\" + 0.001*\"seo\" + 0.001*\"beginner\" + 0.001*\"chance\" + 0.001*\"mentor\" + 0.001*\"foreign\" + 0.001*\"position\" + 0.001*\"diy-ing\" + 0.001*\"arises\" + 0.001*\"paid\" + 0.001*\"living\" + 0.001*\"new\" + 0.001*\"mozilla\" + 0.001*\"industry\" + 0.001*\"weird\" + 0.001*\"type paid apprenticeship\" + 0.001*\"motivational\" + 0.001*\"sale\" + 0.001*\"seo optimisation…\"'),\n",
       " (6,\n",
       "  '0.004*\"hi\" + 0.003*\"old\" + 0.003*\"advice\" + 0.003*\"life\" + 0.003*\"little\" + 0.003*\"background\" + 0.003*\"challenge\" + 0.003*\"bachelor\" + 0.003*\"gaining\" + 0.003*\"united\" + 0.003*\"posting\" + 0.003*\"whoo\" + 0.003*\"fcc”\" + 0.003*\"“how\" + 0.003*\"aws/azure\" + 0.003*\"horse\" + 0.003*\"dead horse\" + 0.003*\"powershell/bash\" + 0.003*\"scripting\" + 0.003*\"enterprise\"'),\n",
       " (7,\n",
       "  '0.003*\"basketball\" + 0.003*\"hands-on\" + 0.003*\"purchaser\" + 0.003*\"economics\" + 0.002*\"supply chain\" + 0.002*\"lucas\" + 0.002*\"supply\" + 0.002*\"direction\" + 0.002*\"road\" + 0.002*\"tutorials…\" + 0.002*\"pleasure\" + 0.002*\"retailer belgium\" + 0.002*\"hobby\" + 0.002*\"finger\" + 0.002*\"belgium\" + 0.002*\"it-consultancy\" + 0.002*\"chain\" + 0.002*\"retailer\" + 0.002*\"trough\" + 0.002*\"scratch\"'),\n",
       " (8,\n",
       "  '0.004*\"test\" + 0.003*\"btter\" + 0.003*\"9-10\" + 0.003*\"optimism\" + 0.003*\"requested\" + 0.003*\"webdev\" + 0.003*\"competent\" + 0.003*\"vuejs\" + 0.003*\"domain\" + 0.003*\"title\" + 0.003*\"structured\" + 0.003*\"api\" + 0.003*\"someone\" + 0.003*\"eye\" + 0.003*\"js\" + 0.003*\"thread\" + 0.003*\"aware\" + 0.003*\"motivation\" + 0.003*\"impression\" + 0.003*\"brazil\"'),\n",
       " (9,\n",
       "  '0.004*\"hi\" + 0.004*\"community\" + 0.003*\"dev\" + 0.003*\"amazing\" + 0.003*\"cert\" + 0.003*\"fcc\" + 0.003*\"board\" + 0.003*\"week\" + 0.003*\"single\" + 0.003*\"frontend\" + 0.003*\"first\" + 0.003*\"stuff\" + 0.003*\"class\" + 0.003*\"programming\" + 0.003*\"agency\" + 0.002*\"time\" + 0.002*\"datetimetoken\" + 0.002*\"question\" + 0.002*\"tech\" + 0.002*\"huge\"'),\n",
       " (10,\n",
       "  '0.004*\"platform\" + 0.004*\"phone\" + 0.003*\"development\" + 0.003*\"last\" + 0.003*\"good\" + 0.003*\"community\" + 0.003*\"lot\" + 0.003*\"gig\" + 0.003*\"frontend\" + 0.003*\"agency\" + 0.003*\"summer\" + 0.003*\"knowledge\" + 0.003*\"field\" + 0.003*\"thing\" + 0.003*\"learning\" + 0.002*\"fulltime\" + 0.002*\"many\" + 0.002*\"something\" + 0.002*\"new\" + 0.002*\"response\"'),\n",
       " (11,\n",
       "  '0.004*\"hour\" + 0.003*\"self\" + 0.003*\"big\" + 0.003*\"test\" + 0.002*\"nurse\" + 0.002*\"css\" + 0.002*\"month\" + 0.002*\"thiswasalink\" + 0.002*\"position\" + 0.002*\"course\" + 0.002*\"codepen\" + 0.002*\"thank\" + 0.002*\"limit\" + 0.002*\"stuff\" + 0.002*\"fcc\" + 0.002*\"first\" + 0.002*\"system\" + 0.002*\"one\" + 0.002*\"\\'yes\" + 0.002*\"progressing\"'),\n",
       " (12,\n",
       "  '0.001*\"ad\" + 0.001*\"experienced\" + 0.001*\"solid\" + 0.001*\"quality\" + 0.001*\"wasted\" + 0.001*\"ziprecruiter\" + 0.001*\"angellist\" + 0.001*\"ratio\" + 0.001*\"daily\" + 0.001*\"follow\" + 0.001*\"end\" + 0.001*\"mid-level\" + 0.001*\"apis…\" + 0.001*\"coworkers\" + 0.001*\"amongst\" + 0.001*\"meeting…\" + 0.001*\"bio\" + 0.001*\"situation\" + 0.001*\"brief bio\" + 0.001*\"architect\"'),\n",
       " (13,\n",
       "  '0.004*\"“to\" + 0.004*\"cv/cv\" + 0.004*\"teach.”\" + 0.004*\"solar\" + 0.004*\"sector\" + 0.004*\"outcome\" + 0.004*\"factory\" + 0.004*\"organization\" + 0.004*\"roadmap\" + 0.004*\"layer\" + 0.004*\"frontend/fullstack\" + 0.004*\"mdn\" + 0.004*\"cause\" + 0.004*\"gitter\" + 0.004*\"room\" + 0.003*\"single\" + 0.003*\"many layer organization\" + 0.003*\"reply\" + 0.003*\"beginning\" + 0.003*\"lot\"'),\n",
       " (14,\n",
       "  '0.003*\"frontend\" + 0.003*\"dev\" + 0.003*\"css\" + 0.003*\"cheer\" + 0.003*\"thank\" + 0.003*\"mean\" + 0.003*\"portfolio\" + 0.003*\"part\" + 0.003*\"course\" + 0.003*\"job\" + 0.003*\"post\" + 0.003*\"book\" + 0.003*\"today\" + 0.003*\"lot\" + 0.002*\"month\" + 0.002*\"type\" + 0.002*\"week\" + 0.002*\"thought\" + 0.002*\"accomplishment\" + 0.002*\"datetimetoken\"'),\n",
       " (15,\n",
       "  '0.003*\"opening\" + 0.003*\"advice\" + 0.003*\"example\" + 0.003*\"position\" + 0.002*\"training\" + 0.002*\"fulltime\" + 0.002*\"someone\" + 0.002*\"ongoing\" + 0.002*\"instant\" + 0.002*\"training-no\" + 0.002*\"boot\" + 0.002*\"hours/week\" + 0.002*\"courses/sites/group\" + 0.002*\"dice\" + 0.002*\"etc…\" + 0.002*\"builder\" + 0.002*\"month\" + 0.002*\"forum\" + 0.002*\"career\" + 0.002*\"frontend\"'),\n",
       " (16,\n",
       "  '0.002*\"link\" + 0.002*\"ground\" + 0.002*\"entry\" + 0.002*\"open\" + 0.001*\"interviewer\" + 0.001*\"impressed\" + 0.001*\"stackoverflow\" + 0.001*\"mern\" + 0.001*\"twitch\" + 0.001*\"example\" + 0.001*\"web-app\" + 0.001*\"ad\" + 0.001*\"circumstance\" + 0.001*\"mern stack\" + 0.001*\"story\" + 0.001*\"freedom\" + 0.001*\"process\" + 0.001*\"league\" + 0.001*\"long\" + 0.001*\"message\"'),\n",
       " (17,\n",
       "  '0.005*\"thank\" + 0.004*\"article\" + 0.004*\"story\" + 0.003*\"dev\" + 0.003*\"frontend\" + 0.003*\"resource\" + 0.003*\"week\" + 0.003*\"bit\" + 0.003*\"share\" + 0.003*\"place\" + 0.003*\"chance\" + 0.003*\"community\" + 0.003*\"belong\" + 0.003*\"xml\" + 0.003*\"contributor\" + 0.003*\"xsl\" + 0.002*\"hi\" + 0.002*\"office\" + 0.002*\"part\" + 0.002*\"thiswasalink\"'),\n",
       " (18,\n",
       "  '0.004*\"website\" + 0.003*\"week\" + 0.003*\"someone\" + 0.003*\"computer\" + 0.003*\"first\" + 0.002*\"programming\" + 0.002*\"dev\" + 0.002*\"apps\" + 0.002*\"career\" + 0.002*\"stuck\" + 0.002*\"thank\" + 0.002*\"cs50 edx\" + 0.002*\"edx\" + 0.002*\"hybrid\" + 0.002*\"politics\" + 0.002*\"insurmountable\" + 0.002*\"cs50\" + 0.002*\"optimistic\" + 0.002*\"increadible\" + 0.002*\"aproach\"'),\n",
       " (19,\n",
       "  '0.004*\"hi\" + 0.003*\"website\" + 0.003*\"week\" + 0.003*\"science\" + 0.003*\"skype\" + 0.003*\"learning\" + 0.003*\"political\" + 0.002*\"2-3\" + 0.002*\"html/css/php\" + 0.002*\"bright\" + 0.002*\"codacademy\" + 0.002*\"profit\" + 0.002*\"armenia\" + 0.002*\"relation\" + 0.002*\"diploma\" + 0.002*\"cad\" + 0.002*\"dps\" + 0.002*\"flexible\" + 0.002*\"month\" + 0.002*\"basic\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordimportance[\"new language framework\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "k-means to approximate the number of topics before trying a more elaborate form\n",
    "Check and improve previous work:\n",
    "* https://github.com/evaristoc/fccgitterDataScience/blob/master/Identifying%20Relevant%20Topics%20in%20a%20Chatroom.ipynb\n",
    "* https://stackoverflow.com/questions/24816912/number-of-latent-semantic-indexing-topics\n",
    "* https://stackoverflow.com/questions/9582291/how-do-we-decide-the-number-of-dimensions-for-latent-semantic-analysis/9759218#9759218\n",
    "\n",
    "Also check:\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/ (good example but conceptually a bit wrong)\n",
    "* https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "* http://hojunhao.github.io/sgparliament/LDA.html\n",
    "* https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process\n",
    "* https://nlpforhackers.io/recipe-text-clustering/\n",
    "* https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling\n",
    "* http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=3&lambda=1&term=\n",
    "* https://stackoverflow.com/questions/50106516/k-means-for-topic-modelling-elbow-method\n",
    "* https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html\n",
    "* http://wdsinet.org/Annual_Meetings/2016_Proceedings/papers/Paper45.pdf\n",
    "* http://ramet.elte.hu/~podani/Methods.htm\n",
    "* https://hk.saowen.com/a/edc29232eae094158f66e8ff3f08d6f35b8a2a45d628fce8917d2dce6f94282e\n",
    "* https://rare-technologies.com/validating-gensims-topic-coherence-pipeline/\n",
    "* http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html\n",
    "* https://www.searchenginejournal.com/latent-semantic-indexing-wont-help-seo/240705/\n",
    "* https://www.quora.com/What-is-topic-coherence + http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "* https://stackoverflow.com/questions/50340657/pyldavis-with-mallet-lda-implementation-ldamallet-object-has-no-attribute-inf\n",
    "* https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 8\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "K = list(range(1, n_components+1))\n",
    "KM = [KMeans(n_clusters=k).fit(X) for k in K]\n",
    "centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [numpy.argmin(D,axis=1) for D in D_k]\n",
    "dist = [numpy.min(D,axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "\n",
    "kIdx = 8-1\n",
    "\n",
    "# elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(K, avgWithinSS, 'b*-')\n",
    "ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for KMeans clustering')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "true_k = 8\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "newX = numpy.array(pandas.concat([pandas.DataFrame(X),datadf_foran['timestamp_norm'].reset_index()['timestamp_norm']],axis=1))\n",
    "km.fit(newX)\n",
    "\n",
    "print()\n",
    "\n",
    "labels = [x for x in range(datadf_foran.shape[0])]\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(newX, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "fig_clusters = plt.figure()\n",
    "fig_clusters.suptitle('Clusters over first 2 Components')\n",
    "ax = fig_clusters.add_subplot(111)\n",
    "ax.set_xlabel('Component I')\n",
    "ax.set_ylabel('Component II')\n",
    "plt.scatter(newX[:,0],newX[:,1], c=km.fit_predict(newX))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727"
    }
   },
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "038497e7-3cf4-4c64-867c-1bc637cad5e5"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## raw_lda_frankjupyter2: modified to fit allrecordsPreparation2\n",
    "############################\n",
    "def raw_lda_frankjupyter2(lemmposrecs, wordimportance, metmodel=2, STOPWORDS=nltk.corpus.stopwords.words('english')):\n",
    "    '''\n",
    "    description: modified model based on https://www.frankcleary.com/svd/ for a more raw construction of a lda\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    def metriccalc(st, normalizer, wordimportance):\n",
    "        '''\n",
    "        description:\n",
    "        text normalization based on ALL characters in the sentence; why? Example: if two writers wrote 20 words, 2 of them very important, but one of them wrote half of characters stopwords, those 2 words wouldnt be penalized accordingly for this writer: the other wrote more important content\n",
    "        '''       \n",
    "        metfuncs = [\n",
    "            lambda w: math.pow(0.1+float(wordimportance[w]),textbow[w]/normalizer) if w in list(wordimportance.keys()) else 0.0, #a sort of idf-normalization based on number of words in the text: the more the words in a text, the more important\n",
    "            lambda w: float(wordimportance[w])*textbow[w] if w in list(wordimportance.keys()) else 0.0, #good but ignore those words with worimportance too low or 0 but that are frequent in text\n",
    "            lambda w: 1.0+2.0**float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- probably the best one; because it is not normilized this indicator would simply say that if it has the word at least once is already on topic\n",
    "            lambda w: float(wordimportance[w]) + textbow[w]/normalizer if w in list(wordimportance.keys()) else 0.0,\n",
    "            lambda w: (1.0+textbow[w]/normalizer)*float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- apparently the second best with the adhoc word ranking\n",
    "            lambda w: normalizer/textbow[w]*wordimportance[w]/sum(list(wordimportance.values())), #tfidf-ish\n",
    "            lambda w: float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0\n",
    "        ]\n",
    "        likedict = collections.defaultdict(float)\n",
    "        textbow = collections.Counter(st)\n",
    "        for w in st:\n",
    "            likedict[w] = metfuncs[metmodel](w)\n",
    "        return likedict\n",
    "\n",
    "    #redo_corpus_by_sts = []\n",
    "    words_df = pandas.DataFrame()\n",
    "    textreference = {}\n",
    "    \n",
    "    \n",
    "    for textindex, lemmpos_r in enumerate(lemmposrecs):\n",
    "        lemmpos_t = lemmpos_r[3]\n",
    "        #print('lemmpos_t', len(lemmpos_t))\n",
    "        lemm_sts = ''\n",
    "        for lemmpos_TUPLE in lemmpos_t:\n",
    "            w = lemmpos_TUPLE[2]\n",
    "            lemm_sts = lemm_sts + w + ' '\n",
    "        sts = lemm_sts.split('.')\n",
    "        for stindex, lemmpos_st in enumerate(sts):\n",
    "            treated_st, lensts = cleaningtext(lemmpos_st.split(), STOPWORDS=STOPWORDS)        \n",
    "            #print('treated_st', lensts)\n",
    "            if len(treated_st) > 3:\n",
    "                likedict = metriccalc(treated_st, lensts, wordimportance)\n",
    "                st_df = pandas.DataFrame.from_dict(likedict, orient='index')\n",
    "                textindexing = str(textindex)+'_'+str(stindex)\n",
    "                st_df.columns = [textindexing]\n",
    "                textreference[textindexing] = {}\n",
    "                textreference[textindexing]['treated_st'] = treated_st\n",
    "                #st_df.columns = [str(count)]\n",
    "                words_df = words_df.join(st_df, how='outer', )\n",
    "    \n",
    "    words_df = words_df.fillna(0)\n",
    "    print(\"Number of unique words: %s\" % len(words_df))\n",
    "    print(words_df.head(10))\n",
    "    #print(words_df.sort(columns=words_df.columns[0], ascending=False).head(10))\n",
    "    \n",
    "    return words_df, textreference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e3fa11e-feb2-487d-9d53-43dc3deb650c": {
     "id": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "prev": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "regions": {
      "07b1e699-5143-43f7-a938-d61bc27e8e62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e1c251e9-1608-43d5-ba5a-e27bb09739f5",
        "part": "whole"
       },
       "id": "07b1e699-5143-43f7-a938-d61bc27e8e62"
      }
     }
    },
    "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5": {
     "id": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "prev": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "regions": {
      "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727",
        "part": "whole"
       },
       "id": "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0"
      }
     }
    },
    "1cf808ac-39f2-4009-8112-3ef3a17b4af6": {
     "id": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "prev": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "regions": {
      "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5992afc-b214-46ae-8f12-1026835184cd",
        "part": "whole"
       },
       "id": "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8"
      }
     }
    },
    "1f7a3d0c-f066-482e-8b40-cf107299d110": {
     "id": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "prev": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "regions": {
      "1badc1e9-6ff7-4358-9493-665026267eab": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d236a59f-ff21-406d-b1df-9436aedbdb11",
        "part": "whole"
       },
       "id": "1badc1e9-6ff7-4358-9493-665026267eab"
      }
     }
    },
    "3659283d-893a-48fa-9115-7dd64d2aed00": {
     "id": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "prev": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "regions": {
      "835576ab-c1f9-4bfa-af97-ed77dbde5925": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e294b910-1a25-4e48-abb5-37239f441f2e",
        "part": "whole"
       },
       "id": "835576ab-c1f9-4bfa-af97-ed77dbde5925"
      }
     }
    },
    "395c02de-c982-4ca8-a48e-10a79828812d": {
     "id": "395c02de-c982-4ca8-a48e-10a79828812d",
     "prev": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "regions": {
      "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3cda7d21-4a2d-4d01-ac74-10e33c83629e",
        "part": "whole"
       },
       "id": "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7"
      }
     }
    },
    "3f4f1ce3-4872-48cc-8c96-31c19dba5a74": {
     "id": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "prev": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "regions": {
      "7fb22b2c-4bff-4c35-86aa-40952a6b9b24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5bb4cd0-4cb1-493f-99e0-e75214f084a7",
        "part": "whole"
       },
       "id": "7fb22b2c-4bff-4c35-86aa-40952a6b9b24"
      }
     }
    },
    "45ee2b8f-c113-44cf-a993-6a20d43e99f5": {
     "id": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "prev": "532186bd-d619-49da-be76-e28eb7db691a",
     "regions": {
      "e4a63835-d541-4073-bee5-0017da9a065b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8e459ff-ae34-48aa-8146-50365df9ea53",
        "part": "whole"
       },
       "id": "e4a63835-d541-4073-bee5-0017da9a065b"
      }
     }
    },
    "532186bd-d619-49da-be76-e28eb7db691a": {
     "id": "532186bd-d619-49da-be76-e28eb7db691a",
     "prev": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "regions": {
      "2b7cff32-e764-4b30-8301-0cb4bb2268dc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384",
        "part": "whole"
       },
       "id": "2b7cff32-e764-4b30-8301-0cb4bb2268dc"
      }
     }
    },
    "99fa0d29-85d3-4e57-a540-74b76519f4ac": {
     "id": "99fa0d29-85d3-4e57-a540-74b76519f4ac",
     "prev": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "regions": {
      "afebcb71-18ec-40b4-9e65-8f01e4c70d57": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "038497e7-3cf4-4c64-867c-1bc637cad5e5",
        "part": "whole"
       },
       "id": "afebcb71-18ec-40b4-9e65-8f01e4c70d57"
      }
     }
    },
    "a59429ee-539c-4c02-bfc8-8582111bf455": {
     "id": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "prev": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "regions": {
      "34164544-7832-445e-a820-4ef4dbc522ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a",
        "part": "whole"
       },
       "id": "34164544-7832-445e-a820-4ef4dbc522ac"
      }
     }
    },
    "aa51d16f-a5b4-460f-91fa-569faac9a6fe": {
     "id": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "prev": null,
     "regions": {
      "e46d430e-2c0b-416c-bac3-86a2d27711c1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "560a94b7-6c8e-465f-96fa-fcf2c3859245",
        "part": "whole"
       },
       "id": "e46d430e-2c0b-416c-bac3-86a2d27711c1"
      }
     }
    },
    "ac6a71ca-a1d6-4449-9ee7-87ff28115363": {
     "id": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "prev": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "regions": {
      "ce9295e6-aa18-4ceb-a277-fb19127d89ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67bf390c-9e34-4764-a6de-ab9d3082090b",
        "part": "whole"
       },
       "id": "ce9295e6-aa18-4ceb-a277-fb19127d89ed"
      }
     }
    },
    "baa3a1da-cdb2-4638-8bec-2986349ac603": {
     "id": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "prev": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "regions": {
      "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59c7646c-3944-4e2c-81a1-1728a02396ae",
        "part": "whole"
       },
       "id": "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce"
      }
     }
    },
    "ca7a6c34-27e7-4921-9da2-43a92faf8e8e": {
     "id": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "prev": "395c02de-c982-4ca8-a48e-10a79828812d",
     "regions": {
      "6414ffa6-ea4d-40fe-9d90-5b0816ff0789": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c1fd569-b2c4-423d-a598-1466fc992a49",
        "part": "whole"
       },
       "id": "6414ffa6-ea4d-40fe-9d90-5b0816ff0789"
      }
     }
    },
    "dae9886f-f215-4b89-aefb-8831c7f2ddfc": {
     "id": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "prev": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "regions": {
      "5eeb753c-493e-4bba-b5ab-025339c264f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d80b2206-d953-46b3-b807-2bf5a8557d28",
        "part": "whole"
       },
       "id": "5eeb753c-493e-4bba-b5ab-025339c264f2"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
