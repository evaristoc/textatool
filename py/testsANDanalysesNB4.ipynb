{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/derekgreene/topic-model-tutorial \n",
    "* https://github.com/derekgreene/topic-model-tutorial/blob/master/topic-modelling-with-scikitlearn.pdf\n",
    "* https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the other hello\\nworld\n",
      "in the other hello\\nworld\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "#basic standard modules\n",
    "#######################\n",
    "import sys, os\n",
    "import time\n",
    "import collections, itertools, copy, operator\n",
    "\n",
    "#######################\n",
    "#custom config modules\n",
    "#######################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "load_dotenv()\n",
    "\n",
    "#######################\n",
    "#file manipulation modules\n",
    "#######################\n",
    "import pickle, json\n",
    "\n",
    "#######################\n",
    "#string manipulation modules\n",
    "#######################\n",
    "import re, string\n",
    "import nltk\n",
    "print(\"in the other\",os.getenv(\"TEST_MULTILINE_VAR\"))\n",
    "nltk.data.path.append(os.getenv(\"NLTKDATADIR\"))\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#######################\n",
    "#general data manipulation and data analysis modules\n",
    "#######################\n",
    "import pandas, gensim, sklearn, scipy, numpy, math\n",
    "\n",
    "#######################\n",
    "#custom modules\n",
    "#######################\n",
    "import processingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d80b2206-d953-46b3-b807-2bf5a8557d28"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## extract_candidate_chunks: candidate phrases based on http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "############################\n",
    "def extract_candidate_chunks(lemmpostxt, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}', STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "    #def redotaggedtext(lemmpostxt, lemmpossts = [], lemmposst = []):\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    lemmpostxt_redo = []\n",
    "    for lemmposst in lemmpostxt:\n",
    "        lemmposst_redo = []\n",
    "        for w,val,l,pos in lemmposst:\n",
    "            if w not in STOPWORDS:\n",
    "                #or not all(char in punct for char in w):\n",
    "                lemmposst_redo.append((l,pos))\n",
    "        lemmpostxt_redo.append(lemmposst_redo)\n",
    "            \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(lemmposst_redo)) for lemmposst_redo in lemmpostxt_redo))\n",
    "\n",
    "    #print(all_chunks)\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    def createcands(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        #print(list(group)[::-1])\n",
    "        reversing = [None]\n",
    "        #for el in group:\n",
    "        #    print(el)\n",
    "        for el in group:\n",
    "            if reversing[0] == None:\n",
    "                reversing[0] = el[0]\n",
    "            else:\n",
    "                reversing.insert(0, el[0])\n",
    "        #print(reversing)\n",
    "        for word in reversing:\n",
    "            if current == '':\n",
    "                current = word\n",
    "            else:\n",
    "                current = word + ' ' + current\n",
    "            total.append(current)\n",
    "        #print(list(set(reversing+total)))\n",
    "        return list(set(reversing+total))\n",
    "    \n",
    "    def createcands2(group):\n",
    "        total = []\n",
    "        current = ''\n",
    "        #print(type(list(group))) #group is an ITERATOR; once I call it, it will dissappear unless I do something else\n",
    "        for word, pos, chunk in list(group):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            #print(word)\n",
    "            if current == '':\n",
    "                current = word\n",
    "                total.append(word)\n",
    "            else:\n",
    "                current = current+' '+word\n",
    "                total.append(word)\n",
    "        total.append(current)\n",
    "\n",
    "        return list(set(total))\n",
    "            \n",
    "    #candidates = [' '.join(word for word, pos, chunk in group)\n",
    "    candidates = [createcands2(group)\n",
    "                    for key, group in itertools.groupby(all_chunks, lambda w_TUPLE: w_TUPLE[2] != 'O') if key]\n",
    "    #print(candidates)\n",
    "\n",
    "    #return [cand for cand in candidates]\n",
    "    #print(list(itertools.chain.from_iterable(candidates)))\n",
    "    return list(itertools.chain.from_iterable(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## allrecordsPreparation 3: revision of 2 to extend it for keyphrase candidate analysis\n",
    "## some articles:\n",
    "## -- https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "############################\n",
    "\n",
    "def allrecordsPreparation3(allrecords, STOPWORDS=nltk.corpus.stopwords.words('english'), punct = set(string.punctuation)):\n",
    "    '''\n",
    "    description: tokenization and POS tagging\n",
    "    input: dict of allrecords texts and data from different sources\n",
    "    treatment: separating only those with posts in the forum and tokenizing the posts\n",
    "    output:\n",
    "    1) list of lists, each with:\n",
    "    -- id\n",
    "    -- username\n",
    "    -- link of the post\n",
    "    -- tokenized text\n",
    "    -- POS tagged text\n",
    "    2) list of post ids\n",
    "    '''\n",
    "    print('in allrecordsPreparation (len(allrecords))::',len(allrecords))\n",
    " \n",
    "       \n",
    "    def HTMLtoText(u):\n",
    "        #block arguments\n",
    "        record = u[\"data\"]\n",
    "        forumpost = record['forum']['foundjob_msg']['text']\n",
    "        if forumpost == '':\n",
    "            return False\n",
    "        forumpostID = record['forum']['foundjob_msg']['id']\n",
    "        forumpostLINK = record['forum']['foundjob_msg']['link']\n",
    "        soup_forumpost = BeautifulSoup(forumpost)\n",
    "        pattern_A01 = re.compile(r'^hey( |$)')\n",
    "        \n",
    "        ## Text extraction, REGEXs and Replacements\n",
    "        soup_forumpostTEXT = soup_forumpost.find('body').get_text() \\\n",
    "                                                        .lower() \\\n",
    "                                                        .replace('’',\"'\") \\\n",
    "                                                        .replace('‘', \"'\") \\\n",
    "                                                        .replace('´', \"'\") \\\n",
    "                                                        .replace('free code camp', 'fcc') \\\n",
    "                                                        .replace('freecodecamp', 'fcc') \\\n",
    "                                                        .replace('javascript', 'js') \\\n",
    "                                                        .replace('part time', 'parttime') \\\n",
    "                                                        .replace('part-time', 'parttime') \\\n",
    "                                                        .replace('full time', 'fulltime') \\\n",
    "                                                        .replace('full-time', 'fulltime') \\\n",
    "                                                        .replace('web application', 'web-app') \\\n",
    "                                                        .replace('web app', 'web-app') \\\n",
    "                                                        .replace('web development', 'dev') \\\n",
    "                                                        .replace('web-development', 'dev') \\\n",
    "                                                        .replace('developer', 'dev') \\\n",
    "                                                        .replace('web dev', 'dev') \\\n",
    "                                                        .replace('dev position', 'dev job') \\\n",
    "                                                        .replace('dev role', 'dev job') \\\n",
    "                                                        .replace('front end', 'frontend') \\\n",
    "                                                        .replace('front-end','frontend') \\\n",
    "                                                        .replace('back end', 'backend') \\\n",
    "                                                        .replace('back-end','backend') \\\n",
    "                                                        .replace('full stack', 'fullstack') \\\n",
    "                                                        .replace('full-stack','fullstack') \\\n",
    "                                                        .replace('frontend job', 'dev job') \\\n",
    "                                                        .replace('frontend position', 'dev job') \\\n",
    "                                                        .replace('frontend role', 'dev job') \\\n",
    "                                                        .replace('frontend web job', 'dev job') \\\n",
    "                                                        .replace('frontend web position', 'dev job') \\\n",
    "                                                        .replace('frontend web role', 'dev job') \\\n",
    "                                                        .replace('frontend web dev job', 'dev job') \\\n",
    "                                                        .replace('frontend web dev position', 'dev job') \\\n",
    "                                                        .replace('frontend web dev role', 'dev job') \\\n",
    "                                                        .replace('resume', 'cv') \\\n",
    "                                                        .replace('angularjs', 'angular') \\\n",
    "                                                        .replace('angular', 'angularjs') \\\n",
    "                                                        .replace('certification', 'cert') \\\n",
    "                                                        .replace('certificate', 'cert') \\\n",
    "                                                        .replace('machine learning', 'machinelearning') \\\n",
    "                                                        .replace('data science', 'datascience') \\\n",
    "                                                        .replace('self learning', 'self-taught') \\\n",
    "                                                        .replace('self learned', 'self-taught') \\\n",
    "                                                        .replace('self-learning', 'self-taught') \\\n",
    "                                                        .replace('self-learned', 'self-taught') \\\n",
    "                                                        .replace('self taught', 'self-taught') \\\n",
    "                                                        .replace('thanks', 'thank') \\\n",
    "                                                        .replace('thankful', 'thank') \\\n",
    "                                                        .replace('gratitude', 'thank') \\\n",
    "                                                        .replace('many thank', 'thank') \\\n",
    "                                                        .replace('much thank', 'thank') \\\n",
    "                                                        .replace('special thank', 'thank') \\\n",
    "                                                        .replace('big thank', 'thank')\n",
    "                                                        #.replace('app', 'web-app') \\\n",
    "                                                        #.replace('web-dev job', 'dev-job') \\\n",
    "                                                        #.replace('web-dev position', 'web-dev-job') \\\n",
    "                                                        #.replace('web-dev role', 'web-dev-job') \\\n",
    "                                                        #.replace('backend job', 'dev job') \\\n",
    "                                                        #.replace('backend position', 'dev job') \\\n",
    "                                                        #.replace('backend role', 'dev job') \\\n",
    "                                                        #.replace('backend web job', 'dev job') \\\n",
    "                                                        #.replace('backend web position', 'dev job') \\\n",
    "                                                        #.replace('backend web role', 'dev job') \\\n",
    "                                                        #.replace('backend web dev job', 'dev job') \\\n",
    "                                                        #.replace('backend web dev position', 'dev job') \\\n",
    "                                                        #.replace('backend web dev role', 'dev job') \\ \n",
    "                                                        #.replace('dev job', 'dev job') \\\n",
    "                                                        \n",
    "        soup_forumpostTEXT = re.sub(pattern_A01, ' hi ', soup_forumpostTEXT)\n",
    "        soup_forumpostTEXT = soup_forumpostTEXT.replace('fellow camper', '') \\\n",
    "                                    .replace('everybody', 'everyone') \\\n",
    "                                    .replace('hello', 'hi') \\\n",
    "                                    .replace('hi everyone', 'hi') \\\n",
    "                                    .replace('hi, everyone', 'hi') \\\n",
    "                                    .replace('hi everybody', 'hi') \\\n",
    "                                    .replace('hi, everybody', 'hi')\n",
    "        \n",
    "        tksoup_forumpostTEXT = [\n",
    "                                #nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) \n",
    "                                token\n",
    "                                for token in nltk.word_tokenize(soup_forumpostTEXT)\n",
    "                                #if token.lower() not in STOPWORDS \n",
    "                                #and not re.match(r'\\d+?', token)\n",
    "                                #and not all(char in set(string.punctuation) for char in token)\n",
    "                               ]\n",
    "        return forumpostID, forumpostLINK, tksoup_forumpostTEXT\n",
    "    \n",
    "    def approxsts(text):\n",
    "        modtext = []\n",
    "        for w in tksoup_forumpostTEXT:\n",
    "            w = w.lower()\n",
    "            rws = []\n",
    "            if len(w) > 1 and len({'.','-',':'}.intersection(w)) >= 1:\n",
    "                #print(w)\n",
    "                for punc in {'.','-',':'}.intersection(w):\n",
    "                    rws = w.replace(punc, ' '+punc+' ').split()\n",
    "                #print(rws)\n",
    "            if len(rws) == 0:\n",
    "                modtext.append(w)\n",
    "            else:\n",
    "                for w in rws:\n",
    "                    modtext.append(w)\n",
    "        return modtext\n",
    "    \n",
    "    def lemmatizationofpos(postxt, lemmws):\n",
    "        lemmposws = []\n",
    "        counterrors = 0\n",
    "        countKerrors = 0\n",
    "        countIerrors = 0\n",
    "        countNCerrors = 0\n",
    "        for posw in postxt:\n",
    "            w = posw[0]\n",
    "            pos = posw[1]\n",
    "            if nltk.corpus.wordnet.synsets(w):\n",
    "                try:\n",
    "                    n = ''\n",
    "                    if nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0] == 'entity':\n",
    "                        n = w\n",
    "                    else:\n",
    "                        n = nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms()[0].name().split('.')[0]\n",
    "                    if pos[0] == 'V':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                    if pos == 'NNS' or pos == 'NN$':\n",
    "                        n = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w)\n",
    "                    #print(w, nltk.corpus.wordnet.synsets(w,pos[0].lower())[0].root_hypernyms(), n)\n",
    "                    lemmposws.append((w,True,n,pos))\n",
    "                    lemmws.append(n)\n",
    "                except KeyError: #in some cases the POS tag is not recognised by wordnet synset\n",
    "                    #print(\"pos KeyErrors\", w,pos)\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countKerrors += 1\n",
    "                except IndexError:\n",
    "                    #print(\"IndexErrors (POS not found)\", w,pos) #in some cases (w,pos) pair was not found at some point of the synsets root hyernyms\n",
    "                    #print(nltk.corpus.wordnet.synsets(w,pos[0].lower()))\n",
    "                    lemmposws.append((w,False,w,pos))\n",
    "                    lemmws.append(w)\n",
    "                    counterrors += 1\n",
    "                    countIerrors += 1\n",
    "            else:\n",
    "                #print(w, [])\n",
    "                #print(\"Error (not in corpus)\", w,pos)\n",
    "                lemmposws.append((w,False,w,pos))\n",
    "                lemmws.append(w)\n",
    "                counterrors += 1\n",
    "                countNCerrors += 1\n",
    "        #print(\"\\n\\ntotal number of errors : \", counterrors)\n",
    "        #print(\"total number of noPOSerrors : \", countKerrors)\n",
    "        #print(\"total number of indexerrors : \", countIerrors)\n",
    "        #print(\"total number of nonincorpuserrors : \", countNCerrors)\n",
    "        return lemmposws, counterrors\n",
    "      \n",
    "    \n",
    "    lemmws = []\n",
    "    lemmposrecs = []\n",
    "    count = 0\n",
    "    lemerrors = 0\n",
    "    for u in allrecords:\n",
    "        ## Getting the data as a text from HTML format (raw dataset)\n",
    "        userdata = HTMLtoText(u)\n",
    "        if userdata != False:\n",
    "            forumpostID, forumpostLINK, tksoup_forumpostTEXT = userdata\n",
    "            \n",
    "            ## Sentence identification, tokenization and POS\n",
    "            txt2possts = []\n",
    "            st = []\n",
    "            for token in tksoup_forumpostTEXT:\n",
    "                st.append(token)\n",
    "                if re.match(r'^[.!?]+|\\n$', token):\n",
    "                    postst = nltk.pos_tag(st)\n",
    "                    txt2possts.append(postst)\n",
    "                    st = []\n",
    "            if not re.match(r'^[.!?]+|\\n$', token):\n",
    "                posst = nltk.pos_tag(st)\n",
    "                txt2possts.append(posst)\n",
    "\n",
    "           \n",
    "            ## Lemm text\n",
    "            lemmpostxt = []\n",
    "            for posst in txt2possts:\n",
    "                lemst, err = lemmatizationofpos(posst, lemmws)\n",
    "                lemmpostxt.append(lemst)\n",
    "                lemerrors += err\n",
    "\n",
    "            \n",
    "            ## Keyphrases candidates; complete lemmws with candidates that are not still there\n",
    "            candidates = extract_candidate_chunks(lemmpostxt)\n",
    "            for cand in candidates:\n",
    "                if cand not in lemmws:\n",
    "                    lemmws.append(cand)\n",
    "            \n",
    "            ## Adding data to the new created dataset\n",
    "            lemmposrecs.append((\n",
    "                            'f_'+forumpostID,\n",
    "                            u[\"user\"],\n",
    "                            forumpostLINK,\n",
    "                            lemmpostxt,\n",
    "                            candidates\n",
    "                            ))\n",
    "            count += 1\n",
    "\n",
    "    print(\"number of treated posts (len(count)) ::\", count)\n",
    "    print(\"lemm errs:\", lemerrors)\n",
    "    #return all_posedsts, forum_ids\n",
    "    return lemmposrecs, nltk.FreqDist(lemmws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f8e459ff-ae34-48aa-8146-50365df9ea53"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## wordimportance_var4: wordimportance_var3 modified to fit keyphrases; changes in the \"opacity\" formula\n",
    "############################\n",
    "def wordimportance_var4(lemmposrecs, lemmws_fd, unigrams_test=True):\n",
    "    '''\n",
    "    description:\n",
    "    \n",
    "    This metric tries to solve some of the issues that appeared in `wordimportance_var1` metric, in particular the values of zero.\n",
    "    \n",
    "    This is done by setting lower bounds when required.\n",
    "    \n",
    "    --- In the case of opacity, a non-zero lower bound is set by changing the equation to the following:\n",
    "        ```\n",
    "        if 1-math.log(v)/maxdiv == 0: 1-math.log(maxdiv-1)/maxdiv # 1-math.log(v)/maxdiv == 0 if v == maxdiv\n",
    "        ```\n",
    "    --- In the case of sizing, a redefinition of the metric force a non-zero lower bound as well as rebumpimg rare terms in documents:\n",
    "        ```\n",
    "        (sum(vector)-max(vector))/sum(vector)\n",
    "        ```\n",
    "    \n",
    "    input:\n",
    "        1) tokenized list of texts\n",
    "        2) freqDist of lemmatized words\n",
    "    \n",
    "    output: wordimportance \n",
    "    '''\n",
    "    if unigrams_test:\n",
    "        unigrams = dict([(grams, count) \n",
    "                         for grams, count in lemmws_fd.items() \n",
    "                         if len(grams.split()) == 1 or (len(grams.split()) == 2 and '' in grams.split())\n",
    "                        ])\n",
    "        print('unigrams',len(unigrams))\n",
    "        maxdiv = math.log(sorted(unigrams.items(), key=lambda x: x[1], reverse=True)[0][1])\n",
    "        print('maxdiv', maxdiv)\n",
    "        opacity = collections.defaultdict(float)\n",
    "        #for grams, counts in lemmws_fd.items(): #grams assumes a phrase is possible\n",
    "        for grams, counts in unigrams.items():\n",
    "            if grams == '':\n",
    "                opacity[grams] = 0.0\n",
    "                continue\n",
    "            opval = []\n",
    "            #assert len(grams.split()) == 1, print(grams)\n",
    "            for gram in grams.split():\n",
    "                if gram == '':\n",
    "                    continue\n",
    "                if gram in list(unigrams.keys()):\n",
    "                    opval.append(math.log(unigrams[gram]))\n",
    "                else:\n",
    "                    opval.append(0.0)\n",
    "            #assert len(opval) != 0, print('grams',grams)\n",
    "            averopval = sum(opval)/len(opval)\n",
    "            if 1 - averopval/maxdiv != 0.0:\n",
    "                opacity[grams] = 1 - averopval/maxdiv\n",
    "            else:\n",
    "                opacity[grams] = 1-math.log(maxdiv-1)/maxdiv\n",
    "\n",
    "        #print('opval',opval[:10])\n",
    "\n",
    "        #sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(lemmws_fd.keys())])\n",
    "        sizing_matrix = dict([(w, [0]*len(lemmposrecs)) for w in list(unigrams.keys())])\n",
    "\n",
    "        ## Count lemmatized words/characters per text  \n",
    "        for i,lemmpos_t in enumerate(lemmposrecs):\n",
    "            for k, lemmpos_sts in enumerate(lemmpos_t[3]):\n",
    "                ## Use lemmatized word\n",
    "                #print(lemmpos_sts)\n",
    "                for tk_TUPLE in lemmpos_sts: \n",
    "                    lemmw = tk_TUPLE[2]\n",
    "                    if lemmw in list(unigrams.keys()):\n",
    "                        sizing_matrix[lemmw][i] = sizing_matrix[lemmw][i] + 1\n",
    "            #for cand in lemmpos_t[4]: #<--------------------------------- problem!! it counted candidates only once!!!\n",
    "            #    if cand not in sizing_matrix:\n",
    "            #        sizing_matrix[cand][i] = sizing_matrix[cand][i] + 1\n",
    "\n",
    "\n",
    "    ## Normalization\n",
    "    normalization = dict([(k, (sum(vector)-max(vector))/sum(vector)) if sum(vector) != 0 else (k,0.0) for k, vector in sizing_matrix.items()])\n",
    "    #normalization = collections.defaultdict(float)\n",
    "    #for k, vector in sizing_matrix.items():\n",
    "    #    if sum(vector) != 0:\n",
    "    #        normalization[k] = (sum(vector)-max(vector))/sum(vector)\n",
    "    #    else:\n",
    "    #        \n",
    "        \n",
    "    \n",
    "    wordimportance = dict([(k, valnorm*opacity[k]) for k, valnorm in normalization.items()])\n",
    "\n",
    "    return wordimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a"
    }
   },
   "outputs": [],
   "source": [
    "def cleaningtext(st, unigrams_test = False):\n",
    "    treated_st = []\n",
    "    if not unigrams_test:\n",
    "        for w in st:\n",
    "            treated_st.append(w)\n",
    "    else:\n",
    "        for w in st:\n",
    "            if len(w.split()) == 1:\n",
    "                treated_st.append(w)\n",
    "    countwds = len(treated_st)\n",
    "    return treated_st, countwds\n",
    "\n",
    "def cleaningtext2(st): #discharge phrases\n",
    "    treated_st = []\n",
    "    countwds = len(st)\n",
    "\n",
    "    return treated_st, countwds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "59c7646c-3944-4e2c-81a1-1728a02396ae"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## gensim_models2 re-evaluated: modification of gensim_model2's cleanedsts_from_lemmpostxts to fit keyphrases\n",
    "############################\n",
    "\n",
    "def gensim_models2(lemmposrecs, NUM_TOPICS = 15, lemmws_fd = {}, wordimportance = {}, unigrams_test = False, nltk = nltk, gensim = gensim):\n",
    "    \n",
    "    def cleanedsts_from_lemmpostxts2(lemmposrecs, STOPWORDS = nltk.corpus.stopwords.words('english')):\n",
    "        redo_corpus_by_sts = []\n",
    "        for lemmpos_r in lemmposrecs:\n",
    "            candidates = lemmpos_r[4]\n",
    "            treated_lemmcands = cleaningtext(candidates, unigrams_test=unigrams_test)[0] #passing candidates only\n",
    "            redo_corpus_by_sts.append(treated_lemmcands)\n",
    "            #redo_corpus_by_sts.append(candidates)\n",
    "        \n",
    "        #print(len(redo_corpus_by_sts), redo_corpus_by_sts[-1])\n",
    "        return redo_corpus_by_sts\n",
    "        \n",
    "    def basedonBOW(redo_corpus_by_sts):\n",
    "        dictionary = gensim.corpora.Dictionary(redo_corpus_by_sts) #[token for st in redo_corpus_by_sts for token in st]\n",
    "        corpus = [dictionary.doc2bow(text) for text in redo_corpus_by_sts]\n",
    "        return corpus, dictionary\n",
    "    \n",
    "    def basedonTFIDF(corpus):\n",
    "        return gensim.models.TfidfModel(corpus)\n",
    "    \n",
    "    def basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance):\n",
    "        \n",
    "        def metriccalc(w):\n",
    "            if w in wordimportance:\n",
    "                return 1.0+2.0**float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        def metriccalc2(w):\n",
    "            if w in wordimportance:\n",
    "                return float(wordimportance[w])\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        corpus = []\n",
    "        for sts in redo_corpus_by_sts:\n",
    "            st = []\n",
    "            for w in sts:\n",
    "                st.append((dictionary.token2id[w], metriccalc2(w)))\n",
    "            corpus.append(st)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "    redo_corpus_by_sts  = cleanedsts_from_lemmpostxts2(lemmposrecs)\n",
    "    \n",
    "    corpus, dictionary = basedonBOW(redo_corpus_by_sts)\n",
    "    if wordimportance == {'tfidf':True}:\n",
    "        tfidf = basedonTFIDF(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "    if wordimportance != {} and wordimportance != {'tfidf':True}:\n",
    "        corpus = basedonOTHER(redo_corpus_by_sts, dictionary, wordimportance)\n",
    "    \n",
    "    \n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, minimum_probability=0.005, iterations=100, per_word_topics = True, minimum_phi_value = 0.001, id2word=dictionary)\n",
    "    lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "\n",
    "   \n",
    "    return lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e294b910-1a25-4e48-abb5-37239f441f2e"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "with open('../data/jobproject_forum.json','r') as message:\n",
    "    otp = json.load(message)\n",
    "print(len(otp))\n",
    "allrecords = [{ \"user\": k, \"data\": otp[k] }  for k in otp]\n",
    "print(len(allrecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d236a59f-ff21-406d-b1df-9436aedbdb11"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in allrecordsPreparation (len(allrecords)):: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of treated posts (len(count)) :: 53\n",
      "lemm errs: 19264\n"
     ]
    }
   ],
   "source": [
    "lemmposrecs, lemmws_fd = allrecordsPreparation3(allrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, \n",
    "#                                                                                NUM_TOPICS=20,\n",
    "#                                                                                wordimportance = {'tfidf':True})\n",
    "\n",
    "#lda_model.print_topics(num_words=15)\n",
    "\n",
    "#[' '.join([l for wr in rec[3] for w,_,l,pos in wr]) for rec in lemmposrecs]\n",
    "#[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs]\n",
    "#[' '.join([cand for cand in rec[4]]) for rec in lemmposrecs]\n",
    "##https://stackoverflow.com/questions/46282473/error-while-identify-the-coherence-value-from-lda-model\n",
    "#texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "#texts\n",
    "\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                                                  texts=texts,\n",
    "#                                                                  #corpus=corpus,\n",
    "#                                                                  window_size=20,\n",
    "#                                                                  dictionary=dictionary, \n",
    "#                                                                  coherence='c_uci')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, \n",
    "#                                                   texts=texts, \n",
    "#                                                   dictionary=dictionary,\n",
    "#                                                   coherence='c_v')\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "## Compute Coherence Score using UMass\n",
    "#coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "#                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "#                                     corpus = corpus,\n",
    "#                                     dictionary=dictionary, \n",
    "#                                     coherence=\"u_mass\")\n",
    "#coherence_lda = coherence_model_lda.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "#dictionary\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TOPICS = range(2,61,2)\n",
    "cv_coherence_values = []\n",
    "for numtopics in TOPICS:\n",
    "    print('\\n\\nFor NUM_TOPICS:', numtopics)\n",
    "    lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=numtopics, wordimportance = {'tfidf':True})\n",
    "    umass_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     #texts=[[l for wr in rec[3] for w,_,l,pos in wr] for rec in lemmposrecs],\n",
    "                                     corpus = corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"u_mass\")\n",
    "    umass_coherence_lda = umass_coherence_model_lda.get_coherence()\n",
    "    print('U-Mass Coherence Score: ', umass_coherence_lda)\n",
    "    texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "    cv_coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, \n",
    "                                     texts=texts,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence=\"c_v\")\n",
    "    cv_coherence_lda = cv_coherence_model_lda.get_coherence()\n",
    "    cv_coherence_values.append(cv_coherence_lda)\n",
    "    print('C_V Coherence Score: ', cv_coherence_lda)\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.quora.com/Can-I-combine-LSI-and-K-means-for-text-document-clustering-Are-there-any-sources-to-learn-about-it\n",
    "#http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##https://stackoverflow.com/questions/14261903/how-can-i-open-the-interactive-matplotlib-window-in-ipython-notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "limit=61; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, cv_coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=40, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lda_model.print_topics(num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = {'tfidf':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.127*\"project\" + -0.102*\"company\" + -0.099*\"interview\" + -0.098*\"people\" + -0.095*\"lot\" + -0.094*\"code\" + -0.094*\"way\" + -0.088*\"cv\" + -0.087*\"app\" + -0.087*\"skill\" + -0.086*\"day\" + -0.085*\"time\" + -0.079*\"experience\" + -0.079*\"recruiter\" + -0.078*\"good\"'),\n",
       " (1,\n",
       "  '0.198*\"design\" + 0.139*\"graphic\" + -0.114*\"success\" + -0.107*\"backend\" + 0.096*\"role\" + -0.093*\"group\" + -0.092*\"mern stack\" + -0.092*\"mern\" + 0.080*\"support\" + -0.080*\"hard work\" + 0.077*\"lawyer\" + 0.076*\"internship\" + -0.075*\"friend\" + -0.074*\"source\" + 0.073*\"challenge\"'),\n",
       " (2,\n",
       "  '-0.124*\"resource\" + -0.109*\"lawyer\" + -0.108*\"success\" + -0.104*\"story\" + -0.094*\"current\" + -0.089*\"science\" + 0.087*\"cv\" + -0.086*\"article\" + 0.084*\"jquery\" + -0.076*\"degree\" + 0.076*\"company\" + -0.074*\"group\" + -0.072*\"article share story resource\" + -0.072*\"school kid\" + -0.072*\"belong\"'),\n",
       " (3,\n",
       "  '0.150*\"app\" + 0.117*\"study\" + 0.116*\"logic\" + 0.103*\"adwords\" + -0.103*\"role\" + 0.097*\"thought\" + -0.094*\"lawyer\" + 0.090*\"salary\" + -0.088*\"current\" + 0.081*\"html\" + -0.079*\"student\" + 0.076*\"skill\" + 0.075*\"css js\" + -0.075*\"qa\" + 0.075*\"lol\"'),\n",
       " (4,\n",
       "  '-0.115*\"internship\" + -0.105*\"success\" + -0.103*\"story\" + 0.101*\"role\" + -0.100*\"jquery\" + 0.099*\"technology\" + 0.095*\"current\" + -0.085*\"knowledge\" + -0.085*\"share\" + -0.083*\"test\" + -0.082*\"fulltime\" + 0.078*\"app\" + 0.075*\"lawyer\" + 0.074*\"failure\" + 0.072*\"challenge\"'),\n",
       " (5,\n",
       "  '0.109*\"dev job\" + -0.107*\"group\" + 0.096*\"dream\" + -0.094*\"role\" + 0.090*\"upwork\" + 0.090*\"css js\" + -0.086*\"challenge\" + 0.084*\"start\" + 0.083*\"year time\" + 0.083*\"self-taught fcc\" + 0.083*\"interview frontend dev job today\" + 0.083*\"idea type question\" + -0.083*\"thats\" + 0.081*\"idea\" + 0.074*\"sleep\"'),\n",
       " (6,\n",
       "  '-0.211*\"internship\" + -0.123*\"technology\" + -0.112*\"software\" + -0.105*\"single company\" + -0.105*\"lot interest technology roadmap\" + -0.105*\"bachelor software technology\" + -0.105*\"roadmap\" + -0.105*\"student dev within day company\" + -0.105*\"cv/cv\" + -0.103*\"student\" + -0.101*\"reply\" + -0.091*\"internship couple freelance project\" + -0.091*\"gaining\" + -0.091*\"little bit code free time\" + -0.091*\"whoo\"'),\n",
       " (7,\n",
       "  '-0.175*\"internship\" + 0.110*\"qa\" + -0.098*\"little bit code free time\" + -0.098*\"development knowledge gaining experience fulltime job\" + -0.098*\"gaining\" + -0.098*\"helpful tip\" + -0.098*\"internship couple freelance project\" + -0.098*\"many many job application\" + -0.098*\"share others\" + -0.098*\"whoo\" + -0.097*\"people\" + -0.093*\"adwords\" + 0.093*\"design\" + -0.091*\"lawyer\" + -0.087*\"others\"'),\n",
       " (8,\n",
       "  '-0.126*\"email\" + -0.096*\"graduation\" + 0.085*\"interview call\" + 0.085*\"happy part platform\" + 0.085*\"experience real time project\" + 0.085*\"dev post yesterday\" + 0.085*\"portfolio employer\" + 0.081*\"happy\" + -0.081*\"idea\" + -0.080*\"marketing\" + 0.079*\"year half\" + -0.078*\"mobile\" + -0.078*\"promising\" + -0.078*\"first time\" + -0.078*\"web-apps hybrid mobile apps\"'),\n",
       " (9,\n",
       "  '0.117*\"thats\" + 0.117*\"quincy\" + 0.106*\"qa\" + -0.101*\"design\" + -0.099*\"graphic\" + 0.096*\"right\" + -0.086*\"internship\" + 0.085*\"august\" + 0.083*\"frontend dev\" + 0.080*\"analyst\" + -0.079*\"jquery\" + -0.078*\"self-taught\" + -0.076*\"udemy\" + -0.069*\"github\" + 0.067*\"profession\"'),\n",
       " (10,\n",
       "  '-0.165*\"lawyer\" + 0.149*\"internship\" + 0.115*\"quincy\" + 0.103*\"thats\" + 0.095*\"fullstack\" + -0.090*\"medium article\" + -0.085*\"article\" + 0.083*\"codecs\" + 0.083*\"organisation\" + 0.083*\"fullstack dev job offer\" + -0.082*\"satisfied\" + -0.082*\"satisfied current job\" + -0.082*\"ideas.ataccama.com\" + -0.081*\"medium\" + 0.081*\"cheer\"'),\n",
       " (11,\n",
       "  '0.157*\"lawyer\" + -0.111*\"intro\" + -0.106*\"interview process\" + 0.090*\"design\" + 0.086*\"medium article\" + -0.085*\"path\" + -0.083*\"venezuela\" + -0.082*\"college\" + -0.082*\"graduation\" + -0.080*\"learner\" + 0.078*\"ideas.ataccama.com\" + 0.078*\"satisfied current job\" + 0.078*\"satisfied\" + 0.076*\"pay\" + -0.072*\"reading\"'),\n",
       " (12,\n",
       "  '-0.112*\"amazing platform community\" + -0.112*\"first job frontend email dev digital marketing agency\" + -0.112*\"journey february\" + -0.112*\"previous programming knowledge\" + -0.112*\"thank much fcc\" + -0.112*\"larson\" + -0.105*\"february\" + 0.087*\"upwork\" + -0.084*\"journey\" + -0.077*\"angularjs\" + -0.073*\"amazing\" + 0.073*\"way\" + -0.071*\"knowledge\" + 0.068*\"graphic\" + 0.068*\"person\"'),\n",
       " (13,\n",
       "  '0.145*\"lawyer\" + 0.122*\"jquery\" + 0.104*\"article\" + 0.092*\"process\" + 0.088*\"belong\" + 0.088*\"school kid\" + 0.088*\"article share story resource\" + 0.088*\"community thank\" + 0.088*\"contributor\" + 0.088*\"doesnt\" + 0.088*\"part learning process\" + 0.088*\"topic doesnt belong\" + 0.082*\"github\" + -0.079*\"amazing platform community\" + -0.079*\"thank much fcc\"'),\n",
       " (14,\n",
       "  '-0.222*\"lawyer\" + -0.130*\"xsl\" + -0.130*\"first frontend dev job\" + -0.130*\"backend cert\" + -0.130*\"xml\" + -0.130*\"first period\" + -0.130*\"second week new place\" + -0.127*\"backend\" + -0.126*\"venezuela\" + -0.111*\"ideas.ataccama.com\" + -0.111*\"satisfied\" + -0.111*\"satisfied current job\" + -0.107*\"dev post yesterday\" + -0.107*\"happy part platform\" + -0.107*\"interview call\"'),\n",
       " (15,\n",
       "  '-0.143*\"lawyer\" + 0.103*\"app\" + 0.103*\"adwords\" + -0.103*\"logic\" + 0.100*\"year half\" + -0.100*\"math\" + 0.099*\"half\" + 0.088*\"dev post yesterday\" + 0.088*\"portfolio employer\" + 0.088*\"interview call\" + 0.088*\"happy part platform\" + 0.088*\"experience real time project\" + 0.081*\"failure\" + -0.076*\"current\" + -0.072*\"satisfied\"'),\n",
       " (16,\n",
       "  '-0.115*\"freelance\" + -0.112*\"venezuela\" + 0.095*\"student\" + 0.091*\"logic\" + -0.085*\"design\" + 0.082*\"cv/cv\" + 0.082*\"bachelor software technology\" + 0.082*\"roadmap\" + 0.082*\"lot interest technology roadmap\" + 0.082*\"student dev within day company\" + 0.082*\"single company\" + 0.081*\"within\" + 0.079*\"team\" + 0.078*\"energy\" + -0.077*\"fulltime\"'),\n",
       " (17,\n",
       "  '0.113*\"topic doesnt belong\" + 0.113*\"school kid\" + 0.113*\"article share story resource\" + 0.113*\"belong\" + 0.113*\"community thank\" + 0.113*\"contributor\" + 0.113*\"doesnt\" + 0.113*\"part learning process\" + 0.092*\"kid\" + -0.088*\"path\" + -0.084*\"github\" + -0.083*\"lawyer\" + 0.080*\"within\" + 0.079*\"single company\" + 0.079*\"lot interest technology roadmap\"'),\n",
       " (18,\n",
       "  '0.119*\"employer\" + -0.116*\"recruiter\" + 0.113*\"portfolio employer\" + 0.113*\"interview call\" + 0.113*\"happy part platform\" + 0.113*\"experience real time project\" + 0.113*\"dev post yesterday\" + 0.104*\"qa\" + -0.097*\"lawyer\" + -0.094*\"student\" + -0.090*\"udacity\" + -0.084*\"first frontend dev job\" + -0.084*\"first period\" + -0.084*\"second week new place\" + -0.084*\"xml\"'),\n",
       " (19,\n",
       "  '0.130*\"experience real time project\" + 0.130*\"dev post yesterday\" + 0.130*\"happy part platform\" + 0.130*\"portfolio employer\" + 0.130*\"interview call\" + -0.109*\"group\" + 0.105*\"self-taught fcc\" + 0.105*\"idea type question\" + 0.105*\"interview frontend dev job today\" + 0.105*\"year time\" + -0.103*\"lawyer\" + -0.101*\"first frontend dev job\" + -0.101*\"first period\" + -0.101*\"second week new place\" + -0.101*\"backend cert\"')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"adwords\" + 0.002*\"success\" + 0.001*\"post long time\" + 0.001*\"job fulltime dev\" + 0.001*\"frontend cert along udemy course\" + 0.001*\"proud\" + 0.001*\"success story fcc forum\" + 0.001*\"additional library\" + 0.001*\"na\" + 0.001*\"friend local fcc group everyone\" + 0.001*\"thank success story\" + 0.001*\"story\" + 0.001*\"script\" + 0.001*\"app\" + 0.001*\"boss\" + 0.001*\"mern stack\" + 0.001*\"holiday\" + 0.001*\"success story\" + 0.001*\"mern\" + 0.001*\"along\"'),\n",
       " (1,\n",
       "  '0.002*\"year time\" + 0.002*\"idea type question\" + 0.002*\"interview frontend dev job today\" + 0.002*\"self-taught fcc\" + 0.002*\"year half\" + 0.001*\"goal\" + 0.001*\"css js\" + 0.001*\"bachelor software technology\" + 0.001*\"single company\" + 0.001*\"roadmap\" + 0.001*\"lot interest technology roadmap\" + 0.001*\"student dev within day company\" + 0.001*\"cv/cv\" + 0.001*\"technology\" + 0.001*\"type\" + 0.001*\"failure\" + 0.001*\"self-taught\" + 0.001*\"start\" + 0.001*\"process\" + 0.001*\"student\"'),\n",
       " (2,\n",
       "  '0.001*\"closure\" + 0.001*\"luck\" + 0.001*\"meetup\" + 0.001*\"answer\" + 0.001*\"self\" + 0.001*\"js”\" + 0.001*\"fcc one\" + 0.001*\"zeppelin\" + 0.001*\"strength\" + 0.001*\"self strength\" + 0.001*\"fcc flaw\" + 0.001*\"sake\" + 0.001*\"\\'yes\" + 0.001*\"edit:3.\" + 0.001*\"much detail\" + 0.001*\"answer system test\" + 0.001*\"big thank\" + 0.001*\"flaw\" + 0.001*\"edit:3. interview\" + 0.001*\"true\"'),\n",
       " (3,\n",
       "  '0.002*\"amazing platform community\" + 0.002*\"journey february\" + 0.002*\"first job frontend email dev digital marketing agency\" + 0.002*\"larson\" + 0.002*\"previous programming knowledge\" + 0.002*\"thank much fcc\" + 0.002*\"february\" + 0.001*\"marketing\" + 0.001*\"digital\" + 0.001*\"previous\" + 0.001*\"amazing\" + 0.001*\"email\" + 0.001*\"platform\" + 0.001*\"journey\" + 0.001*\"agency\" + 0.001*\"knowledge\" + 0.001*\"much\" + 0.001*\"programming\" + 0.001*\"community\" + 0.001*\"hi\"'),\n",
       " (4,\n",
       "  '0.002*\"portfolio employer\" + 0.002*\"experience real time project\" + 0.002*\"dev post yesterday\" + 0.002*\"happy part platform\" + 0.002*\"interview call\" + 0.002*\"venezuela\" + 0.002*\"call\" + 0.002*\"freelance\" + 0.001*\"happy\" + 0.001*\"yesterday\" + 0.001*\"post\" + 0.001*\"first dev job age\" + 0.001*\"fcc november\" + 0.001*\"post fcc solid path\" + 0.001*\"local dev job\" + 0.001*\"night hour spent reading coding\" + 0.001*\"call guy\" + 0.001*\"food home\" + 0.001*\"js technology deep way\" + 0.001*\"frontend backend cert\"'),\n",
       " (5,\n",
       "  '0.000*\"grunt\" + 0.000*\"inbuilt\" + 0.000*\"informed decision\" + 0.000*\"individual job application\" + 0.000*\"gulp grunt\" + 0.000*\"first glance\" + 0.000*\"great cv\" + 0.000*\"good place\" + 0.000*\"good cv cv template\" + 0.000*\"glance\" + 0.000*\"github page\" + 0.000*\"general interview\" + 0.000*\"frontend project\" + 0.000*\"foot door interview\" + 0.000*\"foot\" + 0.000*\"focus\" + 0.000*\"first role\" + 0.000*\"help.”\" + 0.000*\"informed\" + 0.000*\"hunting\"'),\n",
       " (6,\n",
       "  '0.002*\"second week new place\" + 0.002*\"first period\" + 0.002*\"first frontend dev job\" + 0.002*\"xml\" + 0.002*\"backend cert\" + 0.002*\"xsl\" + 0.002*\"upwork\" + 0.001*\"period\" + 0.001*\"place\" + 0.001*\"second\" + 0.001*\"view\" + 0.001*\"vp\" + 0.001*\"profile\" + 0.001*\"client\" + 0.001*\"backend\" + 0.001*\"communication\" + 0.001*\"first\" + 0.001*\"gig\" + 0.001*\"basic\" + 0.001*\"summer\"'),\n",
       " (7,\n",
       "  '0.001*\"gulp\" + 0.001*\"day\" + 0.001*\"accomplishment\" + 0.001*\"blog\" + 0.001*\"thought\" + 0.001*\"fcc frontend certicate day\" + 0.001*\"momentum\" + 0.001*\"netflix\" + 0.001*\"react markdown app day\" + 0.001*\"60-75\" + 0.001*\"blog post\" + 0.001*\"timeline\" + 0.001*\"thought accomplishment\" + 0.001*\"offer thought\" + 0.001*\"technology language\" + 0.001*\"scss\" + 0.001*\"recipe+box\" + 0.001*\"react+redux recipe+box app day\" + 0.001*\"react+redux\" + 0.001*\"web optimization day\"'),\n",
       " (8,\n",
       "  '0.001*\"junior\" + 0.001*\"london\" + 0.001*\"decision\" + 0.001*\"box\" + 0.001*\"ability\" + 0.001*\"final interview\" + 0.001*\"factor\" + 0.001*\"access\" + 0.001*\"progression\" + 0.001*\"senior\" + 0.001*\"informed\" + 0.001*\"building\" + 0.001*\"progression opportunity\" + 0.001*\"final\" + 0.001*\"etc\" + 0.001*\"sale\" + 0.001*\"take\" + 0.001*\"role\" + 0.001*\"skill\" + 0.001*\"technology\"'),\n",
       " (9,\n",
       "  '0.003*\"lawyer\" + 0.002*\"current\" + 0.002*\"satisfied\" + 0.002*\"satisfied current job\" + 0.002*\"ideas.ataccama.com\" + 0.002*\"role\" + 0.002*\"group\" + 0.002*\"medium article\" + 0.001*\"prototype\" + 0.001*\"good thing\" + 0.001*\"daughter\" + 0.001*\"portal\" + 0.001*\"medium\" + 0.001*\"look\" + 0.001*\"infrastructure\" + 0.001*\"support\" + 0.001*\"article\" + 0.001*\"engineering\" + 0.001*\"weekend\" + 0.001*\"people\"'),\n",
       " (10,\n",
       "  '0.001*\"role\" + 0.000*\"building\" + 0.000*\"take\" + 0.000*\"ability\" + 0.000*\"london\" + 0.000*\"decision\" + 0.000*\"final\" + 0.000*\"progression opportunity\" + 0.000*\"skill\" + 0.000*\"etc\" + 0.000*\"informed\" + 0.000*\"junior\" + 0.000*\"progression\" + 0.000*\"senior\" + 0.000*\"management\" + 0.000*\"method\" + 0.000*\"access\" + 0.000*\"factor\" + 0.000*\"final interview\" + 0.000*\"box\"'),\n",
       " (11,\n",
       "  '0.001*\"recruiter\" + 0.001*\"medior\" + 0.001*\"hell\" + 0.001*\"essential\" + 0.001*\"call\" + 0.001*\"level\" + 0.001*\"lesson\" + 0.001*\"pressure\" + 0.001*\"car\" + 0.001*\"scope\" + 0.001*\"async\" + 0.001*\"super\" + 0.001*\"company\" + 0.001*\"second\" + 0.001*\"backend\" + 0.001*\"database\" + 0.001*\"es6\" + 0.001*\"functionality\" + 0.001*\"moment\" + 0.001*\"question\"'),\n",
       " (12,\n",
       "  '0.001*\"student\" + 0.001*\"webmaster\" + 0.001*\"puzzle\" + 0.001*\"rail\" + 0.001*\"english\" + 0.001*\"universe\" + 0.001*\"graduate\" + 0.001*\"fresh\" + 0.001*\"jr\" + 0.001*\"-i\" + 0.001*\"jrs\" + 0.001*\"michael\" + 0.001*\"city\" + 0.001*\"dev job\" + 0.001*\"confidence\" + 0.001*\"scrum\" + 0.001*\"salary\" + 0.001*\"passion\" + 0.001*\"love\" + 0.001*\"recruiter\"'),\n",
       " (13,\n",
       "  '0.002*\"design\" + 0.002*\"thats\" + 0.001*\"right\" + 0.001*\"challenge\" + 0.001*\"data\" + 0.001*\"jquery\" + 0.001*\"graphic\" + 0.001*\"hope\" + 0.001*\"head\" + 0.001*\"drunk\" + 0.001*\"frontend dev\" + 0.001*\"ad\" + 0.001*\"course\" + 0.001*\"sale\" + 0.001*\"bar\" + 0.001*\"language\" + 0.001*\"morning\" + 0.001*\"frontend course\" + 0.001*\"python\" + 0.001*\"//manuelbasanta.github.io/\"'),\n",
       " (14,\n",
       "  '0.002*\"internship\" + 0.001*\"energy\" + 0.001*\"nice\" + 0.001*\"cv\" + 0.001*\"framework\" + 0.001*\"code\" + 0.001*\"company\" + 0.001*\"huge\" + 0.001*\"phone\" + 0.001*\"hang\" + 0.001*\"recruitment\" + 0.001*\"angularjs\" + 0.001*\"yesterday\" + 0.001*\"cover\" + 0.001*\"way\" + 0.001*\"js\" + 0.001*\"object\" + 0.001*\"learn\" + 0.001*\"people\" + 0.001*\"read\"'),\n",
       " (15,\n",
       "  '0.002*\"qa\" + 0.002*\"lol\" + 0.002*\"analyst\" + 0.001*\"assignment\" + 0.001*\"texas\" + 0.001*\"fun\" + 0.001*\"tuesday\" + 0.001*\"\" + 0.001*\"home\" + 0.001*\"start-up\" + 0.001*\"agency\" + 0.001*\"dev job\" + 0.001*\"year\" + 0.001*\"day\" + 0.001*\"exercise\" + 0.001*\"course\" + 0.001*\"quincy\" + 0.001*\"hunt\" + 0.001*\"person\" + 0.001*\"cool stuff fcc shop\"'),\n",
       " (16,\n",
       "  '0.001*\"coursera\" + 0.001*\"github\" + 0.001*\"organisation\" + 0.001*\"codecs\" + 0.001*\"fullstack dev job offer\" + 0.001*\"july\" + 0.001*\"stack\" + 0.001*\"mean\" + 0.001*\"college degree\" + 0.001*\"tracker\" + 0.001*\"college\" + 0.001*\"app\" + 0.001*\"source\" + 0.001*\"hard\" + 0.001*\"open\" + 0.001*\"work github\" + 0.001*\"league\" + 0.001*\"new app ground\" + 0.001*\"lot freedom\" + 0.001*\"make sure\"'),\n",
       " (17,\n",
       "  '0.001*\"tip\" + 0.001*\"math\" + 0.001*\"logic\" + 0.001*\"fulltime commitment\" + 0.001*\"helpful tip\" + 0.001*\"internship couple freelance project\" + 0.001*\"many many job application\" + 0.001*\"little bit code free time\" + 0.001*\"whoo\" + 0.001*\"development knowledge gaining experience fulltime job\" + 0.001*\"share others\" + 0.001*\"gaining\" + 0.001*\"sleep\" + 0.001*\"dream\" + 0.001*\"profession\" + 0.001*\"share\" + 0.001*\"skill\" + 0.001*\"bright\" + 0.001*\"much time learning\" + 0.001*\"codacademy\"'),\n",
       " (18,\n",
       "  '0.002*\"topic doesnt belong\" + 0.002*\"part learning process\" + 0.002*\"doesnt\" + 0.002*\"contributor\" + 0.002*\"belong\" + 0.002*\"school kid\" + 0.002*\"community thank\" + 0.002*\"article share story resource\" + 0.001*\"kid\" + 0.001*\"school\" + 0.001*\"topic\" + 0.001*\"article\" + 0.001*\"dev job\" + 0.001*\"process\" + 0.001*\"share\" + 0.001*\"resource\" + 0.001*\"curriculum\" + 0.001*\"part\" + 0.001*\"story\" + 0.001*\"way\"'),\n",
       " (19,\n",
       "  '0.002*\"time http\" + 0.002*\"http\" + 0.001*\"university\" + 0.001*\"graphic design year\" + 0.001*\"subject\" + 0.001*\"something stuck\" + 0.001*\"promising career\" + 0.001*\"promising\" + 0.001*\"politics\" + 0.001*\"optimistic\" + 0.001*\"minute monday\" + 0.001*\"job reading job\" + 0.001*\"idea graduation\" + 0.001*\"insurmountable task\" + 0.001*\"insurmountable\" + 0.001*\"first dev job small agency\" + 0.001*\"first time\" + 0.001*\"hybrid\" + 0.001*\"lucky optimistic future\" + 0.001*\"politics university\"')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda_model.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dictionary.id2token[2298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "adwords 1616 0.00205492\n",
      "success 768 0.00185609\n",
      "frontend cert along udemy course 2686 0.00129769\n",
      "post long time 2690 0.00129769\n",
      "job fulltime dev 2688 0.00129769\n",
      "success story fcc forum 2692 0.00129769\n",
      "proud 2691 0.00129769\n",
      "thank success story 2693 0.00129769\n",
      "additional library 2684 0.00129769\n",
      "friend local fcc group everyone 2685 0.00129769\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "year time 3371 0.00173954\n",
      "idea type question 3368 0.00173954\n",
      "interview frontend dev job today 3369 0.00173954\n",
      "self-taught fcc 3370 0.00173954\n",
      "year half 1559 0.00158198\n",
      "goal 1139 0.00147681\n",
      "css js 3220 0.00147108\n",
      "bachelor software technology 3820 0.00143518\n",
      "student dev within day company 3825 0.00143518\n",
      "lot interest technology roadmap 3822 0.00143518\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "closure 2733 0.0014222\n",
      "luck 1330 0.00120494\n",
      "meetup 469 0.001123\n",
      "answer 26 0.00106812\n",
      "self 697 0.0010499\n",
      "js” 2761 0.00102383\n",
      "big thank 2696 0.000991495\n",
      "sake 2711 0.000991495\n",
      "edit:3. interview 2701 0.000991495\n",
      "fcc one 2703 0.000991495\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "amazing platform community 2480 0.00194501\n",
      "thank much fcc 2485 0.00194501\n",
      "first job frontend email dev digital marketing agency 2481 0.00194501\n",
      "larson 2483 0.00194501\n",
      "previous programming knowledge 2484 0.00194501\n",
      "journey february 2482 0.00194501\n",
      "february 1224 0.0016501\n",
      "digital 902 0.00126024\n",
      "marketing 932 0.00126024\n",
      "previous 604 0.00126024\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "experience real time project 1508 0.00214704\n",
      "portfolio employer 1511 0.00214704\n",
      "dev post yesterday 1507 0.00214704\n",
      "interview call 1510 0.00214704\n",
      "happy part platform 1509 0.00214704\n",
      "venezuela 2441 0.00210659\n",
      "call 67 0.00159513\n",
      "freelance 985 0.00157587\n",
      "happy 1072 0.00137742\n",
      "yesterday 876 0.00121652\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "general interview 2549 0.00026137\n",
      "frontend project 2548 0.00026137\n",
      "good place 2553 0.00026137\n",
      "good cv cv template 2552 0.00026137\n",
      "glance 2551 0.00026137\n",
      "github page 2550 0.00026137\n",
      "grunt 2555 0.00026137\n",
      "first role 2544 0.00026137\n",
      "foot 2546 0.00026137\n",
      "help.” 2557 0.00026137\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "second week new place 1661 0.00197822\n",
      "xsl 1663 0.00197822\n",
      "first period 1659 0.00197822\n",
      "first frontend dev job 1658 0.00197822\n",
      "xml 1662 0.00197822\n",
      "backend cert 1657 0.00197822\n",
      "upwork 1035 0.00177484\n",
      "period 561 0.0014982\n",
      "place 1660 0.0013725\n",
      "second 695 0.00119534\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "gulp 1459 0.00137259\n",
      "day 181 0.00132224\n",
      "accomplishment 6 0.00123383\n",
      "blog 54 0.00113539\n",
      "thought 804 0.00105903\n",
      "fcc frontend certicate day 1457 0.000930461\n",
      "netflix 1467 0.000930461\n",
      "momentum 1466 0.000930461\n",
      "recipe+box 1475 0.000930461\n",
      "scss 1476 0.000930461\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "junior 1249 0.000808829\n",
      "london 1084 0.000697967\n",
      "decision 1748 0.000619527\n",
      "box 2505 0.0006181\n",
      "ability 5 0.000616502\n",
      "final interview 2541 0.000612926\n",
      "factor 2537 0.00061127\n",
      "access 2488 0.000604933\n",
      "progression 2623 0.000596627\n",
      "senior 699 0.000587449\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "lawyer 1372 0.00337186\n",
      "current 162 0.00192587\n",
      "satisfied 1374 0.00180312\n",
      "satisfied current job 1375 0.00180312\n",
      "ideas.ataccama.com 1371 0.00180312\n",
      "role 681 0.00161889\n",
      "group 328 0.00153904\n",
      "medium article 1373 0.00152925\n",
      "prototype 2412 0.00141193\n",
      "daughter 2384 0.00118546\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "#https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-0e631fc43b3b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-0e631fc43b3b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    END HERE\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams 2915\n",
      "maxdiv 7.202661196523238\n"
     ]
    }
   ],
   "source": [
    "wordimportance = wordimportance_var4(lemmposrecs, lemmws_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model, lsi_model, redo_corpus_by_sts, corpus, dictionary = gensim_models2(lemmposrecs, NUM_TOPICS=20, wordimportance = wordimportance, unigrams_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.300*\"company\" + 0.248*\"interview\" + 0.240*\"project\" + 0.238*\"job\" + 0.205*\"time\" + 0.154*\"day\" + 0.153*\"people\" + 0.143*\"thing\" + 0.140*\"experience\" + 0.136*\"way\" + 0.133*\"code\" + 0.130*\"lot\" + 0.127*\"cv\" + 0.123*\"dev\" + 0.120*\"month\" + 0.118*\"many\" + 0.116*\"fcc\" + 0.111*\"question\" + 0.104*\"something\" + 0.101*\"new\"'),\n",
       " (1,\n",
       "  '0.283*\"fcc\" + 0.252*\"job\" + -0.187*\"company\" + 0.180*\"year\" + -0.154*\"cv\" + 0.151*\"dev\" + 0.144*\"week\" + 0.135*\"backend\" + -0.133*\"code\" + 0.128*\"frontend\" + -0.127*\"thing\" + 0.112*\"recruiter\" + 0.111*\"skill\" + -0.109*\"github\" + -0.108*\"react\" + 0.107*\"end\" + -0.105*\"people\" + -0.103*\"technology\" + -0.101*\"problem\" + 0.100*\"guy\"'),\n",
       " (2,\n",
       "  '0.258*\"dev\" + -0.213*\"company\" + -0.203*\"recruiter\" + 0.187*\"lot\" + -0.180*\"day\" + -0.170*\"interview\" + -0.169*\"call\" + 0.153*\"job\" + 0.145*\"skill\" + -0.144*\"level\" + 0.138*\"good\" + -0.129*\"question\" + -0.129*\"end\" + 0.128*\"frontend\" + 0.118*\"course\" + -0.115*\"next\" + -0.108*\"moment\" + 0.107*\"fcc\" + -0.106*\"second\" + 0.103*\"year\"'),\n",
       " (3,\n",
       "  '-0.242*\"dev\" + 0.222*\"junior\" + 0.193*\"opportunity\" + 0.189*\"skill\" + 0.175*\"role\" + 0.167*\"couple\" + 0.131*\"etc\" + 0.131*\"professional\" + 0.124*\"interview\" + 0.123*\"senior\" + 0.123*\"people\" + 0.123*\"employer\" + 0.121*\"tech\" + 0.117*\"life\" + 0.117*\"sale\" + 0.114*\"decision\" + 0.109*\"technical\" + 0.109*\"way\" + 0.108*\"technology\" + 0.105*\"message\"'),\n",
       " (4,\n",
       "  '0.222*\"backend\" + 0.204*\"fcc\" + 0.191*\"lot\" + 0.177*\"skill\" + 0.173*\"good\" + 0.157*\"fullstack\" + 0.152*\"pay\" + 0.151*\"startup\" + 0.140*\"city\" + 0.132*\"dream\" + -0.131*\"learning\" + 0.130*\"story\" + 0.127*\"experience\" + -0.125*\"js\" + -0.122*\"week\" + 0.116*\"everyone\" + 0.113*\"coding\" + 0.108*\"friend\" + -0.107*\"email\" + 0.107*\"team\"'),\n",
       " (5,\n",
       "  '-0.302*\"degree\" + -0.273*\"lot\" + -0.233*\"course\" + 0.196*\"dev\" + -0.180*\"current\" + -0.170*\"year\" + -0.163*\"resource\" + -0.157*\"college\" + -0.143*\"science\" + 0.139*\"fcc\" + -0.134*\"life\" + -0.128*\"chance\" + -0.126*\"programming\" + 0.117*\"good\" + -0.116*\"little\" + 0.107*\"skill\" + -0.107*\"university\" + 0.097*\"tech\" + -0.093*\"computer\" + -0.090*\"internet\"'),\n",
       " (6,\n",
       "  '-0.216*\"js\" + -0.215*\"project\" + -0.198*\"someone\" + 0.196*\"job\" + -0.194*\"skill\" + -0.183*\"something\" + -0.147*\"basic\" + -0.145*\"css\" + -0.136*\"angularjs\" + -0.131*\"area\" + -0.125*\"portfolio\" + -0.122*\"tip\" + -0.118*\"try\" + 0.116*\"week\" + -0.116*\"time\" + -0.113*\"lead\" + 0.109*\"dev\" + -0.108*\"fact\" + -0.103*\"top\" + -0.101*\"strong\"'),\n",
       " (7,\n",
       "  '-0.260*\"project\" + -0.199*\"path\" + 0.195*\"skill\" + -0.160*\"december\" + -0.155*\"test\" + -0.146*\"day\" + 0.143*\"week\" + -0.141*\"phone\" + -0.134*\"fcc\" + 0.127*\"salary\" + -0.120*\"community\" + 0.120*\"program\" + -0.119*\"course\" + -0.116*\"career\" + 0.111*\"website\" + -0.110*\"process\" + 0.108*\"web\" + 0.103*\"resource\" + -0.101*\"class\" + 0.101*\"experience\"'),\n",
       " (8,\n",
       "  '0.241*\"design\" + 0.203*\"html\" + 0.200*\"language\" + 0.159*\"position\" + 0.140*\"basic\" + -0.140*\"lot\" + 0.139*\"new\" + 0.137*\"web\" + -0.135*\"degree\" + 0.131*\"js\" + 0.130*\"day\" + 0.129*\"challenge\" + 0.127*\"tutorial\" + -0.121*\"thing\" + -0.121*\"resource\" + 0.120*\"fcc\" + 0.116*\"css\" + 0.111*\"programming\" + 0.110*\"graphic\" + 0.108*\"hi\"'),\n",
       " (9,\n",
       "  '-0.258*\"year\" + 0.206*\"month\" + 0.200*\"lot\" + 0.177*\"design\" + 0.170*\"position\" + -0.153*\"team\" + 0.139*\"experience\" + -0.135*\"web\" + -0.126*\"time\" + -0.122*\"first\" + -0.114*\"something\" + -0.113*\"stack\" + -0.113*\"language\" + 0.109*\"test\" + 0.109*\"learning\" + -0.109*\"python\" + -0.105*\"interest\" + -0.101*\"week\" + -0.099*\"code\" + -0.097*\"life\"'),\n",
       " (10,\n",
       "  '0.207*\"dev\" + -0.188*\"team\" + -0.183*\"path\" + -0.176*\"week\" + -0.169*\"local\" + -0.159*\"career\" + -0.134*\"programming\" + -0.132*\"test\" + -0.128*\"good\" + -0.127*\"office\" + -0.123*\"js\" + -0.121*\"python\" + -0.119*\"interest\" + -0.110*\"process\" + 0.108*\"year\" + -0.104*\"couple\" + -0.104*\"bit\" + -0.103*\"environment\" + -0.103*\"december\" + 0.101*\"portfolio\"'),\n",
       " (11,\n",
       "  '-0.212*\"learning\" + 0.203*\"day\" + 0.192*\"project\" + -0.172*\"fulltime\" + -0.149*\"year\" + 0.144*\"week\" + 0.137*\"way\" + 0.136*\"app\" + -0.132*\"process\" + -0.130*\"coding\" + 0.127*\"course\" + -0.126*\"community\" + -0.121*\"test\" + -0.111*\"php\" + -0.110*\"goal\" + -0.108*\"class\" + -0.106*\"design\" + -0.106*\"employer\" + -0.103*\"interview\" + 0.103*\"tech\"'),\n",
       " (12,\n",
       "  '-0.226*\"app\" + 0.203*\"year\" + -0.193*\"dev\" + -0.181*\"basic\" + 0.171*\"email\" + -0.161*\"knowledge\" + -0.159*\"study\" + 0.155*\"fcc\" + 0.150*\"day\" + 0.141*\"interview\" + -0.129*\"hi\" + -0.127*\"website\" + 0.114*\"css\" + -0.104*\"way\" + 0.102*\"last\" + 0.102*\"fullstack\" + -0.102*\"first\" + 0.097*\"course\" + 0.096*\"hunt\" + -0.096*\"bachelor\"'),\n",
       " (13,\n",
       "  '0.209*\"design\" + -0.162*\"interview\" + -0.160*\"thank\" + 0.151*\"project\" + 0.143*\"way\" + -0.142*\"study\" + 0.133*\"support\" + -0.128*\"salary\" + 0.127*\"current\" + 0.125*\"fulltime\" + -0.121*\"test\" + -0.121*\"thought\" + 0.117*\"science\" + -0.117*\"question\" + 0.114*\"client\" + 0.112*\"fcc\" + 0.106*\"graphic\" + -0.105*\"day\" + -0.105*\"time\" + 0.103*\"world\"'),\n",
       " (14,\n",
       "  '-0.210*\"frontend\" + 0.175*\"class\" + 0.167*\"skill\" + -0.153*\"development\" + 0.148*\"people\" + 0.139*\"job\" + 0.124*\"thing\" + -0.120*\"goal\" + 0.110*\"portfolio\" + -0.106*\"year\" + -0.106*\"fulltime\" + 0.102*\"website\" + 0.101*\"lot\" + -0.099*\"environment\" + -0.099*\"share\" + 0.097*\"app\" + -0.097*\"client\" + -0.096*\"story\" + 0.094*\"html\" + -0.092*\"bit\"'),\n",
       " (15,\n",
       "  '-0.216*\"day\" + -0.214*\"community\" + 0.208*\"dev\" + 0.132*\"year\" + -0.128*\"something\" + -0.125*\"git\" + 0.124*\"confidence\" + -0.120*\"huge\" + 0.119*\"article\" + -0.117*\"coding\" + 0.117*\"career\" + -0.116*\"book\" + 0.111*\"design\" + -0.110*\"platform\" + -0.103*\"curriculum\" + -0.100*\"guy\" + -0.100*\"hi\" + 0.098*\"current\" + 0.098*\"role\" + -0.097*\"job\"'),\n",
       " (16,\n",
       "  '-0.241*\"day\" + -0.164*\"networking\" + -0.159*\"cert\" + -0.153*\"forum\" + -0.148*\"role\" + -0.146*\"group\" + -0.144*\"person\" + -0.140*\"fcc\" + 0.121*\"hour\" + 0.121*\"experience\" + -0.121*\"science\" + -0.114*\"employer\" + 0.109*\"bit\" + 0.107*\"lot\" + 0.106*\"test\" + 0.104*\"something\" + -0.103*\"people\" + -0.101*\"way\" + 0.100*\"position\" + 0.100*\"js\"'),\n",
       " (17,\n",
       "  '-0.171*\"frontend\" + -0.167*\"fulltime\" + -0.162*\"course\" + 0.145*\"something\" + -0.145*\"good\" + -0.142*\"app\" + -0.137*\"time\" + 0.133*\"fcc\" + -0.132*\"profile\" + -0.117*\"contract\" + 0.113*\"community\" + 0.111*\"salary\" + 0.108*\"people\" + -0.105*\"portfolio\" + -0.103*\"test\" + -0.103*\"summer\" + -0.095*\"front\" + 0.095*\"book\" + 0.095*\"luck\" + -0.094*\"group\"'),\n",
       " (18,\n",
       "  '0.255*\"day\" + -0.180*\"knowledge\" + -0.153*\"project\" + 0.148*\"client\" + -0.145*\"course\" + 0.137*\"idea\" + -0.136*\"position\" + 0.124*\"app\" + 0.113*\"cert\" + 0.111*\"takeaway\" + 0.106*\"profile\" + 0.105*\"professional\" + 0.103*\"challenge\" + 0.101*\"way\" + -0.100*\"jquery\" + 0.099*\"living\" + 0.098*\"college\" + 0.097*\"coding\" + 0.097*\"freelance\" + -0.096*\"response\"'),\n",
       " (19,\n",
       "  '0.234*\"month\" + -0.202*\"frontend\" + 0.164*\"home\" + -0.160*\"person\" + 0.157*\"first\" + -0.147*\"project\" + -0.127*\"entry\" + -0.119*\"knowledge\" + -0.118*\"love\" + -0.116*\"jquery\" + -0.113*\"way\" + 0.112*\"quincy\" + -0.109*\"world\" + 0.106*\"curriculum\" + 0.106*\"stuff\" + -0.105*\"interview\" + -0.103*\"web\" + -0.103*\"language\" + 0.100*\"fullstack\" + -0.099*\"chance\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"result\" + 0.001*\"perfect\" + 0.001*\"opening\" + 0.001*\"big\" + 0.001*\"codepen\" + 0.001*\"via\" + 0.001*\"session\" + 0.001*\"networking\" + 0.001*\"express\" + 0.001*\"difficult\" + 0.001*\"outside\" + 0.001*\"key\" + 0.001*\"syndrome\" + 0.001*\"yes\" + 0.001*\"name\" + 0.001*\"computer\" + 0.001*\"journey\" + 0.001*\"anything\" + 0.001*\"fulltime\" + 0.001*\"word\"'),\n",
       " (1,\n",
       "  '0.001*\"foundation\" + 0.001*\"commitment\" + 0.001*\"amazing\" + 0.001*\"codepen\" + 0.001*\"value\" + 0.001*\"strong\" + 0.001*\"tip\" + 0.001*\"choice\" + 0.001*\"environment\" + 0.001*\"everything\" + 0.001*\"top\" + 0.001*\"enough\" + 0.001*\"editor\" + 0.001*\"stuff\" + 0.001*\"yesterday\" + 0.001*\"i.e\" + 0.001*\"bit\" + 0.001*\"small\" + 0.001*\"path\" + 0.001*\"text\"'),\n",
       " (2,\n",
       "  '0.002*\"bit\" + 0.002*\"resource\" + 0.002*\"js\" + 0.002*\"language\" + 0.002*\"small\" + 0.002*\"programming\" + 0.002*\"many\" + 0.001*\"real\" + 0.001*\"hi\" + 0.001*\"python\" + 0.001*\"account\" + 0.001*\"community\" + 0.001*\"foundation\" + 0.001*\"data\" + 0.001*\"book\" + 0.001*\"guy\" + 0.001*\"frontend\" + 0.001*\"good\" + 0.001*\"issue\" + 0.001*\"camper\"'),\n",
       " (3,\n",
       "  '0.009*\"community\" + 0.008*\"amazing\" + 0.007*\"frontend\" + 0.007*\"hi\" + 0.006*\"first\" + 0.006*\"programming\" + 0.006*\"dev\" + 0.006*\"class\" + 0.006*\"board\" + 0.006*\"fcc\" + 0.005*\"new\" + 0.005*\"year\" + 0.005*\"camper\" + 0.005*\"digital\" + 0.005*\"beginning\" + 0.005*\"week\" + 0.005*\"field\" + 0.004*\"knowledge\" + 0.004*\"agency\" + 0.004*\"path\"'),\n",
       " (4,\n",
       "  '0.006*\"month\" + 0.006*\"website\" + 0.006*\"week\" + 0.005*\"dev\" + 0.005*\"frontend\" + 0.005*\"thank\" + 0.005*\"someone\" + 0.005*\"hi\" + 0.005*\"time\" + 0.005*\"bit\" + 0.005*\"career\" + 0.005*\"year\" + 0.005*\"life\" + 0.004*\"computer\" + 0.004*\"stuff\" + 0.004*\"something\" + 0.004*\"community\" + 0.004*\"day\" + 0.004*\"thing\" + 0.004*\"coding\"'),\n",
       " (5,\n",
       "  '0.008*\"uk\" + 0.008*\"advice\" + 0.008*\"hi\" + 0.007*\"community\" + 0.006*\"thank\" + 0.006*\"year\" + 0.005*\"anything\" + 0.005*\"video\" + 0.005*\"forum\" + 0.005*\"us\" + 0.005*\"dev\" + 0.005*\"code\" + 0.005*\"across\" + 0.005*\"i.e\" + 0.004*\"heart\" + 0.004*\"screening\" + 0.004*\"hang\" + 0.004*\"fcc\" + 0.004*\"mean\" + 0.004*\"home\"'),\n",
       " (6,\n",
       "  '0.009*\"thank\" + 0.008*\"stuff\" + 0.007*\"dev\" + 0.006*\"story\" + 0.006*\"part\" + 0.006*\"tutorial\" + 0.006*\"job\" + 0.006*\"fulltime\" + 0.006*\"post\" + 0.006*\"month\" + 0.005*\"course\" + 0.005*\"udemy\" + 0.005*\"experience\" + 0.005*\"position\" + 0.005*\"platform\" + 0.005*\"local\" + 0.005*\"year\" + 0.005*\"something\" + 0.005*\"school\" + 0.005*\"way\"'),\n",
       " (7,\n",
       "  '0.003*\"responsive\" + 0.003*\"template\" + 0.003*\"interest\" + 0.002*\"search\" + 0.002*\"advice\" + 0.002*\"cool\" + 0.002*\"rate\" + 0.002*\"local\" + 0.002*\"focus\" + 0.002*\"command\" + 0.002*\"line\" + 0.002*\"home\" + 0.002*\"career\" + 0.002*\"agency\" + 0.002*\"learning\" + 0.002*\"couple\" + 0.002*\"around\" + 0.002*\"position\" + 0.002*\"commitment\" + 0.002*\"cert\"'),\n",
       " (8,\n",
       "  '0.001*\"udacity\" + 0.001*\"princeton\" + 0.001*\"payment\" + 0.001*\"pragmatic\" + 0.001*\"fight\" + 0.001*\"uneducated\" + 0.001*\"upper\" + 0.001*\"variety\" + 0.001*\"vast\" + 0.001*\"workaround\" + 0.001*\"worker\" + 0.001*\"“our\" + 0.001*\"\\'you\" + 0.001*\"april\" + 0.001*\"art\" + 0.001*\"ask\" + 0.001*\"check\" + 0.001*\"ticket\" + 0.001*\"pos-\" + 0.001*\"questions…\"'),\n",
       " (9,\n",
       "  '0.006*\"last\" + 0.006*\"month\" + 0.006*\"week\" + 0.006*\"hour\" + 0.006*\"lot\" + 0.006*\"programming\" + 0.005*\"contract\" + 0.005*\"learning\" + 0.005*\"hi\" + 0.005*\"guy\" + 0.005*\"test\" + 0.005*\"time\" + 0.004*\"new\" + 0.004*\"year\" + 0.004*\"everything\" + 0.004*\"platform\" + 0.004*\"fcc\" + 0.004*\"camper\" + 0.004*\"project\" + 0.004*\"good\"'),\n",
       " (10,\n",
       "  '0.001*\"udacity\" + 0.001*\"princeton\" + 0.001*\"payment\" + 0.001*\"pragmatic\" + 0.001*\"fight\" + 0.001*\"uneducated\" + 0.001*\"upper\" + 0.001*\"variety\" + 0.001*\"vast\" + 0.001*\"workaround\" + 0.001*\"worker\" + 0.001*\"“our\" + 0.001*\"\\'you\" + 0.001*\"april\" + 0.001*\"art\" + 0.001*\"ask\" + 0.001*\"check\" + 0.001*\"ticket\" + 0.001*\"pos-\" + 0.001*\"questions…\"'),\n",
       " (11,\n",
       "  '0.007*\"article\" + 0.007*\"many\" + 0.006*\"hi\" + 0.006*\"programming\" + 0.006*\"knowledge\" + 0.006*\"medium\" + 0.006*\"dev\" + 0.005*\"share\" + 0.005*\"bit\" + 0.005*\"fulltime\" + 0.005*\"thank\" + 0.005*\"project\" + 0.005*\"experience\" + 0.005*\"computer\" + 0.005*\"job\" + 0.005*\"fcc\" + 0.005*\"last\" + 0.005*\"tutorial\" + 0.004*\"foundation\" + 0.004*\"current\"'),\n",
       " (12,\n",
       "  '0.009*\"frontend\" + 0.008*\"course\" + 0.008*\"portfolio\" + 0.008*\"week\" + 0.008*\"dev\" + 0.007*\"website\" + 0.007*\"cert\" + 0.006*\"lot\" + 0.006*\"html\" + 0.006*\"https\" + 0.006*\"hi\" + 0.006*\"career\" + 0.006*\"first\" + 0.006*\"css\" + 0.006*\"thank\" + 0.005*\"position\" + 0.005*\"development\" + 0.005*\"job\" + 0.005*\"fcc\" + 0.005*\"js\"'),\n",
       " (13,\n",
       "  '0.006*\"lot\" + 0.006*\"course\" + 0.006*\"programming\" + 0.005*\"bachelor\" + 0.005*\"school\" + 0.005*\"interest\" + 0.005*\"dev\" + 0.005*\"month\" + 0.005*\"part\" + 0.005*\"fcc\" + 0.004*\"day\" + 0.004*\"bit\" + 0.004*\"journey\" + 0.004*\"chance\" + 0.004*\"degree\" + 0.004*\"project\" + 0.004*\"something\" + 0.004*\"last\" + 0.004*\"cert\" + 0.004*\"css\"'),\n",
       " (14,\n",
       "  '0.009*\"frontend\" + 0.008*\"cert\" + 0.007*\"coding\" + 0.007*\"today\" + 0.007*\"year\" + 0.006*\"post\" + 0.006*\"css\" + 0.006*\"dev\" + 0.006*\"thought\" + 0.005*\"day\" + 0.005*\"fcc\" + 0.005*\"type\" + 0.005*\"time\" + 0.005*\"js\" + 0.005*\"freelance\" + 0.005*\"job\" + 0.005*\"hard\" + 0.005*\"interview\" + 0.005*\"thank\" + 0.005*\"accomplishment\"'),\n",
       " (15,\n",
       "  '0.005*\"networking\" + 0.005*\"camper\" + 0.005*\"platform\" + 0.005*\"strong\" + 0.005*\"general\" + 0.005*\"night\" + 0.005*\"session\" + 0.005*\"standard\" + 0.005*\"dozen\" + 0.005*\"shop\" + 0.005*\"pull\" + 0.004*\"free\" + 0.004*\"curriculum\" + 0.004*\"java\" + 0.004*\"response\" + 0.004*\"remote\" + 0.004*\"hi\" + 0.004*\"website\" + 0.004*\"internet\" + 0.004*\"value\"'),\n",
       " (16,\n",
       "  '0.001*\"udacity\" + 0.001*\"princeton\" + 0.001*\"payment\" + 0.001*\"pragmatic\" + 0.001*\"fight\" + 0.001*\"uneducated\" + 0.001*\"upper\" + 0.001*\"variety\" + 0.001*\"vast\" + 0.001*\"workaround\" + 0.001*\"worker\" + 0.001*\"“our\" + 0.001*\"\\'you\" + 0.001*\"april\" + 0.001*\"art\" + 0.001*\"ask\" + 0.001*\"check\" + 0.001*\"ticket\" + 0.001*\"pos-\" + 0.001*\"questions…\"'),\n",
       " (17,\n",
       "  '0.001*\"udacity\" + 0.001*\"princeton\" + 0.001*\"payment\" + 0.001*\"pragmatic\" + 0.001*\"fight\" + 0.001*\"uneducated\" + 0.001*\"upper\" + 0.001*\"variety\" + 0.001*\"vast\" + 0.001*\"workaround\" + 0.001*\"worker\" + 0.001*\"“our\" + 0.001*\"\\'you\" + 0.001*\"april\" + 0.001*\"art\" + 0.001*\"ask\" + 0.001*\"check\" + 0.001*\"ticket\" + 0.001*\"pos-\" + 0.001*\"questions…\"'),\n",
       " (18,\n",
       "  '0.006*\"education\" + 0.006*\"life\" + 0.005*\"year\" + 0.005*\"goal\" + 0.005*\"old\" + 0.005*\"degree\" + 0.004*\"word\" + 0.004*\"frontend\" + 0.004*\"part\" + 0.004*\"state\" + 0.004*\"background\" + 0.004*\"networking\" + 0.004*\"frustration\" + 0.004*\"stress\" + 0.004*\"day\" + 0.004*\"process\" + 0.004*\"last\" + 0.004*\"clone\" + 0.004*\"article\" + 0.004*\"fcc\"'),\n",
       " (19,\n",
       "  '0.007*\"month\" + 0.007*\"hi\" + 0.007*\"learning\" + 0.006*\"thank\" + 0.006*\"dev\" + 0.006*\"story\" + 0.006*\"frontend\" + 0.006*\"new\" + 0.006*\"day\" + 0.005*\"someone\" + 0.005*\"phone\" + 0.005*\"fcc\" + 0.005*\"knowledge\" + 0.005*\"chance\" + 0.005*\"development\" + 0.005*\"course\" + 0.005*\"next\" + 0.005*\"people\" + 0.005*\"fulltime\" + 0.005*\"good\"')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic num. 0  :\n",
      "result 402 0.0009558\n",
      "perfect 338 0.000929645\n",
      "opening 324 0.000878873\n",
      "big 32 0.000864936\n",
      "codepen 64 0.000862951\n",
      "via 514 0.000851116\n",
      "session 427 0.000850831\n",
      "networking 306 0.000847225\n",
      "express 153 0.000846781\n",
      "difficult 121 0.000846512\n",
      "\n",
      "\n",
      "Topic num. 1  :\n",
      "foundation 1040 0.00134804\n",
      "commitment 599 0.00132499\n",
      "amazing 648 0.00124367\n",
      "codepen 64 0.00115859\n",
      "value 511 0.00115414\n",
      "strong 811 0.0011438\n",
      "tip 1135 0.00111966\n",
      "choice 1358 0.00108301\n",
      "environment 738 0.00106157\n",
      "everything 146 0.00105697\n",
      "\n",
      "\n",
      "Topic num. 2  :\n",
      "bit 34 0.00191829\n",
      "resource 709 0.0018778\n",
      "js 239 0.00178481\n",
      "language 245 0.00178213\n",
      "small 444 0.0017535\n",
      "programming 372 0.00166218\n",
      "many 276 0.00152776\n",
      "real 388 0.00147919\n",
      "hi 660 0.00147701\n",
      "python 380 0.00147328\n",
      "\n",
      "\n",
      "Topic num. 3  :\n",
      "community 71 0.00947275\n",
      "amazing 648 0.00848911\n",
      "frontend 178 0.00749094\n",
      "hi 660 0.00695469\n",
      "first 166 0.00635575\n",
      "programming 372 0.0061309\n",
      "dev 117 0.00599922\n",
      "class 58 0.00569726\n",
      "board 36 0.00565379\n",
      "fcc 162 0.00562532\n",
      "\n",
      "\n",
      "Topic num. 4  :\n",
      "month 300 0.00566077\n",
      "website 523 0.00564381\n",
      "week 524 0.00559681\n",
      "dev 117 0.00532644\n",
      "frontend 178 0.00518636\n",
      "thank 484 0.00505604\n",
      "someone 448 0.00493448\n",
      "hi 660 0.00491588\n",
      "time 492 0.00489093\n",
      "bit 34 0.0046296\n",
      "\n",
      "\n",
      "Topic num. 5  :\n",
      "uk 840 0.00836573\n",
      "advice 13 0.00798568\n",
      "hi 660 0.00788354\n",
      "community 71 0.00719116\n",
      "thank 484 0.00628866\n",
      "year 537 0.00558869\n",
      "anything 19 0.0047897\n",
      "forum 827 0.0047897\n",
      "video 676 0.0047897\n",
      "us 1225 0.00475898\n",
      "\n",
      "\n",
      "Topic num. 6  :\n",
      "thank 484 0.00944821\n",
      "stuff 589 0.00768839\n",
      "dev 117 0.00740691\n",
      "story 460 0.00633343\n",
      "part 334 0.00606104\n",
      "tutorial 594 0.00593445\n",
      "job 235 0.00570639\n",
      "fulltime 182 0.00567987\n",
      "post 355 0.00566652\n",
      "month 300 0.00562635\n",
      "\n",
      "\n",
      "Topic num. 7  :\n",
      "responsive 673 0.00348\n",
      "template 877 0.0030788\n",
      "interest 229 0.00262772\n",
      "search 417 0.00246138\n",
      "advice 13 0.00246135\n",
      "cool 89 0.00242452\n",
      "rate 1110 0.00237346\n",
      "local 266 0.00236895\n",
      "focus 1374 0.00231358\n",
      "command 68 0.00229493\n",
      "\n",
      "\n",
      "Topic num. 8  :\n",
      "worker 1285 0.00051975\n",
      "'you 1287 0.00051975\n",
      "upper 1281 0.00051975\n",
      "variety 1282 0.00051975\n",
      "vast 1283 0.00051975\n",
      "workaround 1284 0.00051975\n",
      "udacity 1279 0.00051975\n",
      "“our 1286 0.00051975\n",
      "check 1291 0.00051975\n",
      "art 1289 0.00051975\n",
      "\n",
      "\n",
      "Topic num. 9  :\n",
      "last 248 0.00629111\n",
      "month 300 0.00591734\n",
      "week 524 0.00588774\n",
      "hour 214 0.00583377\n",
      "lot 270 0.00570107\n",
      "programming 372 0.00566303\n",
      "contract 602 0.00543964\n",
      "learning 250 0.00524835\n",
      "hi 660 0.00515451\n",
      "guy 569 0.00471972\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('\\n\\nTopic num.', str(t),' :')\n",
    "    for ids, prob in lda_model.get_topic_terms(t, topn=10):\n",
    "        print(dictionary.id2token[ids], ids, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "k-means to approximate the number of topics before trying a more elaborate form\n",
    "Check and improve previous work:\n",
    "* https://github.com/evaristoc/fccgitterDataScience/blob/master/Identifying%20Relevant%20Topics%20in%20a%20Chatroom.ipynb\n",
    "* https://stackoverflow.com/questions/24816912/number-of-latent-semantic-indexing-topics\n",
    "* https://stackoverflow.com/questions/9582291/how-do-we-decide-the-number-of-dimensions-for-latent-semantic-analysis/9759218#9759218\n",
    "\n",
    "Also check:\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/ (good example but conceptually a bit wrong)\n",
    "* https://radimrehurek.com/gensim/models/hdpmodel.html\n",
    "* http://hojunhao.github.io/sgparliament/LDA.html\n",
    "* https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process\n",
    "* https://nlpforhackers.io/recipe-text-clustering/\n",
    "* https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-is-the-best-way-to-determine-k-number-of-topics-in-topic-modeling\n",
    "* http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=3&lambda=1&term=\n",
    "* https://stackoverflow.com/questions/50106516/k-means-for-topic-modelling-elbow-method\n",
    "* https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html\n",
    "* http://wdsinet.org/Annual_Meetings/2016_Proceedings/papers/Paper45.pdf\n",
    "* http://ramet.elte.hu/~podani/Methods.htm\n",
    "* https://hk.saowen.com/a/edc29232eae094158f66e8ff3f08d6f35b8a2a45d628fce8917d2dce6f94282e\n",
    "* https://rare-technologies.com/validating-gensims-topic-coherence-pipeline/\n",
    "* http://sujitpal.blogspot.com/2014/08/topic-modeling-with-gensim-over-past.html\n",
    "* https://www.searchenginejournal.com/latent-semantic-indexing-wont-help-seo/240705/\n",
    "* https://www.quora.com/What-is-topic-coherence + http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "* https://stackoverflow.com/questions/50340657/pyldavis-with-mallet-lda-implementation-ldamallet-object-has-no-attribute-inf\n",
    "* https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 8\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "K = list(range(1, n_components+1))\n",
    "KM = [KMeans(n_clusters=k).fit(X) for k in K]\n",
    "centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [numpy.argmin(D,axis=1) for D in D_k]\n",
    "dist = [numpy.min(D,axis=1) for D in D_k]\n",
    "avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "\n",
    "\n",
    "kIdx = 8-1\n",
    "\n",
    "# elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(K, avgWithinSS, 'b*-')\n",
    "ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average within-cluster sum of squares')\n",
    "plt.title('Elbow for KMeans clustering')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "true_k = 8\n",
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "newX = numpy.array(pandas.concat([pandas.DataFrame(X),datadf_foran['timestamp_norm'].reset_index()['timestamp_norm']],axis=1))\n",
    "km.fit(newX)\n",
    "\n",
    "print()\n",
    "\n",
    "labels = [x for x in range(datadf_foran.shape[0])]\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(newX, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "fig_clusters = plt.figure()\n",
    "fig_clusters.suptitle('Clusters over first 2 Components')\n",
    "ax = fig_clusters.add_subplot(111)\n",
    "ax.set_xlabel('Component I')\n",
    "ax.set_ylabel('Component II')\n",
    "plt.scatter(newX[:,0],newX[:,1], c=km.fit_predict(newX))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727"
    }
   },
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "038497e7-3cf4-4c64-867c-1bc637cad5e5"
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## raw_lda_frankjupyter2: modified to fit allrecordsPreparation2\n",
    "############################\n",
    "def raw_lda_frankjupyter2(lemmposrecs, wordimportance, metmodel=2, STOPWORDS=nltk.corpus.stopwords.words('english')):\n",
    "    '''\n",
    "    description: modified model based on https://www.frankcleary.com/svd/ for a more raw construction of a lda\n",
    "    '''\n",
    "    \n",
    "   \n",
    "    def metriccalc(st, normalizer, wordimportance):\n",
    "        '''\n",
    "        description:\n",
    "        text normalization based on ALL characters in the sentence; why? Example: if two writers wrote 20 words, 2 of them very important, but one of them wrote half of characters stopwords, those 2 words wouldnt be penalized accordingly for this writer: the other wrote more important content\n",
    "        '''       \n",
    "        metfuncs = [\n",
    "            lambda w: math.pow(0.1+float(wordimportance[w]),textbow[w]/normalizer) if w in list(wordimportance.keys()) else 0.0, #a sort of idf-normalization based on number of words in the text: the more the words in a text, the more important\n",
    "            lambda w: float(wordimportance[w])*textbow[w] if w in list(wordimportance.keys()) else 0.0, #good but ignore those words with worimportance too low or 0 but that are frequent in text\n",
    "            lambda w: 1.0+2.0**float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- probably the best one; because it is not normilized this indicator would simply say that if it has the word at least once is already on topic\n",
    "            lambda w: float(wordimportance[w]) + textbow[w]/normalizer if w in list(wordimportance.keys()) else 0.0,\n",
    "            lambda w: (1.0+textbow[w]/normalizer)*float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0, #<- apparently the second best with the adhoc word ranking\n",
    "            lambda w: normalizer/textbow[w]*wordimportance[w]/sum(list(wordimportance.values())), #tfidf-ish\n",
    "            lambda w: float(wordimportance[w]) if w in list(wordimportance.keys()) else 0.0\n",
    "        ]\n",
    "        likedict = collections.defaultdict(float)\n",
    "        textbow = collections.Counter(st)\n",
    "        for w in st:\n",
    "            likedict[w] = metfuncs[metmodel](w)\n",
    "        return likedict\n",
    "\n",
    "    #redo_corpus_by_sts = []\n",
    "    words_df = pandas.DataFrame()\n",
    "    textreference = {}\n",
    "    \n",
    "    \n",
    "    for textindex, lemmpos_r in enumerate(lemmposrecs):\n",
    "        lemmpos_t = lemmpos_r[3]\n",
    "        #print('lemmpos_t', len(lemmpos_t))\n",
    "        lemm_sts = ''\n",
    "        for lemmpos_TUPLE in lemmpos_t:\n",
    "            w = lemmpos_TUPLE[2]\n",
    "            lemm_sts = lemm_sts + w + ' '\n",
    "        sts = lemm_sts.split('.')\n",
    "        for stindex, lemmpos_st in enumerate(sts):\n",
    "            treated_st, lensts = cleaningtext(lemmpos_st.split(), STOPWORDS=STOPWORDS)        \n",
    "            #print('treated_st', lensts)\n",
    "            if len(treated_st) > 3:\n",
    "                likedict = metriccalc(treated_st, lensts, wordimportance)\n",
    "                st_df = pandas.DataFrame.from_dict(likedict, orient='index')\n",
    "                textindexing = str(textindex)+'_'+str(stindex)\n",
    "                st_df.columns = [textindexing]\n",
    "                textreference[textindexing] = {}\n",
    "                textreference[textindexing]['treated_st'] = treated_st\n",
    "                #st_df.columns = [str(count)]\n",
    "                words_df = words_df.join(st_df, how='outer', )\n",
    "    \n",
    "    words_df = words_df.fillna(0)\n",
    "    print(\"Number of unique words: %s\" % len(words_df))\n",
    "    print(words_df.head(10))\n",
    "    #print(words_df.sort(columns=words_df.columns[0], ascending=False).head(10))\n",
    "    \n",
    "    return words_df, textreference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "0e3fa11e-feb2-487d-9d53-43dc3deb650c": {
     "id": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "prev": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "regions": {
      "07b1e699-5143-43f7-a938-d61bc27e8e62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e1c251e9-1608-43d5-ba5a-e27bb09739f5",
        "part": "whole"
       },
       "id": "07b1e699-5143-43f7-a938-d61bc27e8e62"
      }
     }
    },
    "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5": {
     "id": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "prev": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "regions": {
      "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c31ae14-8fe5-47c4-83c1-6f4c6e48a727",
        "part": "whole"
       },
       "id": "15418ed7-e8de-4d76-ae88-a5cf2d3b15f0"
      }
     }
    },
    "1cf808ac-39f2-4009-8112-3ef3a17b4af6": {
     "id": "1cf808ac-39f2-4009-8112-3ef3a17b4af6",
     "prev": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "regions": {
      "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a5992afc-b214-46ae-8f12-1026835184cd",
        "part": "whole"
       },
       "id": "4c64cd78-0b2f-47f0-b41d-b53cef3ff5c8"
      }
     }
    },
    "1f7a3d0c-f066-482e-8b40-cf107299d110": {
     "id": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "prev": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "regions": {
      "1badc1e9-6ff7-4358-9493-665026267eab": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d236a59f-ff21-406d-b1df-9436aedbdb11",
        "part": "whole"
       },
       "id": "1badc1e9-6ff7-4358-9493-665026267eab"
      }
     }
    },
    "3659283d-893a-48fa-9115-7dd64d2aed00": {
     "id": "3659283d-893a-48fa-9115-7dd64d2aed00",
     "prev": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "regions": {
      "835576ab-c1f9-4bfa-af97-ed77dbde5925": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e294b910-1a25-4e48-abb5-37239f441f2e",
        "part": "whole"
       },
       "id": "835576ab-c1f9-4bfa-af97-ed77dbde5925"
      }
     }
    },
    "395c02de-c982-4ca8-a48e-10a79828812d": {
     "id": "395c02de-c982-4ca8-a48e-10a79828812d",
     "prev": "0e3fa11e-feb2-487d-9d53-43dc3deb650c",
     "regions": {
      "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3cda7d21-4a2d-4d01-ac74-10e33c83629e",
        "part": "whole"
       },
       "id": "ecb49d92-4f47-445e-a1a3-1daaae9e0ff7"
      }
     }
    },
    "3f4f1ce3-4872-48cc-8c96-31c19dba5a74": {
     "id": "3f4f1ce3-4872-48cc-8c96-31c19dba5a74",
     "prev": "1f7a3d0c-f066-482e-8b40-cf107299d110",
     "regions": {
      "7fb22b2c-4bff-4c35-86aa-40952a6b9b24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f5bb4cd0-4cb1-493f-99e0-e75214f084a7",
        "part": "whole"
       },
       "id": "7fb22b2c-4bff-4c35-86aa-40952a6b9b24"
      }
     }
    },
    "45ee2b8f-c113-44cf-a993-6a20d43e99f5": {
     "id": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "prev": "532186bd-d619-49da-be76-e28eb7db691a",
     "regions": {
      "e4a63835-d541-4073-bee5-0017da9a065b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8e459ff-ae34-48aa-8146-50365df9ea53",
        "part": "whole"
       },
       "id": "e4a63835-d541-4073-bee5-0017da9a065b"
      }
     }
    },
    "532186bd-d619-49da-be76-e28eb7db691a": {
     "id": "532186bd-d619-49da-be76-e28eb7db691a",
     "prev": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "regions": {
      "2b7cff32-e764-4b30-8301-0cb4bb2268dc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67c414c2-1a9a-4cd8-a9c3-0e9d5f61a384",
        "part": "whole"
       },
       "id": "2b7cff32-e764-4b30-8301-0cb4bb2268dc"
      }
     }
    },
    "99fa0d29-85d3-4e57-a540-74b76519f4ac": {
     "id": "99fa0d29-85d3-4e57-a540-74b76519f4ac",
     "prev": "13b1c7f3-b891-4a35-88a4-8a0ba6d92cc5",
     "regions": {
      "afebcb71-18ec-40b4-9e65-8f01e4c70d57": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "038497e7-3cf4-4c64-867c-1bc637cad5e5",
        "part": "whole"
       },
       "id": "afebcb71-18ec-40b4-9e65-8f01e4c70d57"
      }
     }
    },
    "a59429ee-539c-4c02-bfc8-8582111bf455": {
     "id": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "prev": "45ee2b8f-c113-44cf-a993-6a20d43e99f5",
     "regions": {
      "34164544-7832-445e-a820-4ef4dbc522ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5dd4726b-fc1b-4c66-b1cb-25e019ad4d6a",
        "part": "whole"
       },
       "id": "34164544-7832-445e-a820-4ef4dbc522ac"
      }
     }
    },
    "aa51d16f-a5b4-460f-91fa-569faac9a6fe": {
     "id": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "prev": null,
     "regions": {
      "e46d430e-2c0b-416c-bac3-86a2d27711c1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "560a94b7-6c8e-465f-96fa-fcf2c3859245",
        "part": "whole"
       },
       "id": "e46d430e-2c0b-416c-bac3-86a2d27711c1"
      }
     }
    },
    "ac6a71ca-a1d6-4449-9ee7-87ff28115363": {
     "id": "ac6a71ca-a1d6-4449-9ee7-87ff28115363",
     "prev": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "regions": {
      "ce9295e6-aa18-4ceb-a277-fb19127d89ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "67bf390c-9e34-4764-a6de-ab9d3082090b",
        "part": "whole"
       },
       "id": "ce9295e6-aa18-4ceb-a277-fb19127d89ed"
      }
     }
    },
    "baa3a1da-cdb2-4638-8bec-2986349ac603": {
     "id": "baa3a1da-cdb2-4638-8bec-2986349ac603",
     "prev": "a59429ee-539c-4c02-bfc8-8582111bf455",
     "regions": {
      "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "59c7646c-3944-4e2c-81a1-1728a02396ae",
        "part": "whole"
       },
       "id": "a4ad5d4b-8d00-4c0e-8edf-9163ad9284ce"
      }
     }
    },
    "ca7a6c34-27e7-4921-9da2-43a92faf8e8e": {
     "id": "ca7a6c34-27e7-4921-9da2-43a92faf8e8e",
     "prev": "395c02de-c982-4ca8-a48e-10a79828812d",
     "regions": {
      "6414ffa6-ea4d-40fe-9d90-5b0816ff0789": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1c1fd569-b2c4-423d-a598-1466fc992a49",
        "part": "whole"
       },
       "id": "6414ffa6-ea4d-40fe-9d90-5b0816ff0789"
      }
     }
    },
    "dae9886f-f215-4b89-aefb-8831c7f2ddfc": {
     "id": "dae9886f-f215-4b89-aefb-8831c7f2ddfc",
     "prev": "aa51d16f-a5b4-460f-91fa-569faac9a6fe",
     "regions": {
      "5eeb753c-493e-4bba-b5ab-025339c264f2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d80b2206-d953-46b3-b807-2bf5a8557d28",
        "part": "whole"
       },
       "id": "5eeb753c-493e-4bba-b5ab-025339c264f2"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
